<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Passionate about NLP">
<meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="http://yoursite.com/page/7/index.html">
<meta property="og:site_name">
<meta property="og:description" content="Passionate about NLP">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title">
<meta name="twitter:description" content="Passionate about NLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'PJWL3PD75I',
      apiKey: '5589833aa2fa703729b4e7e939ac7dec',
      indexName: 'test_index',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/7/"/>





  <title></title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title"></span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/25/Recurrent-Neural-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/25/Recurrent-Neural-Network/" itemprop="url">Recurrent_Neural_Network</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-25T11:09:32+08:00">
                2018-07-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Gradient vanishing or exploding:<br>Since the time step 1 would have effect on time step n, when doing back propagation, according to the chain rule, the same weight matrix W would be multiplied by n times which is W to the power of n. So the norm of W is greater than 1, there is gradient exploding. And vice versa, gradient vanishing if the norm of W is smaller than 1.</p>
<p>Solution to gradient vanishing:<br>Initialize weight matrix to identity matrix  rather than a matrix of random values. And use Relu as activation function. The intuition of this is to take average of input and hidden state. Then start to optimize the weight matrix.</p>
<h3 id="Attention-Model"><a href="#Attention-Model" class="headerlink" title="Attention Model"></a>Attention Model</h3><p>The input includes all or some of the hidden states in the encoder. Utilize a score function to weight importance of those hidden states and feed into the decoder with the input word.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/23/Tensorflow/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/23/Tensorflow/" itemprop="url">Tensorflow</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-23T14:42:47+08:00">
                2018-07-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Characteristic: Lazy evaluation like scala</p>
<p>Two steps to build and run a model:</p>
<ul>
<li>assemble a graph</li>
<li>use a session to execute operations in the graph</li>
</ul>
<p>Placeholder: a node to store variable which would not be modified during backpropagation</p>
<p>Variable: a node that would be modified during backpropagation</p>
<p>Constant: values that cannot be changed</p>
<p>Running parts need to be encapsulated into session.run(). session.run() is evaluation of the graph.</p>
<p>Variables could have name and scope.</p>
<p>Use TF DType to accelerate the computation or tensorflow has to infer the type.</p>
<p>create variables with tf.get_variable</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m = tf.get_variable(&quot;matrix&quot;, initializer=tf.constant([[0, 1], [2, 3]]))</span><br></pre></td></tr></table></figure>
<p>rather than </p>
<figure class="highlight m"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">Before running, variables need to be initialized.</span><br></pre></td></tr></table></figure>
<p>initializer = tf.global_variables_initializer()<br>with tf.Session() as sess:<br>    sess.run(initializer)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Initialize only a subset of variables:</span><br></pre></td></tr></table></figure></p>
<p>with tf.Session() as sess:<br>    sess.run([a,b])<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Initialize only a single variable:</span><br></pre></td></tr></table></figure></p>
<p>m=tf.get_variable(“matrix”,initializer=tf.constant([[0, 1], [2, 3]]))<br>with tf.Session() as sess:<br>    sess.run(m.initializer)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">W.assign(100) creates an assign operation that assign 100 to variable W. The operation needs to be executed in a session to take effect.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensorflow separates definitions of computation and execution. It first assembles a graph. Second use a session to execute.</span><br><span class="line"></span><br><span class="line">Nodes are operators, variables, and constants</span><br><span class="line"></span><br><span class="line">Edges are tensors.</span><br><span class="line"></span><br><span class="line">A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated.  Session will also allocate memory to store the current values of variables.</span><br></pre></td></tr></table></figure></p>
<p>tf.Session.run(fetches,    feed_dict=None,    options=None,    run_metadata=None)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">fetches is a list of tensors whose values you want. tensorflow would return the values you want and skip computing unnecessary values.</span><br><span class="line"></span><br><span class="line">Multiple graphs require multiple sessions, each will try to use all available resources by default</span><br><span class="line"></span><br><span class="line">why graphs:</span><br><span class="line">1. break computation to facilitate auto-differentiation.</span><br><span class="line">2. save computation. only compute necessary values</span><br><span class="line">3. easier to distribute</span><br><span class="line">4. ML models visulised as graphs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Can&apos;t pass data between them without passing them through python/numpy, which doesn&apos;t work in distributed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Create the summary writer after graph definition and before running your session.</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"># The default path for saving event files is the same folder of this python file.</span><br><span class="line">tf.app.flags.DEFINE_string(</span><br><span class="line">&apos;log_dir&apos;, os.path.dirname(os.path.abspath(__file__)) + &apos;/logs&apos;,</span><br><span class="line">&apos;Directory where event logs are written to.&apos;)</span><br><span class="line"></span><br><span class="line"># Store all elements in FLAG structure!</span><br><span class="line">FLAGS = tf.app.flags.FLAGS</span><br><span class="line"></span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line">    writer = tf.summary.FileWriter(os.path.expanduser(FLAGS.log_dir), sess.graph)</span><br><span class="line">    .....</span><br><span class="line"># Closing the writer.</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure></p>
<p>These codes are used to specify the path of logs directory and store all elements in FLAG structure.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># clean flag name</span></span><br><span class="line"><span class="keyword">from</span> absl <span class="keyword">import</span> flags</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> list(flags.FLAGS):</span><br><span class="line">  delattr(flags.FLAGS, name)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">UnrecognizedFlagError: Unknown command line flag <span class="string">'f'</span></span><br></pre></td></tr></table></figure>
<p>To solve this problem, just add a flag. In coommand line, there is not such a problem.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.app.flags.DEFINE_string(<span class="string">'f'</span>, <span class="string">''</span>, <span class="string">'kernel'</span>)</span><br></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.app.flags.DEFINE_string(&apos;str_name&apos;, &apos;def_v_1&apos;,&quot;descrip1&quot;)</span><br></pre></td></tr></table></figure>
<p>tf.app.flags is used to assign a name to a variable and parse variables in command line. The first argument is the variable name. The second argument is the default value. The third argument is a description. argparse module in python could also be employed to parse arguments.</p>
<p>Any evaluation of variable must be run in a session.</p>
<p>Three ways to initialize variables:</p>
<ol>
<li>Initializing Specific Variables.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># &quot;variable_list_custom&quot; is the list of variables that we want to initialize.</span><br><span class="line">variable_list_custom = [weights, custom_variable]</span><br><span class="line"></span><br><span class="line"># The initializer</span><br><span class="line">init_custom_op = tf.variables_initializer(var_list=all_variables_list)</span><br></pre></td></tr></table></figure>
<p>This enable us to initialize specific variables. We are supposed to initialize all variables after specific initialization.</p>
<ol start="2">
<li>Global variable initialization. This operation has to be done after the construction of a model.</li>
<li>Initialization of a variables using other existing variables. New variables can be initialized using other existing variables’ initial values by taking the values using initialized_value().<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Create another variable with the same value as 'weights'.</span></span><br><span class="line">WeightsNew = tf.Variable(weights.initialized_value(), name=<span class="string">"WeightsNew"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now, the variable must be initialized.</span></span><br><span class="line">init_WeightsNew_op = tf.variables_initializer(var_list=[WeightsNew])</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>tf.one_hot() The axis argument determines which axis is the one-hot vector. </p>
<p>Name scope is for grouping nodes together to facilitate visulisation in Tensorboard. Name scope is for reusing of operations while variable scope is for reusing of variables. </p>
<p>Name scope is able to assign names to a set of variables. It is equivalent to add “name_scope/” to the variable name as prefix.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&quot;outer&quot;):  c_2 = tf.constant(2, name=&quot;c&quot;)  # =&gt; operation named &quot;outer/c&quot;</span><br></pre></td></tr></table></figure>
<p>An <code>Operation</code> is a node in a TensorFlow <code>Graph</code> that takes zero or more <code>Tensor</code> objects as input, and produces zero or more <code>Tensor</code> objects as output. Objects of type <code>Operation</code> are created by calling a Python op constructor (such as [<code>tf.matmul</code>] or [<code>tf.Graph.create_op</code>].</p>
<p>A <code>Tensor</code> is a symbolic handle to one of the outputs of an <code>Operation</code>. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow [<code>tf.Session</code>].</p>
<p>Note that [<code>tf.Tensor</code>] objects are implicitly named after the [<code>tf.Operation</code>]that produces the tensor as output. A tensor name has the form <code>&quot;&lt;OP_NAME&gt;:&lt;i&gt;&quot;</code>.</p>
<p>tf.get_variable would search the variable with the given name. If there is not such a variable, it would create a new variable.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.reduce_mean()</span><br></pre></td></tr></table></figure>
<p>Sum over a vector in a particular dimension and compute the mean. The axis argument is used to specify the particular dimension.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.nn.softmax_cross_entropy_with_logits_v2()</span><br></pre></td></tr></table></figure>
<p>Take unscaled log probabilities as input and perform softmax. After that, compute cross entropy loss. This functino allows gradient to backpropagated to labels.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cast()</span><br></pre></td></tr></table></figure>
<p>Casts a tensor to a new type.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = tf.constant([1.8, 2.2], dtype=tf.float32)tf.cast(x, tf.int32)  # [1, 2], dtype=tf.int32</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tf.Graph().as_default():</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>
<p>There is a default graph in tensorflow. All operations and variables are added to the graph if not specified. This line of code use another graph and add variables to that graph. Outside the code block, variables would be added to the default graph.</p>
<p> TensorFlow will create a new [<code>tf.Tensor</code>]each time you use the same tensor-like object. If the tensor-like object is large (e.g. a <code>numpy.ndarray</code> containing a set of training examples) and you use it multiple times, you may run out of memory.</p>
<p>By default, tf.Session is bounded to the default graph. If there is multiple graphs, specify graph parameter in tf.Session().</p>
<p>The allow_soft_placement flag allows the switching back-and-forth between different devices. In tensorflow, not all operations are supported in GPU. The log_device_placement flag is to present which operations are set on what devices.</p>
<p>The <strong>max_to_keep</strong> flags determine the maximum number of the saved models that the TensorFlow keeps and its default is set to ‘5’ by TensorFlow.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.zeros_like(input_tensor, dtype=None, name=None, optimize=True)</span><br></pre></td></tr></table></figure>
<p>create a tensor filled with zero with the same shape as input_tensor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.fill(dims, value, name=None)</span><br></pre></td></tr></table></figure>
<p>create a tensor filled with a scalar value</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.lin_space(start, stop, num, name=None)</span><br><span class="line"></span><br><span class="line">tf.range(start, limit=None, delta=1, dtype=None, name=&apos;range&apos;)</span><br></pre></td></tr></table></figure>
<p>create a tensor with a sequence of values</p>
<p>scalars are treated like tensors with zero dimension</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a = sess.run(a)    #use a_out = sess.run(a) instead for the sake of memory</span><br></pre></td></tr></table></figure>
<p>Use TF DType whenever possible for computation convenience.</p>
<p> Each session maintains its own copy of variables</p>
<p>tf.constant is an op. tf.Variable is a class with many ops.</p>
<p>Only use tf.constant for primitive type. Otherwise the loading of graphs would be expensive.</p>
<p>Constant values are stored in the graph definition.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">s = tf.get_variable(&quot;scalar&quot;, initializer=tf.constant(2))  #is preferred</span><br><span class="line"></span><br><span class="line">s = tf.Variable(2)</span><br></pre></td></tr></table></figure>
<p>Initializer is an op. You need to execute it within the context of a session.</p>
<p>Each variable has its own initializer.</p>
<p>Sessions allocate memory to store variable values.</p>
<p>Eval() a variable is equivalent to sess.run(a variable)</p>
<p>tf.Variable.assign() is an op.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.placeholder(dtype, shape=None, name=None)</span><br></pre></td></tr></table></figure>
<p>shape=None is easy to construct graphs but it is difficult for debugging.</p>
<p>tensors could be fed with some dummy values for testing purpose.</p>
<p>Lazy loading would make the graph bloated and hard to load. <strong>Remeber to seperate definition of a graph from computing/running ops</strong>.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># feed 1.0 to x</span><br><span class="line">x = tf.placeholder(tf.float32, shape=None, name=&apos;x&apos;)</span><br></pre></td></tr></table></figure>
<p>InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor ‘x’ with dtype float.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># feed 1.0 to x</span><br><span class="line">x = tf.placeholder(tf.float32, name=&apos;x&apos;)</span><br></pre></td></tr></table></figure>
<p>works…….</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.cond( pred, true_fn=None, false_fn=None,    strict=False, name=None)</span><br></pre></td></tr></table></figure>
<p>tf.cond: pred is an expression. true_fn and flase_fn are callable function. If the expression is true, return the result of true_fn and vise versa.</p>
<p>tf.placeholder is pythonic, but it is also easy to become bottleneck if running in single thread.</p>
<p>tf.data is preferred. For prototyping, feed dict can be faster and easier to write (pythonic). But tf.data is tricky to use when you have complicated preprocessing or multiple data sources.</p>
<p>Session looks at all trainable variables that optimizer depends on and update them.</p>
<p>NCE guarantees approximation to softmax but Negative Sampling doesn’t.</p>
<p>tf.train.Saver saves graph’s variables in binary files. It saves session rather than graph. When re-using parameters, it is necessart to construct the graph first and then restore the parameters.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope(&quot;summaries&quot;): </span><br><span class="line">  tf.summary.scalar(&quot;loss&quot;, self.loss)</span><br><span class="line">  tf.summary.scalar(&quot;accuracy&quot;, self.accuracy)</span><br><span class="line">  tf.summary.histogram(&quot;histogram loss&quot;, self.loss)</span><br><span class="line">  summary_op = tf.summary.merge_all()</span><br></pre></td></tr></table></figure>
<p>merge them all into one summary op to make managing them easier.</p>
<p>Like everything else in TF, summaries are ops. For the summaries to be built, you have to run it in a session</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss_batch, _, summary = sess.run([loss, optimizer, summary_op])</span><br><span class="line"></span><br><span class="line">writer.add_summary(summary, global_step=step)</span><br></pre></td></tr></table></figure>
<p>Randomization:</p>
<ol>
<li>operation level. my_var = tf.Variable(tf.truncated_normal((-1.0,1.0), stddev=0.1, seed=0))</li>
<li>graph level. tf.set_random_seed(2)</li>
</ol>
<p>tf dataset:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.data.Dataset.from_tensor_slices()</span><br></pre></td></tr></table></figure>
<p>constructs a dataset from one or more <code>tf.Tensor</code> objects. It slices the first dimension of the tensor.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.data.Dataset.batch()</span><br></pre></td></tr></table></figure>
<p>or<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.data.Dataset.map()</span><br></pre></td></tr></table></figure></p>
<p>apply a transformation to a dataset and create another dataset.</p>
<p>A [<code>tf.data.Dataset</code>] represents a sequence of elements, in which each element contains one or more <code>Tensor</code>objects. For example, an element could be an image corresponds to a label or a sequence of tokens correspond to another sequence of tokens or only a simple label.</p>
<p>A <a href="https://www.tensorflow.org/api_docs/python/tf/data/Iterator" target="_blank" rel="noopener"><code>tf.data.Iterator</code></a> provides the main way to extract elements from a dataset. The operation returned by <code>Iterator.get_next()</code> yields the next element of a <code>Dataset</code> when executed, and typically acts as the interface between input pipeline code and your model. For more sophisticated uses, the <code>Iterator.initializer</code>operation enables you to reinitialize and parameterize an iterator with different datasets, so that you can, for example, iterate over training and validation data multiple times in the same program.</p>
<p> <a href="https://www.tensorflow.org/api_docs/python/tf/data/TFRecordDataset" target="_blank" rel="noopener"><code>tf.data.TFRecordDataset</code></a> constructs dataset from files if the files are on disk in the recommended TFRecord format.</p>
<p>If the iterator reaches the end of the dataset, executing the <code>Iterator.get_next()</code> operation will raise a <a href="https://www.tensorflow.org/api_docs/python/tf/errors/OutOfRangeError" target="_blank" rel="noopener"><code>tf.errors.OutOfRangeError</code></a>. After this point the iterator will be in an unusable state, and you must initialize it again if you want to use it further.</p>
<p>A common pattern is to wrap the “training loop” in a <code>try</code>-<code>except</code> block:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(iterator.initializer)<span class="keyword">while</span> <span class="keyword">True</span>:  <span class="keyword">try</span>:    sess.run(result)  <span class="keyword">except</span> tf.errors.OutOfRangeError:    <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>A <strong>reinitializable</strong> iterator can be initialized from multiple different <code>Dataset</code> objects. For example, you might have a training input pipeline that uses random perturbations to the input images to improve generalization, and a validation input pipeline that evaluates predictions on unmodified data. These pipelines will typically use different <code>Dataset</code> objects that have the same structure (i.e. the same types and compatible shapes for each component).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">iterator = tf.data.Iterator.from_structure(training_dataset.output_types,                                           training_dataset.output_shapes)</span><br><span class="line">next_element = iterator.get_next()</span><br></pre></td></tr></table></figure>
<p>A <strong>one-shot</strong> iterator is the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataset = tf.data.Dataset.range(<span class="number">100</span>)</span><br><span class="line">iterator = dataset.make_one_shot_iterator()</span><br><span class="line">next_element = iterator.get_next()</span><br></pre></td></tr></table></figure>
<p>An <strong>initializable</strong> iterator requires you to run an explicit <code>iterator.initializer</code> operation before using it. In exchange for this inconvenience, it enables you to <em>parameterize</em> the definition of the dataset, using one or more <code>tf.placeholder()</code>tensors that can be fed when you initialize the iterator.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">max_value = tf.placeholder(tf.int64, shape=[])</span><br><span class="line">dataset = tf.data.Dataset.range(max_value)</span><br><span class="line">iterator = dataset.make_initializable_iterator()</span><br><span class="line">next_element = iterator.get_next()<span class="comment"># Initialize an iterator over a dataset with 10 elements.</span></span><br><span class="line">sess.run(iterator.initializer, feed_dict=&#123;max_value: <span class="number">10</span>&#125;)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):  </span><br><span class="line">  value = sess.run(next_element)  </span><br><span class="line">  <span class="keyword">assert</span> i == value</span><br></pre></td></tr></table></figure>
<p>The <code>Iterator.get_next()</code> method returns one or more <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor" target="_blank" rel="noopener"><code>tf.Tensor</code></a> objects that correspond to the symbolic next element of an iterator. Each time these tensors are evaluated, they take the value of the next element in the underlying dataset. Note that the iterator does not get to the next element immediately. The returned element is suuposed to be passed to  <code>tf.Session.run()</code> to get the next elements and advance the iterator.</p>
<p>Applying arbitrary Python logic with <code>tf.py_func()</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.stack(    values,    axis=0,    name=&apos;stack&apos;)</span><br></pre></td></tr></table></figure>
<p>concatenate tensors along specified dimension.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.data.Dataset.padded_batch()</span><br></pre></td></tr></table></figure>
<p>padded_shapes is None, the component will be padded out to the maximum length of all elements in that dimension.</p>
<p>Estimators are high-level abstraction of low-level tensorflow operation. It enables easier training, testing on different kinds of devices and modes like training on a single PC or a cluster.</p>
<p>4 steps to use pre-made Estimators</p>
<ol>
<li>write function to import dataset</li>
<li>Define the feature columns.</li>
<li>Instantiate the relevant pre-made Estimator like specifying number of hidden layers, output features</li>
<li>There are training and evaluation methods for Estimators. Call a training, evaluation, or inference method.</li>
</ol>
<p>tf.feature_column could be regarded as a kind of interface between the dataset and model in Estimators. It could abstract continuous values or categorical values.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/23/Dependency-Parsing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/23/Dependency-Parsing/" itemprop="url">Dependency_Parsing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-23T12:23:44+08:00">
                2018-07-23
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Transition-Based-Dependency-Parsing"><a href="#Transition-Based-Dependency-Parsing" class="headerlink" title="Transition-Based Dependency Parsing"></a>Transition-Based Dependency Parsing</h2><p>Linear time, greedy algorithm, no backtracking.</p>
<p>A configuration is a stack, a buffer of token lists and an oracle. The key is to train the oracle. </p>
<p>The algorithm is intuitive. If there is a match in the candidate relation set, pop the words and add the relation to the result set. Push the words in the buffer into the stack. repeat until no word left.</p>
<h3 id="Creating-an-Oracle"><a href="#Creating-an-Oracle" class="headerlink" title="Creating an Oracle"></a>Creating an Oracle</h3><p>Supervised learning is employed to do this.</p>
<h4 id="Generating-Training-Data"><a href="#Generating-Training-Data" class="headerlink" title="Generating Training Data"></a>Generating Training Data</h4><p>given a reference parse and a configuration, the training oracle proceeds as follows:</p>
<ul>
<li>Choose LEFTARC if it produces a correct head-dependent relation </li>
<li>Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent relation<br>and (2) all of its dependents in the buffer have already been assigned.</li>
<li>Otherwise, choose SHIFT.</li>
</ul>
<h4 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h4><p>create feature sets.</p>
<h4 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h4><p>The dominant approaches to training transition-based dependency parsers have been multinomial logistic regression and support vector machines, both of which can make effective use of large numbers of sparse features.</p>
<p>Deep learning approaches have been applied successfully to transition-based parsing. These approaches eliminate the need for complex, hand-crafted features and have been particularly effective at overcoming the data sparsity issues normally associated training transition-based<br>parsers.</p>
<h3 id="Alternative-Transition-Systems"><a href="#Alternative-Transition-Systems" class="headerlink" title="Alternative Transition Systems"></a>Alternative Transition Systems</h3><p><strong>arc eager</strong> transition system is just a modified version of the oracle.</p>
<p>The operation of the oracle:</p>
<ul>
<li>LEFTARC: Assert a head-dependent relation between the word at the front of<br>the input buffer and the word at the top of the stack; pop the stack.</li>
<li>RIGHTARC: Assert a head-dependent relation between the word on the top of<br>the stack and the word at front of the input buffer; shift the word at the front<br>of the input buffer to the stack.</li>
<li>SHIFT: Remove the word from the front of the input buffer and push it onto<br>the stack.</li>
<li>REDUCE: Pop the stack.</li>
</ul>
<h3 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h3><p>BFS with a heuristic filter that prunes the search frontier to stay within a fixed-size beam width.</p>
<h3 id="Graph-Based"><a href="#Graph-Based" class="headerlink" title="Graph-Based"></a>Graph-Based</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/21/Back-Propagation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/21/Back-Propagation/" itemprop="url">Back_Propagation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-21T15:51:30+08:00">
                2018-07-21
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="Derivative-of-composite-function"><a href="#Derivative-of-composite-function" class="headerlink" title="Derivative of composite function"></a>Derivative of composite function</h3><p>$$\frac{d\frac{f(x)}{g(x)}}{dx} = \frac{f’(x)g(x)-f(x)g’(x)}{g^2(x)}$$</p>
<h3 id="Derivative-of-Cross-Entropy"><a href="#Derivative-of-Cross-Entropy" class="headerlink" title="Derivative of Cross Entropy"></a>Derivative of Cross Entropy</h3><p>$$CE(\vec{y},\vec{\hat{y}}) = -\sum_iy_i\log(\hat{y_i})$$</p>
<p>Here \(y_i\) is an one-hot vector denotes the correct class \(c_i\). In the actual computation, it only selects out the correct class without any extra function.</p>
<p>For a single example \(x_i\):</p>
<p>if \(\hat{y_i}=y_i\):</p>
<p>$$\frac{dCE(y_i,\hat{y_i})}{dx_i} = \frac{1}{\hat{y_i}}\frac{d(\hat{y_i})}{dx_i} = \frac{\sum_j^Ce^{(x_j)}}{e^{x_i}}\ \frac{e^{x_i}\sum_j^Ce^{x_j}-e^{2x_i}}{(\sum_j^Ce^{x_j})^2} = \hat{y_i}-1$$</p>
<p>if \(\hat{y_i}\ne y_i\):</p>
<p>$$\frac{dCE(y_i,\hat{y_i})}{dx_i} = \frac{1}{\hat{y_i}}\frac{d(\hat{y_i})}{dx_i} = \frac{\sum_j^Ce^{(x_j)}}{e^{x_i}}\ \frac{e^{x_i}e^{x_k}}{(\sum_j^Ce^{x_j})^2}=\hat{y_i}$$</p>
<p>The rest is just compute the gradient layer by layer.</p>
<p><strong>Note that when compute the gradient with regard to activation function, there would be outer product.</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/18/Pagerank/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/18/Pagerank/" itemprop="url">Pagerank</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-18T12:42:01+08:00">
                2018-07-18
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages.</p>
<h2 id="PageRank"><a href="#PageRank" class="headerlink" title="PageRank"></a>PageRank</h2><h4 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h4><p>Pages that are visited more are given higher weight. Employ graph to represent the internet. Higher in links means visited more.</p>
<p>PageRank values would converge to the final value regardless of the start state and it is in the range [0,1]. So the PageRank values could also be viewed as the probability of the surfer is in the node i at any time given the PageRank value of the node i.</p>
<p>PageRank could be regarded as a random walk in the internet. Start at any state, and then walk through the out links randomly. In case of circular routes and dead end, employ the scaled version of random walk. As the random walk proceeds, some nodes are visited more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. The idea behind PageRank is that pages visited more often in this walk are more important.</p>
<p><strong>Teleport</strong>:if N is the total number of nodes in the web graph, the teleport operation takes the surfer to each node with probability 1/N.</p>
<h4 id="Random-Walk-Interpretation"><a href="#Random-Walk-Interpretation" class="headerlink" title="Random Walk Interpretation"></a>Random Walk Interpretation</h4><p>Scaled version of random walk:<br>with alpha probability take teleport operation and with 1-alpha probability take the out links of the current nodes.</p>
<p>Interpreted in terms of the (scaled) version of PageRank, <strong>Perron’s Theorem</strong> tells us that there is a unique vector y that remains fixed under the application of the scaled update rule, and that repeated application of the update rule from any starting point will converge to y.<br>This vector y thus corresponds to the limiting PageRank values we have been seeking.</p>
<h4 id="Graph-Interpretation"><a href="#Graph-Interpretation" class="headerlink" title="Graph Interpretation"></a>Graph Interpretation</h4><p>Each vertex is a node. PageRank value is 1 in total. Initially, each node has the same pagerank value 1/N. With the different in links and out links, the pagerank value would flow through those links to different notes. So the pagerank value would change accordingly.</p>
<p><strong>Scaled version of graph</strong>:<br>With alpha probability take teleport operation and with 1-alpha probability do the normal flow operation. In terms of graph, it discount the overall pagerank value to \(1-\alpha \) and assign the rest \(\alpha \) pagerank value to each node. So each node would get the same pagerank value \(\alpha /N\).</p>
<h3 id="Markov-chain"><a href="#Markov-chain" class="headerlink" title="Markov chain"></a>Markov chain</h3><p>A Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made. A Markov chain consists of N states. Each web page will correspond to a state in the Markov chain.</p>
<p>A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1.</p>
<p>Transition matrix P:<br>The rows represent that given a node i, the probability of the node i transits to nodes j.(distribute the pagerank score)<br>The columns represent that given a node j, the probability of nodes i transits to node j.(incoming pagerank score)</p>
<p><strong>ERGODIC MARKOV CHAIN</strong>:<br>Definition: A Markov chain is said to be ergodic if there exists a positive integer T0 such that for all pairs of states i, j in the Markov chain, if it is started at time 0 in state i then for all t &gt; T0, the probability of being in state j at time t is greater than 0.</p>
<p>For a Markov chain to be ergodic, two technical conditions are required of its states and the non-zero transition probabilities; these conditions are known as <strong>irreducibility</strong> and <strong>aperiodicity</strong>.</p>
<p><strong>irreducibility</strong> ensures that there is a sequence of transitions of non-zero probability from any state to any other.</p>
<p><strong>aperiodicity</strong> ensures that the states are not partitioned into sets such that all state transitions occur cyclically from one set to another.</p>
<p>Scaled version of random walk which take teleport operation with alpha probability guarantees transition probability are greater than 0 and no loop transitions.(irreducibility and aperiodicity)</p>
<h3 id="Compute-PageRank"><a href="#Compute-PageRank" class="headerlink" title="Compute PageRank"></a>Compute PageRank</h3><ul>
<li>Iteratively compute \(\vec{x}P^t\) s.t. \(\vec{x}\) becomes unchanged.</li>
<li>Compute the eigenvector coresponding to the largest eigenvalue 1 which is the PageRank. But compute the eigenvector could be computationally expensive.</li>
</ul>
<h4 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h4><h5 id="Matrix-Multiplication-version-of-PageRank"><a href="#Matrix-Multiplication-version-of-PageRank" class="headerlink" title="Matrix-Multiplication version of PageRank:"></a>Matrix-Multiplication version of PageRank:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Pagerank</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, N=<span class="number">0</span>, alpha=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.alpha = alpha</span><br><span class="line">        self.N = N</span><br><span class="line">        self.pg_vector = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''derive transition matrix from adjacency matrix'''</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">derive_transition</span><span class="params">(self,adjacency_matrix,alpha=<span class="number">0</span>)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.alpha != <span class="number">0</span>:</span><br><span class="line">            alpha = self.alpha</span><br><span class="line"></span><br><span class="line">        N = adjacency_matrix.shape[<span class="number">0</span>]</span><br><span class="line">        transition_m = np.zeros(adjacency_matrix.shape)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(adjacency_matrix.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="keyword">if</span> np.sum(adjacency_matrix[i,:]) == <span class="number">0</span>:</span><br><span class="line">                transition_m[i,:] = np.array([<span class="number">1</span>/N]*N)   <span class="comment">#this node is a dead end, transit to one of all nodes in the graph</span></span><br><span class="line">            <span class="keyword">else</span>:           <span class="comment"># with alpha probability transit to one of all nodes, 1-alpha take normal random walk</span></span><br><span class="line">                transition_m[i, :] = alpha * np.array([<span class="number">1</span> / N] * N) + (<span class="number">1</span>-alpha) * adjacency_matrix[i,:] / np.sum(adjacency_matrix[i,:])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> transition_m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">propagate_transition</span><span class="params">(self, transition_m, pg_vector=None)</span>:</span>  <span class="comment">#pg_vector is column vectors</span></span><br><span class="line">        <span class="keyword">if</span> pg_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span> np.dot(transition_m.T, pg_vector)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.pg_vector <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                self.initialize_pg_vector()</span><br><span class="line"></span><br><span class="line">            self.pg_vector = np.dot(transition_m.T, self.pg_vector)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">return</span> self.pg_vector</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">initialize_pg_vector</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> self.N != <span class="number">0</span>, <span class="string">"Please input number of Nodes N"</span></span><br><span class="line">        self.pg_vector = np.ones((self.N)).T</span><br><span class="line">        self.pg_vector /= self.N</span><br><span class="line">        print(self.pg_vector)</span><br></pre></td></tr></table></figure>
<h5 id="Graph-version-of-PageRank"><a href="#Graph-version-of-PageRank" class="headerlink" title="Graph version of PageRank:"></a>Graph version of PageRank:</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, d, vertex=None)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> isinstance(d,dict), <span class="string">"please input a dict"</span></span><br><span class="line">        self.edges = d</span><br><span class="line">        self.d = d</span><br><span class="line">        self.vertexes = self.get_vertexes()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_vertexes</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> list(self.d.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_edges</span><span class="params">(self)</span>:</span></span><br><span class="line">        l = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> self.d.keys():</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> self.d[k]:</span><br><span class="line">                l.append(k+<span class="string">'-'</span>+v)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__matrix_to_dict</span><span class="params">(self, matrix)</span>:</span>     <span class="comment">#convert adjacency matrix to dict</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">BFS</span><span class="params">(self)</span>:</span></span><br><span class="line">        done = []</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> self.d.keys():</span><br><span class="line">            <span class="keyword">if</span> k <span class="keyword">not</span> <span class="keyword">in</span> done:</span><br><span class="line">                done.append(k)</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> self.d[k]:</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">in</span> done:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    done.append(v)</span><br><span class="line">        <span class="keyword">return</span> done</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">DFS</span><span class="params">(self)</span>:</span></span><br><span class="line">        done = []</span><br><span class="line">        l = list(self.d.keys())</span><br><span class="line">        <span class="keyword">while</span> len(l) != <span class="number">0</span>:</span><br><span class="line">            current = l.pop()</span><br><span class="line">            <span class="keyword">if</span> current <span class="keyword">not</span> <span class="keyword">in</span> done:</span><br><span class="line">                done.append(current)</span><br><span class="line">                l.extend(list(self.d[current]))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> done</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">DFS_recursive</span><span class="params">(self)</span>:</span></span><br><span class="line">        done = []</span><br><span class="line">        l = list(self.d.keys())</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">recursive</span><span class="params">(element)</span>:</span></span><br><span class="line">            waiting_l = self.d[element]</span><br><span class="line">            <span class="keyword">for</span> e <span class="keyword">in</span> waiting_l:</span><br><span class="line">                <span class="keyword">if</span> e <span class="keyword">not</span> <span class="keyword">in</span> done:</span><br><span class="line">                    done.append(e)</span><br><span class="line">                    recursive(e)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> len(l) != <span class="number">0</span>:</span><br><span class="line">            current = l.pop()</span><br><span class="line">            <span class="keyword">if</span> current <span class="keyword">not</span> <span class="keyword">in</span> done:</span><br><span class="line">                done.append(current)</span><br><span class="line">                recursive(current)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> done</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PageRank</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, graph)</span>:</span></span><br><span class="line">        self.g = graph</span><br><span class="line">        self.vertexes = graph.vertexes</span><br><span class="line">        self.pr_value = &#123;&#125;</span><br><span class="line">        self.num_points = len(self.vertexes)</span><br><span class="line">        self.__pr_value_initialization__()</span><br><span class="line">        self.new_pr_value = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__pr_value_initialization__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> ver <span class="keyword">in</span> self.vertexes:</span><br><span class="line">            self.pr_value[ver] = <span class="number">1.0</span> / self.num_points</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__new_pr_value_initialization__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> ver <span class="keyword">in</span> self.vertexes:</span><br><span class="line">            self.new_pr_value[ver] = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_a_round</span><span class="params">(self, alpha=<span class="number">0</span>)</span>:</span></span><br><span class="line">        self.__new_pr_value_initialization__()</span><br><span class="line">        <span class="keyword">for</span> ver <span class="keyword">in</span> self.vertexes:</span><br><span class="line">            <span class="keyword">if</span> len(self.g.edges[ver]) == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">                <span class="comment"># self.new_pr_value[ver] = self.pr_value[ver]</span></span><br><span class="line">            value_flow = self.pr_value[ver] / len(self.g.edges[ver])</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> child <span class="keyword">in</span> self.g.edges[ver]:</span><br><span class="line">                self.new_pr_value[child] += value_flow</span><br><span class="line">        <span class="keyword">if</span> alpha != <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">for</span> ver <span class="keyword">in</span> self.vertexes:</span><br><span class="line">                self.new_pr_value[ver] = alpha * <span class="number">1.0</span> / self.num_points + (<span class="number">1</span>-alpha) * self.new_pr_value[ver]</span><br><span class="line"></span><br><span class="line">        self.pr_value = copy.deepcopy(self.new_pr_value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">propagate_transition</span><span class="params">(self, alpha=<span class="number">0</span>)</span>:</span></span><br><span class="line">        last = np.zeros((self.num_points))</span><br><span class="line">        <span class="comment"># for i in range(100):</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            self.update_a_round(alpha)</span><br><span class="line">            current = np.array(list(self.pr_value.values()))</span><br><span class="line">            error = abs(current - last)</span><br><span class="line">            <span class="comment"># print('last', last)</span></span><br><span class="line">            <span class="comment"># print('current', current)</span></span><br><span class="line">            <span class="comment"># print('i', i)</span></span><br><span class="line">            <span class="comment"># print('error: ', error)</span></span><br><span class="line">            <span class="keyword">if</span> np.sum(error) &lt; <span class="number">1e-6</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">            last = current</span><br><span class="line"></span><br><span class="line"><span class="string">'''Test'''</span></span><br><span class="line"></span><br><span class="line">d_map = &#123;&#125;</span><br><span class="line">d = &#123;<span class="string">'A'</span>:[<span class="string">'B'</span>,<span class="string">'C'</span>],<span class="string">'B'</span>:[<span class="string">'D'</span>,<span class="string">'E'</span>],<span class="string">'C'</span>:[<span class="string">'F'</span>,<span class="string">'G'</span>],<span class="string">'D'</span>:[<span class="string">'H'</span>,<span class="string">'A'</span>],<span class="string">'E'</span>:[<span class="string">'H'</span>,<span class="string">'A'</span>],<span class="string">'F'</span>:[<span class="string">'A'</span>],<span class="string">'G'</span>:[<span class="string">'A'</span>],<span class="string">'H'</span>:[<span class="string">'A'</span>],<span class="string">'P'</span>:[]&#125;</span><br><span class="line"><span class="comment"># d = &#123;'A':['B'],'B':['A','C'],'C':['B']&#125;</span></span><br><span class="line">g = Graph(d)</span><br><span class="line">pg = PageRank(g)</span><br><span class="line">pg.propagate_transition()</span><br><span class="line">print(pg.pr_value)</span><br></pre></td></tr></table></figure>
<h3 id="Topic-Specific-PageRank"><a href="#Topic-Specific-PageRank" class="headerlink" title="Topic Specific PageRank"></a>Topic Specific PageRank</h3><p>Start at a random page in the topic and also end at a random page in the topic. \(\vec{x}_{sports}\) is a topic specific pagerank vector where for pages belongs to sports, there is a topic specific pagerank and pagerank of other pages are 0.</p>
<p>Calculate the PageRank of a page with regard to topics. The only difference is that the teleportation is different. The naive teleport go to a node in the graph with probability 1/N. The teleportation of topic specific pagerank is to go to a page in a specific topic with probability alpha. Given the number of pages S in the topic T, the probability is \(\frac{1}{S}\) rather than \(\frac{1}{N}\).</p>
<p>i.e.:<br>Let s be [A,B,C,D]. A belongs to topic 1. B,C and D belongs to topic 2.<br>When calculating the topic specific pagerank, for topic 2, the formula is still \(\vec{x}P^t\). But here \(P\) for topic 2 is \(\alpha M + (1-\alpha)\frac{1}{S}\) rather than     \(\alpha M + (1-\alpha)\frac{1}{N}\). Actually the formula for topic 2 is \(\alpha M + (1-\alpha)[0,\frac{1}{3},\frac{1}{3},\frac{1}{3}]\) and in normal PageRank it should be \(\alpha M + (1-\alpha)[\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]\).</p>
<h3 id="Personalized-PageRank"><a href="#Personalized-PageRank" class="headerlink" title="Personalized PageRank"></a>Personalized PageRank</h3><p>Now the difference is still the teleportation. The teleportation is tailored according to the users’ interest. Let’s assume the interest of a person is sports and finance. Their weights are 0.8,0.2 specifically. The teleportation is to go to a topic with the corresponding weight and then a page in the specific topic with probability alpha. This could also be viewed as a linear combination of topic specific vectors. In this example, the formula would be \(\alpha M + (1-\alpha) [0.8\vec{x_{sports}} + 0.2\vec{x_{finance}}]\).</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="http://www.cs.cornell.edu/home/kleinber/networks-book/" target="_blank" rel="noopener">Networks, Crowds, and Markets: Reasoning About a Highly Connected World</a></p>
<p><a href="https://nlp.stanford.edu/IR-book/" target="_blank" rel="noopener">Introduction to Information Retrieval</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/13/Parsing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/13/Parsing/" itemprop="url">Parsing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-13T14:44:51+08:00">
                2018-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p><strong>Context-Free Grammar(CFG)</strong></p>
<p>The symbols that correspond to words in the language (“the”, “nightclub”) are called <strong>terminal</strong> symbols. They are left nodes in a parse tree. </p>
<p>The lexicon is the set of rules that introduce these terminal symbols. </p>
<p>The symbols that express abstractions over these terminals are called <strong>non-terminals</strong>. Ther are the non-left  </p>
<p>Each grammar must have one designated start symbol, which is often called S &lt;\s&gt;.</p>
<h2 id="Formal-Definition-of-CFG"><a href="#Formal-Definition-of-CFG" class="headerlink" title="Formal Definition of CFG"></a>Formal Definition of CFG</h2><p>G=(T,N,S,R)</p>
<ul>
<li>T is a set of terminal symbols.</li>
<li>N is a set of nonterminal symbols</li>
<li>S is the start symbol (S \(\in\) N)</li>
<li>R is a set of rules/productions of the form X -&gt; \(\gamma\).</li>
</ul>
<p>A grammar G generates a language L.</p>
<h2 id="Conversion-to-CNF"><a href="#Conversion-to-CNF" class="headerlink" title="Conversion to CNF"></a>Conversion to CNF</h2><p><strong>Chomsky Normal Form(CNF)</strong>: for each non-terminal, only two non-ternimal or single terminal could be derived from it.</p>
<p><strong>Unit Production</strong>: Rules with a single non-terminal on the right.</p>
<p>three situations needed to address in any generic grammar: </p>
<ul>
<li>rules that mix terminals with non-terminals on the right-hand side</li>
<li>rules that have a single non-terminal on the right-hand side</li>
<li>rules in which the length of the right-hand side is greater than 2</li>
</ul>
<p>rules that mix terminals and non-terminals is to introduce a new dummy non-terminal to substitute the original terminal. For example, a rule for an infinitive verb phrase such as INF-VP → to VP would be replaced by the<br>two rules INF-VP → TO VP and TO → to.</p>
<p>To eliminate unit production:if A<br>-&gt;&gt; B by a chain of one or more unit productions and B → \(\gamma\) is a non-unit production in our grammar, then we add A → γ for each such rule in the grammar and discard all the intervening unit productions. </p>
<p>Rules with right-hand sides longer than 2 are normalized through the introduction<br>of new non-terminals that substitute the longer sequences iteratively.</p>
<p>i.e.: A → B C D. After transformation, A → X D and X -&gt;B C</p>
<p>The entire conversion process can be summarized as follows:</p>
<ul>
<li>Copy all conforming rules to the new grammar unchanged.</li>
<li>Convert terminals within rules to dummy non-terminals.</li>
<li>Convert unit-productions.</li>
<li>Make all rules binary and add them to new grammar.</li>
</ul>
<h3 id="Learn-PCFG-Rules"><a href="#Learn-PCFG-Rules" class="headerlink" title="Learn PCFG Rules"></a>Learn PCFG Rules</h3><p><strong>Count-based</strong></p>
<p>For unambiguous parsing, just count each rule showed in the resulting parsing tree and get the probability like language model.</p>
<p>For ambiguous parsing, initialize the parsing s.t. each rule is assigned the equal probability. And then parse the sentece an calculate probability of each parsing. Update the weight. Iteratively doing this, an extension of EM algorithm.</p>
<h3 id="Problems-of-PCFG"><a href="#Problems-of-PCFG" class="headerlink" title="Problems of PCFG"></a>Problems of PCFG</h3><ul>
<li>Lack of Sensitivity to Lexical Dependencies</li>
<li>Independence Assumptions Miss Structural Dependencies Between Rules. For different location, each rule may be expanded differently.</li>
</ul>
<h3 id="Improving-PCFGs-by-Splitting-Non-Terminals"><a href="#Improving-PCFGs-by-Splitting-Non-Terminals" class="headerlink" title="Improving PCFGs by Splitting Non-Terminals"></a>Improving PCFGs by Splitting Non-Terminals</h3><ul>
<li>split the NP non-terminal into two versions: one for subjects, one for objects.</li>
<li>parent annotation</li>
</ul>
<p>This method also induce new problem. More rules lead to less training which may cause overfitting.</p>
<p>Modern models automatically search for the optimal splits. The split and merge algorithm  for example, starts with a simple X-bar grammar, alternately splits the non-terminals, and merges non-terminals, finding the set of annotated nodes that maximizes the likelihood of the training set treebank.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/13/CKY/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/13/CKY/" itemprop="url">CKY</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-13T12:38:09+08:00">
                2018-07-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>CKY is to initialize a matrix and fill in the upper right triangular matrix. Each word is filled into the matrix from left to right and bottom to up. </p>
<p>There is a probability distribution about contituent for each word. And then the matrix is filled up using viterbi algorithm. Take the max probability.</p>
<p>Transfer the rules to binary reduce the complexity.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/09/Language-Model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/09/Language-Model/" itemprop="url">Language_Model</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-09T20:57:26+08:00">
                2018-07-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Language model is able to capture information and predict next word. It is aware of world information.</p>
<h2 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h2><p>$$p(w_i) = \frac{c(w_i)}{N}$$</p>
<p>\(c(w_i)\) is the count of the word. N is the total number of word.</p>
<h2 id="Bigram"><a href="#Bigram" class="headerlink" title="Bigram"></a>Bigram</h2><p>$$p(w_i|w_{i-1}) = \frac{c(w_iw_{i-1})}{c(w_{i-1})}$$</p>
<h2 id="N-Gram"><a href="#N-Gram" class="headerlink" title="N-Gram"></a>N-Gram</h2><p>Given the previous N words, estimate the probability of the current word.</p>
<p>$$p(w_i|w_{i-N-1}^{N-1}) = \frac{c(w_iw_{i-N-1}^{N-1})}{c(w_{i-N-1}^{N-1})}$$</p>
<h2 id="Deal-with-Unknown-Words"><a href="#Deal-with-Unknown-Words" class="headerlink" title="Deal with Unknown Words"></a>Deal with Unknown Words</h2><p>There are two common ways to train the probabilities of the unknown word model <unk>. </unk></p>
<p>The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance:</p>
<ul>
<li>Choose a vocabulary (word list) that is fixed in advance.</li>
<li>Convert in the training set any word that is not in this set to the unknown word token <unk> in a text normalization step.</unk></li>
<li>Estimate the probabilities for <unk> from its counts just like any other regular word in the training set.</unk></li>
</ul>
<p>The second alternative, replacing words in the training data by <unk> based on their frequency.<br>i.e.: replace all words by <unk> that occur fewer than n times in the training set or choose the top V words by frequency and replace the rest by UNK. </unk></unk></p>
<p>The third way:<br>$$p(w_i)=\lambda_1p_{ML}(w_i)+(1-\lambda_1)\frac{1}{N}$$</p>
<p>Save some probability for unknown words \((\lambda_{unk} = 1-\lambda_1)\). This is in fact another kind of smoothing. Discount the probability of known words and assign to the unknown words.<br> \(\lambda\) is a hyperparameter. It is 0.95 by default. N is the total number of words.</p>
<h2 id="Coverage"><a href="#Coverage" class="headerlink" title="Coverage"></a>Coverage</h2><p>The percentage of known words in the corpus. Usually omit the symbol of the end of a sentence .</p>
<h2 id="Entropy-of-a-sentence"><a href="#Entropy-of-a-sentence" class="headerlink" title="Entropy of a sentence"></a>Entropy of a sentence</h2><p>$$H(S)= -\frac{1}{N}\sum_{w\in S}\log_2 P(w)$$</p>
<h3 id="State-of-the-art-word-embeddings-or-word-tokens"><a href="#State-of-the-art-word-embeddings-or-word-tokens" class="headerlink" title="State-of-the-art word embeddings or word tokens"></a>State-of-the-art word embeddings or word tokens</h3><p>word embeddings or word tokens could be a CNN over a sequence of character embeddings or simply a token embedding.</p>
<h2 id="BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding"><a href="#BERT-Pre-training-of-Deep-Bidirectional-Transformers-for-Language-Understanding" class="headerlink" title="BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"></a>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h2><h2 id="Universal-Language-Model-Fine-tuning-ULMFiT"><a href="#Universal-Language-Model-Fine-tuning-ULMFiT" class="headerlink" title="Universal Language Model Fine-tuning (ULMFiT)"></a>Universal Language Model Fine-tuning (ULMFiT)</h2><p>First pre-train a language model, the key concept is how to fine-tune. </p>
<ol>
<li><p>fine-tune target task language model</p>
<ul>
<li>Discriminative fine-tuning. Each layer is updated separately with different learning rate. Best practice is to choose learning rate by only fine-tuning the last layer. And decrease the selected learning rate layer by layer.</li>
<li>Slanted triangular learning rates. Quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. First increase learning rate to make the model converge to a suitable region. And then decrease the learning rate to fine-tune the model.</li>
</ul>
</li>
<li><p>Target task classifier fine-tuning</p>
<ul>
<li>Concat pooling: maxpool and mean pool all hidden states, and do concatenation.</li>
<li>Gradual unfreezing: unfreeze layer one-by-one and do fine-tuning.</li>
</ul>
</li>
</ol>
<h2 id="ELMO"><a href="#ELMO" class="headerlink" title="ELMO"></a>ELMO</h2><p>ELMO apply a bidirectional RNN to compute the hidden states that are used to predict next words. Here bidirectional means the final output to the softmax is concat of two language model outputs. So it is indeed two single directional language models rather than true bidirectional language models like BERT. For example, “i like machine learning and natural language processing”. When predicting the word “learning”, the left LSTM computes information form “i” to “machine” while the right LSTM computes information from “processing” to “and”. The input to softmax is concat of those two feature vectors.</p>
<p>The final word vector of each word is concat of the original word vectors(the first hidden layer) and all hidden states in the middle bidirecitonal RNN.</p>
<p>The word vectors of ELMO could be task-specific. It is a linear combination of all hidden states in the middle bidirecitonal RNN. The weight for each hidden state is different for different tasks. </p>
<h2 id="TagLM"><a href="#TagLM" class="headerlink" title="TagLM"></a>TagLM</h2><p>将预训练过程中隐藏层的输出特征和任务过程中隐藏层的特征concat到一起，作为最后模型决策的输入特征，显著提升任务效果。<br>concat hidden state of last layer from two direction of RNN as a kind of features. After that, concat hidden states which is the output of RNN during sequence labeling and feed to another RNN as input. This actually enrich the features.<br><img src="https://res.cloudinary.com/leemasterk/image/upload/v1545457413/WX20181222-134300_2x.png" alt="cmd-markdown-logo"></p>
<h3 id="Downstream-tasks"><a href="#Downstream-tasks" class="headerlink" title="Downstream tasks"></a>Downstream tasks</h3><p>‘’‘Word embeddings are nowadays pervasive on a wide spectrum of Natural Language Processing (NLP) and Natural Language Understanding (NLU) applications. These word representations improved downstream tasks in many domains such as machine translation, syntactic parsing, text classification, and machine comprehension, among other’‘’<br>Downstream tasks refer to any task in NLP like text classification, QA and so on.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://arxiv.org/pdf/1705.00108.pdf" target="_blank" rel="noopener"> TagLM: Semi-supervised sequence tagging with bidirectional language models</a></p>
<p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener"> ELMO: Deep contextualized word representations</a></p>
<p><a href="https://arxiv.org/pdf/1802.05365.pdf" target="_blank" rel="noopener"> ULMFiT: Universal Language Model Fine-tuning for Text Classification</a></p>
<p><a href="https://arxiv.org/pdf/1810.04805.pdf" target="_blank" rel="noopener"> BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/06/Word_Senses/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/06/Word_Senses/" itemprop="url">Word_Senses</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-06T12:43:04+08:00">
                2018-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h1 id="Word-sense-disambiguation-WSD"><a href="#Word-sense-disambiguation-WSD" class="headerlink" title="Word sense disambiguation(WSD)"></a>Word sense disambiguation(WSD)</h1><p>Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the performance of tasks such as question answering and machine translation.</p>
<p>Two kinds of WSD tasks:</p>
<ul>
<li>lexical sample</li>
<li>all-words</li>
</ul>
<p>Lexical sample: given a set of target words and with an inventory of senses from some lexicon. Supervised learning is employed to solve this problem at usual. Hand-label a small set of words and corresponding sense, and feed into a classifier.</p>
<p>All-words: given entire texts and a lexicon that contains an inventory of senses for each entry. The objective is to disambiguate every content word in the text. It is impractical to train a classifier for each term because the number of words is large.</p>
<h3 id="Supervised-Learning-Approach"><a href="#Supervised-Learning-Approach" class="headerlink" title="Supervised Learning Approach"></a>Supervised Learning Approach</h3><ol>
<li>Feature Vector Extraction. Process the sentence in a context window to extract feature vector.</li>
</ol>
<p>There are two kinds of feature vector:</p>
<ul>
<li>collocation features. Multiple words in a position-specific relationship to a target word. It is effective at encoding local lexical and grammatical information that usually isolate a given sense. High performing systems generally use POS tags and word collocations of length 1,2 and 3 from a window of words 3.</li>
<li>bag-of-words: neglecting the position information, just record number of context words in a context window. The context words could be pre-selected(frequent used words) given a target word.</li>
</ul>
<p><strong>Evaluation</strong>: most frequent sense as baseline. Most freauent sense usually choose the most frequently used sense given a context.</p>
<h3 id="Dictionary-and-Thesaurus"><a href="#Dictionary-and-Thesaurus" class="headerlink" title="Dictionary and Thesaurus"></a>Dictionary and Thesaurus</h3><h5 id="The-Lesk-Algorithm"><a href="#The-Lesk-Algorithm" class="headerlink" title="The Lesk Algorithm"></a>The Lesk Algorithm</h5><p>It is a family of dictionary-based algorithms.<br>Simplified algorithm compute the number of overlapped words between a given context and all of the word senses. Output the sense with the most overlapped words.</p>
<p>Original Lesk Algorithm compute the number of overlapped words between sense of each context word and the sense of the target word.<br>i.e.: Bank is a financial institute for deposit.<br>Simplified algorithm compute the number of overlapping words between each sense and the context [financial,institute, deposit] and output the sense with the most overlapped words.<br>The original algorithm compute all senses with all senses of each word in the context  [financial,institute, deposit] and output the sense with the most overlapped words.</p>
<p><strong>Notes</strong>:stop-words are not taken into consideration.</p>
<p>Problem: Entries of the dictionary for the target word are not enough which reduce the chance of overlapping.</p>
<p>Solution: To expand the sense, include senses of similar words which may increase the number of overlapping words.</p>
<p>The best solution is to utilize sense-tagged corpus, which is so called <strong><strong>Corpus Lesk Algorithm</strong></strong>. To expand the signature for a word sense, add all word senses with the same label. i.e.: add word sense of large, huge to big. Instead of employ a stop-list, the algorithm apply IDF to reduce the weight of function words like the,of. This algorithm is usually a baseline.</p>
<p>The bag-of-words approach could be improved by combining the Lesk algorithm. The glosses and example sentences for the target sense in WordNet would be used as words features along with <strong>sense-tagged</strong> corpus like SemCor.</p>
<h5 id="Graph-based-Methods"><a href="#Graph-based-Methods" class="headerlink" title="Graph-based Methods"></a>Graph-based Methods</h5><p>A graph is employed to represent words. Each sense is a node and relations between senses are edges. It is usually an undirected graph.</p>
<p>The correct sense is the one that is central in the graph. Use degree to decide centrality.</p>
<p>Another approach is to assign probability to nodes according to personalized page rank.</p>
<p><strong>Problem of both supervised and dictionary-based approach</strong>: need hand-built resouces</p>
<h3 id="Semi-Supervised-Bootstrapping"><a href="#Semi-Supervised-Bootstrapping" class="headerlink" title="Semi-Supervised: Bootstrapping"></a>Semi-Supervised: Bootstrapping</h3><ol>
<li>Use a small set of labeled data to train the classifier. </li>
<li>Do classification.</li>
<li>Select the result with high confidence and add to the training set.</li>
<li>Train the classifier and repeat step 2 and 3.</li>
<li>Stop until there is no untagged data left or reach a specific error rate threshold.</li>
</ol>
<p>To construct the training set, hand-label it or use heuristic to help.</p>
<p>Heuristic:</p>
<ul>
<li><strong>one sense per collocation</strong>: certain words or phrases strongly indicates a specific sense.</li>
<li><strong>one sense per discourse</strong>:a particular word appearing multiple times in a text or discourse often appeared with the same sense. This heuristic seems to hold better for coarse-grained senses and particularly for cases of homonymy rather than polysemy.</li>
</ul>
<h3 id="Unsupervised-Approach-Word-Sense-Induction-WSI"><a href="#Unsupervised-Approach-Word-Sense-Induction-WSI" class="headerlink" title="Unsupervised Approach:Word Sense Induction(WSI)"></a>Unsupervised Approach:Word Sense Induction(WSI)</h3><p>It is actually a clustering of word senses. </p>
<ol>
<li>For each token(sense) of a word in the corpus, use context word vector \(\vec{c}\) to represent it. </li>
<li>Do clustering s.t. there is N clusters and each token \(\vec{c}\) is assigned to a cluster. Each cluster defines a sense.</li>
<li>Recompute the center s.t. the center vector \(\vec{s_j}\) represent a sense.</li>
</ol>
<p>To do word sense disambiguation:</p>
<ol>
<li>compute context word vector \(\vec{c}\) to represent the sense of the word. </li>
<li>Do clustering and assign the context vector to a cluster.</li>
</ol>
<p>All we need is a clustering algorithm and a distance metric between vectors.</p>
<p>A frequently used technique in language applications is known as agglomerative<br>clustering.</p>
<p>Recent algorithms have also used topic modeling algorithms like Latent Dirichlet Allocation (LDA), another way to learn clusters of words based on their distributions.</p>
<p>It is fair to say that no evaluation metric for this task has yet become standard.</p>
<h1 id="Word-Similarity-Thesaurus-Methods"><a href="#Word-Similarity-Thesaurus-Methods" class="headerlink" title="Word Similarity(Thesaurus Methods)"></a>Word Similarity(Thesaurus Methods)</h1><p>Word similarity refers to meaning(synonyms).<br>Word relatedness characterizes relationships(antonyms,synonyms).</p>
<h5 id="Path-length-based-Approach"><a href="#Path-length-based-Approach" class="headerlink" title="Path-length based Approach"></a>Path-length based Approach</h5><p>Thesaurus could be viewed as a graph.</p>
<p>The simplest thesaurus-based algorithms use the edge between words to measure similarity.</p>
<p>pathlen(c1, c2) = 1 + edges in the shortest path between the sense nodes c1 and c2.</p>
<p>path-length based similarity:<br>$$sim_{path}(c_1,c_2)=\frac{1}{pathlen(c_1, c_2)}$$</p>
<p>Path lengths are not the same in different level of the hierarchy.<br>It is possible to refine path-based algorithms with normalizations based on depth in the hierarchy. In general an approach that independently represent the distance associated with each edge it is preferred.</p>
<p>For data without sense tag(words are not presented according to their senses), measure similarity according to senses of words. If there is a pair of senses is similar, then two words are similar.</p>
<p>$$word_{sim}(w_1,w_2)=\max_{c_1\in senses(w_1)\  c_2\in senses(w_2)} sim(c_1, c_2)$$</p>
<h5 id="information-content-word-similarity-algorithms"><a href="#information-content-word-similarity-algorithms" class="headerlink" title="information-content word-similarity algorithms"></a>information-content word-similarity algorithms</h5><p>still rely on the structure of the thesaurus but also add probabilistic information derived from a corpus.</p>
<p>The corpus is represented as a set of concepts. And it is represented as a tree  structure. The leaf nodes are basic concepts and parent nodes contain the children which is a superset of concepts. Each node is assigned a probability. So p(root) = 1.</p>
<p>$$P(c) = \frac{\sum_{w\in words(c)}count(w)}{N}$$</p>
<p>c is a specific concept. The probability of a concept is number of words in the concept divided by total number of words.</p>
<p><strong>information content</strong>: information content (IC) of a concept c</p>
<p>$$IC(c) = −\log P(c) $$</p>
<p>the lowest common subsumer(LCS) of two concepts:<br>LCS(c1, c2) = the lowest common subsumer,the lowest node in the hierarchy that subsumes both c1 and c2.(The lowest parent node of c1 and c2)</p>
<p><strong>Resnik similarity</strong>: similarity between two words is related to their common information; the more two words have in common, the more similar they are. Resnik proposes to estimate the common amount of information by the information content of the lowest common subsumer of the two nodes. </p>
<p>$$sim_{Resnik}(c_1,c_2)=-\log P(LCS(c_1,c_2))$$</p>
<p><strong>Lin similarity</strong></p>
<p>Similarity Theorem: The similarity between A and B is measured by the ratio<br>between the amount of information needed to state the commonality of A and<br>B and the information needed to fully describe what A and B are.<br>$$sim_{Lin}(A,B)=\frac{common(A,B)}{description(A,B)}$$</p>
<p>the information in common between two concepts is twice the information in the lowest common subsumer LCS(c1, c2). Adding in the above definitions of the information content of thesaurus concepts<br>$$sim_{Lin}(C_1,C_2)=\frac{2\times \log P(LCS(c_1,c_2))}{\log P(c_1)+\log P(c_2)}$$</p>
<p><strong>Jiang-Conrath distance</strong> express distance rather than similarity, work as well as or better than all the other thesaurus-based methods:</p>
<p>$$dist_{JC}(c_1, c_2) = 2\times \log P(LCS(c_1, c_2))−(\log P(c_1) +\log P(c_2)) $$</p>
<p>It could be transferred to similarity by take reciprocal.<br>$$sim_{JC}=\frac{1}{dist_{JC}}$$</p>
<p><strong>dictionary-based method</strong>: This method makes use of glosses, which are, in general, a property of dictionaries rather than thesauruses. Two concepts/senses are similar if their glosses contain overlapping words.</p>
<p>For each n-word phrase that occurs in both glosses, Extended Lesk adds in a<br>score of \(n^2\).(the relation is non-linear because of the Zipfian relationship between lengths of phrases and their corpus frequencies; longer overlaps are rare, so they should be weighted more heavily)</p>
<p>Given such an overlap function, when comparing two concepts (synsets), Extended<br>Lesk not only looks for overlap between their glosses but also between the<br>glosses of the senses that are hypernyms, hyponyms, meronyms, and other relations<br>of the two concepts. </p>
<p>If we just considered hyponyms and defined<br>gloss(hypo(A)) as the concatenation of all the glosses of all the hyponym senses of<br>A, the total relatedness between two concepts A and B might be</p>
<p>$$similarity(A,B) = overlap(gloss(A), gloss(B))+overlap(gloss(hypo(A)), gloss(hypo(B)))+overlap(gloss(A), gloss(hypo(B)))+overlap(gloss(hypo(A)),gloss(B))$$</p>
<p>Extended Lesk overlap measure:</p>
<p>$$sim_{eLESK}(c_1,c_2) =\sum_{r,q\in RELS}overlap(gloss(r(c_1)),gloss(q(c_2)))$$</p>
<p>RELS is the set of possible relations whose glosses we compare. Possible relations are  like hypernyms, hyponyms, meronyms, and other relations of the two concepts.</p>
<h2 id="Evaluating-Thesaurus-Based-Similarity"><a href="#Evaluating-Thesaurus-Based-Similarity" class="headerlink" title="Evaluating Thesaurus-Based Similarity"></a>Evaluating Thesaurus-Based Similarity</h2><p>The most common intrinsic evaluation metric computes the correlation coefficient between an algorithm’s word similarity scores and word similarity ratings assigned by humans. </p>
<p>Zipf’s law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.The most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://en.wikipedia.org/wiki/Zipf%27s_law" target="_blank" rel="noopener">Zipf’s law</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/05/vector-semantics/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/07/05/vector-semantics/" itemprop="url">vector_semantics</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-05T18:56:55+08:00">
                2018-07-05
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Pointwise-Mutual-Information-PMI"><a href="#Pointwise-Mutual-Information-PMI" class="headerlink" title="Pointwise Mutual Information (PMI)"></a>Pointwise Mutual Information (PMI)</h2><p>Normal count-based word-context matrix would provide a lot of useless information. i.e.: the entry (apple, the) is large but provide useless information.</p>
<p>Instead we’d like context words that are particularly informative about the target<br>word. The best weighting or measure of association between words should tell us<br>how much more often than chance the two words co-occur.</p>
<p>mutual information </p>
<p>The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent:<br>$$I(x,y)=\log_2\frac{P(x, y)}{p(x)p(y)}$$</p>
<p>The pointwise mutual information of a word w and a cotext word c is:<br>$$PMI(w,c)=\log_2\frac{P(w, c)}{p(w)p(c)}$$</p>
<p>The ratio gives us an estimate of<br>how much more the target and feature co-occur actually than just by chance.</p>
<p><strong>Positive PMI</strong>: use 0 to substitute negetive PMI. It is used commonly because negative PMI is unreliable. Negative PMI means that the two words co-occur less than expectation which could be due to the small corpora.</p>
<p>A normal count-based co-occurrence matrix could be turned into PPMI matrix.</p>
<p><strong>Problem</strong>:biased toward infrequent words because the demoninator which is the frequency of the word is low.</p>
<p>To solve this problem,change the computation for P(c).</p>
<p>$$PPMI_\alpha(w,c)=\max(\log_2\frac{P(w, c)}{p(w)p_\alpha(c)},0)$$<br>$$p_\alpha(c) = \frac{count(c)^\alpha}{\sum_ccount(c)^\alpha}$$</p>
<p>Usually \(\alpha\)=0.75 drawing on a similar weighting used for skipgrams.This increases the probability assigned to rare<br>contexts and solve this problem.</p>
<p>Another solutio is Laplace smoothing.</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>IDF is used to give a higher weight to rare words because they are likely to present more information.</p>
<p>$$idf_i = \log (\frac{N}{df_i})$$</p>
<p>Combining term frequency with IDF results in a scheme known as tf-idf weighting of the value for word i in document j,\(w_{ij}\):\</p>
<p>$$w_{ij}=tf_{ij}idf_i$$</p>
<p>The tf-idf weighting is by far the dominant way of weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing including summarization.</p>
<h3 id="t-test"><a href="#t-test" class="headerlink" title="t-test"></a>t-test</h3><h2 id="Similarity-Measure-for-vectors-binary"><a href="#Similarity-Measure-for-vectors-binary" class="headerlink" title="Similarity Measure for vectors(binary)"></a>Similarity Measure for vectors(binary)</h2><p><strong>Jaccard similarity</strong>:<br>$$sim_{Jaccard}=\frac{\sum_{i=1}^N\min(v_i,w_i)}{\sum_{i=1}^N\max(v_i,w_i)}$$</p>
<p>$$J(A,B)=\frac{|A\cap B|}{|A\cup B|}$$</p>
<p>The min of \(v_i\) and \(w_i\)means if a feature exists in one vector but not the other vector, the result would be zero. The<br>denominator can be viewed as a normalizing factor.</p>
<p><strong>Dice measure</strong>:<br>$$sim_{Dice}=\frac{2 \times \sum_{i=1}^N\min(v_i,w_i)}{\sum_{i=1}^N(v_i+w_i)}$$</p>
<p><strong>KL divergence</strong><br>if two vectors, \(\vec{v}\) and \(\vec{w}\), each express a probability<br>distribution (their values sum to one), then they are are similar to the extent that these probability distributions are similar.</p>
<p>Given two probability distribution or vectors P and Q:</p>
<p>$$D(P||Q) = \sum_xP(x)\log \frac{P(x)}<br>{Q(x)}$$</p>
<p>Unfortunately, the KL-divergence is undefined when Q(x) = 0 and P(x) \(\neq\) 0, which is a problem since these word-distribution vectors are generally quite sparse.</p>
<p>Jensen-Shannon divergence solves this problem.<br>$$JS(P||Q)=D(P|\frac{Q+P}{2})+D(Q|\frac{Q+P}{2})$$</p>
<h3 id="Using-syntax-to-define-a-word’s-context"><a href="#Using-syntax-to-define-a-word’s-context" class="headerlink" title="Using syntax to define a word’s context"></a>Using syntax to define a word’s context</h3><p>Instead of defining a word’s context by nearby words, we could instead define it by<br>the syntactic relations of these neighboring words. i.e.: the word duty could be represented by a bunch of combinations like additional,administrative(adj) or assert, assign(verb). </p>
<h3 id="Evaluating-Vector-Models"><a href="#Evaluating-Vector-Models" class="headerlink" title="Evaluating Vector Models"></a>Evaluating Vector Models</h3><p>test their performance on similarity, and in particular on computing the correlation between an algorithm’s word similarity scores and word similarity ratings<br>assigned by humans.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/6/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/">6</a><span class="page-number current">7</span><a class="page-number" href="/page/8/">8</a><a class="page-number" href="/page/9/">9</a><a class="extend next" rel="next" href="/page/8/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="MK_LEE" />
            
              <p class="site-author-name" itemprop="name">MK_LEE</p>
              <p class="site-description motion-element" itemprop="description">Passionate about NLP</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">89</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MK_LEE</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
