<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Passionate about NLP">
<meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="http://yoursite.com/page/5/index.html">
<meta property="og:site_name">
<meta property="og:description" content="Passionate about NLP">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title">
<meta name="twitter:description" content="Passionate about NLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'PJWL3PD75I',
      apiKey: '5589833aa2fa703729b4e7e939ac7dec',
      indexName: 'test_index',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/5/"/>





  <title></title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title"></span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/Logistic-Regression/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/29/Logistic-Regression/" itemprop="url">Logistic_Regression</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T14:35:03+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Binomial-Logistic-Regression"><a href="#Binomial-Logistic-Regression" class="headerlink" title="Binomial Logistic Regression"></a>Binomial Logistic Regression</h2><p>$$p(1|x) = \frac{\exp(\vec{w} \cdot \vec{x})}{1+\exp(\vec{w} \cdot \vec{x})}$$</p>
<p>$$p(0|x) = \frac{1}{1+\exp(\vec{w} \cdot \vec{x})}$$</p>
<p><strong>odds</strong>: the probability that an event occur divided by the probability that an event doesn’t occur. \(\frac{p}{1-p}\)</p>
<p><strong>log odds</strong>: take log of odds. \(\log \frac{p}{1-p}\). In terms of logistic regression, \(\log \frac{p}{1-p}=\vec{w} \cdot \vec{x}\).</p>
<p>Actually logistic regression turns the linear function \(\vec{w} \cdot \vec{x}\) into probability distribution. So logistic regression is acutually a linear classification model with some operations to turn the output into probability distribution. The final output label is the one with the highest value.</p>
<h3 id="Parameters-Estimation"><a href="#Parameters-Estimation" class="headerlink" title="Parameters Estimation"></a>Parameters Estimation</h3><p>Use gradient descent.</p>
<h2 id="Multinomial-Logistic-Regression"><a href="#Multinomial-Logistic-Regression" class="headerlink" title="Multinomial Logistic Regression"></a>Multinomial Logistic Regression</h2><p>multinomial logistic regression, sometimes referred to within language processing as MaxEnt entropy modeling, MaxEnt for short. Logistic regression belongs to the family of classifiers known as the exponential or log-linear classifiers.<br>Technically, logistic regression refers to a classifier that classifies<br>an observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes</p>
<p>First, wrap the exp function around the weight-feature dot-product \(w\cdot f\) , which will make the values positive.</p>
<p>A discriminative model discriminative<br>model takes this direct approach, computing P(y|x) by discriminating among the different possible values of the class y rather than first computing a likelihood.</p>
<p>Logistic Regression </p>
<p>$$p(c|x) = \frac{\exp(\sum_iw_if_i(c,x))}{\sum_{c’ \in C}\exp(\sum_{i=1}^N w_if_i(c’,x)}$$</p>
<p>In vector from:</p>
<p>$$p(c|x) = \frac{\exp(\vec{w} \cdot \vec{f(c,x)})}{\sum_{c’ \in C}\exp(\vec{w}\cdot \vec{f(c,x)})}$$</p>
<p>Here c actually range from 1 to N+1. And by default \(\exp(\sum_{i=1}^N w_if_i(N+1,x)=0\)</p>
<p>exp is to make to result greater than 0.<br>\(f_i(c, x)\), meaning feature i for a particular class c for a given observation x. In NLP, x is usually a document. Feature i usually means whether a specific word \(x_i\) has shown in the document for a particular class. \(w_i\) is the weight corresponding to the feature i. For example, in sentiment analysis, classes are 0 and 1 corresponding to positive and negative. The word best is a feature, namely \(\ f_1\), its value for class 1 coube be +10 and -10 for class 0.</p>
<p>Computing the actual probability rather than just choosing the best class is useful when the classifier is embedded in a larger system. Otherwise compute the nominater is sufficient to do classification. \(\hat{c}={argmax}<em>{c\in C}\sum</em>{i=1}^N w_if_i(c,x)\)</p>
<h3 id="Learning-weights"><a href="#Learning-weights" class="headerlink" title="Learning weights"></a>Learning weights</h3><p>The intuition is to choose weights that make the classes of the training examples more likely. Indeed, logistic regression is trained with conditional maximum likelihood estimation. This means we choose the parameters w that maximize the (log) probability of the y labels in the training data given the observations x.</p>
<p>$$\hat{w} = {argmax}_w\sum_jlogP(y^{(j)}|x^{(j)})$$</p>
<p>It means given a training set, we choose weights that maximize the probability of \(j^{th}\) training example \(x^{(j)}\) is classified to the lable \(y^{(j)}\).</p>
<p>The objective function L that we are maximizing is thus<br>$$L(w) = \frac{\exp(\sum_i^Nw_if_i(y^{(j)},x^{(j)}))}{\sum_{y’ \in Y}\exp(\sum_{i=1}^N w_if_i(y’^{(j)},x^{(j)})}$$</p>
<h3 id="Max-Entropy"><a href="#Max-Entropy" class="headerlink" title="Max Entropy"></a>Max Entropy</h3><p>When there is no training data available, the model assign the same probability to each class. If there is training data, the model try to reach the max entropy with the constraint that satisfy the training data.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://web.stanford.edu/~jurafsky/slp3/" target="_blank" rel="noopener">Speech and Language Processing</a></p>
<p><a href="http://net.pku.edu.cn/~course/cs410/2015/resource/book/2012-book-StatisticalLearning-LH.pdf" target="_blank" rel="noopener">统计学习方法</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/Machine-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/29/Machine-Learning/" itemprop="url">Machine_Learning</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T13:01:38+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>contingency table</strong> consists of 4 entries: True Positive, True Negative, False Positive, False Negative.</p>
<p>Accuracy doesn’t work well when<br>the classes are unbalanced.</p>
<p>Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as<br>$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$</p>
<p>Recall measures the percentage of items actually present in the input that were<br>correctly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as<br>$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$</p>
<p>precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.<br>In practice, we generally combine precision and recall into a single metric called<br>F-measure the F-measure that is defined as:<br>$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$</p>
<p>The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1<br>anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall.</p>
<p>Harmonic mean is used because it is a conservative metric; the harmonic mean of<br>two values is closer to the minimum of the two values than the arithmetic mean is.<br>Thus it weighs the lower of the two numbers more heavily.</p>
<p>In <strong>any-of</strong> or <strong>multi-label</strong> classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d.</p>
<p><strong>one-of</strong> or <strong>multinomial</strong> classification, multinomial classification in which the classes are mutually exclusive and each document or item appears in<br>exactly one class. Build a separate binary classifier trained on positive<br>examples from c and negative examples from all other classes. Now given a test<br>document or item d, we run all the classifiers and choose the label from the classifier with the highest score. </p>
<p>In <strong>macroaveraging</strong>, we compute the performance for each class, and then average over classes. In <strong>microaveraging</strong>, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important.</p>
<p>The only problem with cross-validation is that because all the data is used for<br>testing, we need the whole corpus to be blind; we can’t examine any of the data<br>to suggest possible features and in general see what’s going on. But looking at the<br>corpus is often important for designing the system. For this reason, it is common<br>to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set.</p>
<h3 id="regularization-used-to-penalize-large-weights"><a href="#regularization-used-to-penalize-large-weights" class="headerlink" title="regularization: used to penalize large weights"></a><strong>regularization</strong>: used to penalize large weights</h3><p>L1 regularization is called ‘the lasso’ or lasso regression and L2 regression is called ridge regression, and both are commonly used in language processing. L2 regularization is easier to optimize because of its simple derivative, while L1 regularization is more complex (the derivative of |w| is noncontinuous<br>at zero). </p>
<p>But where L2 prefers weight vectors with many small weights, L1 prefers sparse solutions with some larger weights but many more weights set to zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features.</p>
<p>L2 regularization:$$\sum_{j=1}^Nw_j^2$$</p>
<p>L1 regularization:$$\sum_{j=1}^N|w_j|$$</p>
<p>Both L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplace<br>prior on the weights. L2 regularization corresponds to assuming that weights are<br>distributed according to a gaussian distribution with mean µ = 0.</p>
<p>The regularization technique is useful for avoiding overfitting by removing or downweighting features that are unlikely to generalize well. Many kinds of classifiers including naive Bayes do not have regularization, and so instead feature selection is used to choose the important features and remove the rest. The basis of feature selection is to assign some metric<br>of goodness to each feature, rank the features, and keep the best ones. The number of features to keep is a meta-parameter that can be optimized on a dev set.</p>
<h3 id="Feature-Selection"><a href="#Feature-Selection" class="headerlink" title="Feature Selection"></a>Feature Selection</h3><p>Features are generally ranked by how informative they are about the classification decision. A very common metric is information gain. Information gain tells us how many bits of information the presence of the word gives us for guessing the class, and can be computed as follows (where ci is the ith class and ¯w means that a document does not contain the word \(w^-\)):</p>
<p>While feature selection is important for unregularized classifiers, it is sometimes<br>also used in regularized classifiers in applications where speed is critical, since it is often possible to get equivalent performance with orders of magnitude fewer features.</p>
<p>The overly strong conditional independence assumptions of Naive Bayes mean that if two features are correlated naive Bayes will multiply them, overestimating the evidence. Logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign half the weight to w1 and half to w2.</p>
<p>naive Bayes works extremely well (even better than logistic regression or SVMs) on small datasets or short documents.<br>Furthermore, naive Bayes is easy to implement and very fast to train. Nonetheless, algorithms like logistic regression and SVMs generally work better on larger documents<br>or datasets.</p>
<h3 id="bias-variance-tradeoff"><a href="#bias-variance-tradeoff" class="headerlink" title="bias-variance tradeoff"></a>bias-variance tradeoff</h3><p>The <strong>bias of a classifier</strong> indicates how accurate it is at modeling different training sets.<br>The <strong>variance of a classifier</strong> indicates how much its decisions are affected by small changes<br>in training sets.</p>
<p>Models with <em>low bias</em> (like SVMs with polynomial or RBF kernels) are very accurate at modeling the training data. Models with <em>low variance</em> (like naive<br>Bayes) are likely to come to the same classification decision even from slightly different training data. </p>
<p>low-bias models tend to overfit, and do not generalize well to very different test sets.<br>low-variance models tend to generalize so well that they may not have sufficient accuracy.<br>Thus model trades off bias and variance. Adding more features decreases bias by making it possible to more accurately model the training data, but<br>increases variance because of overfitting. Regularization and feature selection are<br>ways to improve (lower) the variance of classifier by downweighting or removing<br>features that are likely to overfit.</p>
<p>In addition to the choice of a classifier, the key to successful classification is the<br>design of appropriate features. Features are generally designed by examining the<br>training set with an eye to linguistic intuitions and the linguistic literature on the domain. </p>
<p>For some tasks it is especially helpful to build complex features that are combinations<br>of more primitive features. For logistic regression and naive Bayes these<br>combination features or feature interactions have to be designed by hand. </p>
<p><strong>feature interactions</strong><br>Some other machine learning models can automatically model the interactions<br>between features. For tasks where these combinations of features are important<br>(especially when combination of categorical features and real-valued features might<br>be helpful), the most useful classifiers may be such classifiers,including Support<br>SVMs Vector Machines (SVMs) with polynomial or RBF kernels, and random forests</p>
<h3 id="dimensionality-reduction-PCA-SVD"><a href="#dimensionality-reduction-PCA-SVD" class="headerlink" title="dimensionality reduction(PCA,SVD)"></a>dimensionality reduction(PCA,SVD)</h3><p>First rotate the axes of the original dataset into a new space. The new space is chosen so that the highest order dimension<br>captures the most variance in the original dataset, the next dimension captures the next most variance, and so on. In this new space, the data if represented with a smaller number of dimensions and still<br>capture much of the variation in the original data.</p>
<p>For binary classification, sigmoid would suffice the requirement.</p>
<h3 id="Softmax-Logistic-Regression"><a href="#Softmax-Logistic-Regression" class="headerlink" title="Softmax(Logistic Regression)"></a>Softmax(Logistic Regression)</h3><p>Linear decision boundary.</p>
<h3 id="Bias-Term"><a href="#Bias-Term" class="headerlink" title="Bias Term"></a>Bias Term</h3><p>Bias is significant because it allows the activation function to shift left and right. The weight is to change the steepness of the activation function.</p>
<p><a href="https://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks" target="_blank" rel="noopener">Role of Bias in Neural Networks</a></p>
<h3 id="Max-margin-Objective-function"><a href="#Max-margin-Objective-function" class="headerlink" title="Max-margin Objective function"></a>Max-margin Objective function</h3>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/Sentiment-Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/29/Sentiment-Classification/" itemprop="url">Sentiment_Classification_Naive_Bayes</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T10:15:37+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>In most text classification<br>applications, however, using a stop word list doesn’t improve performance,<br>and so it is more common to make use of the entire vocabulary and not use a stop<br>word list</p>
<h3 id="Naive-Bayes-Classifier-for-Sentiment-Analysis"><a href="#Naive-Bayes-Classifier-for-Sentiment-Analysis" class="headerlink" title="Naive Bayes Classifier for Sentiment Analysis"></a>Naive Bayes Classifier for Sentiment Analysis</h3><p>Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well.</p>
<p>The assumption that terms are independent does not hold. But in terms of classification, the class that a document belongs is usually irrelevant to position. This somehow reduce the influence of the dependence.</p>
<p>i.e.: Finance industry is promising in HK.<br>HK finance is booming.</p>
<p>$$C_{NB}=\underset{c\in C}{argmax}\ logP(c)+\sum_{i \in positions}logP(w_i<br>|c)$$</p>
<p>Each word \(w_i\) could be viewed as a feature for a specific document.</p>
<h3 id="Maximum-Likelihood-to-train-Naive-Bayes"><a href="#Maximum-Likelihood-to-train-Naive-Bayes" class="headerlink" title="Maximum Likelihood to train Naive Bayes"></a>Maximum Likelihood to train Naive Bayes</h3><p>$$\hat{p}(c)=\frac{N_c}{N_{doc}}$$</p>
<p>The standard solution for an unknown word that doesn’t show up in the training set is to ignore such words—remove them from the test document and not include any probability for them at all.</p>
<p>Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization to deal with zero count.</p>
<p>$$\hat{P}(w_i|c) = \frac{count(w_i, c) +1}{<br>(\sum_{w\in V}(count(w, c)) +|V|}$$</p>
<p><strong><strong><em>Note that the vocabulary V consists of the union of all the word types in all classes, not just the words in one class. If the denominator is the word in the class \(c_i\), then given a class, the probability of the sentence shows up is 1. That is meaningless.</em></strong></strong> </p>
<h3 id="Binary-Multinominal-Naive-Bayes-The-Bernoulli-model"><a href="#Binary-Multinominal-Naive-Bayes-The-Bernoulli-model" class="headerlink" title="Binary Multinominal Naive Bayes(The Bernoulli model)"></a>Binary Multinominal Naive Bayes(The Bernoulli model)</h3><p>Whether a word occurs or not seems to matter more than its frequency.</p>
<p>The feature in this model is not term frequency anymore. It is whether a word has shown up in the sentence.</p>
<p>Smoothing could also be employed in this model.</p>
<p>First,it remove all duplicate words such that each sentence contains only one distinct word before concatenating them into the single big document. </p>
<p>Second deal with negation during text normalization. Typically adding negation prefix such as NOT_ to every word. i.e., like -&gt; Not_like.</p>
<p>In some situations we might have insufficient labeled training data. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are pre-annotated with positive or negative sentiment.</p>
<h3 id="Naive-Bayes-as-a-Language-Model"><a href="#Naive-Bayes-as-a-Language-Model" class="headerlink" title="Naive Bayes as a Language Model"></a>Naive Bayes as a Language Model</h3><p>If we use only individual word features, and we use all of the words in the text<br>(not a subset), then naive Bayes has an important similarity to language modeling.<br>Specifically, <strong><strong>a naive Bayes model can be viewed as a set of class-specific unigram language models</strong></strong>, in which the model for each class instantiates a unigram language model.</p>
<p><strong>contingency table</strong> consists of 4 entries: True Positive, True Negative, False Positive, False Negative.</p>
<p>Accuracy doesn’t work well when<br>the classes are unbalanced.</p>
<p>Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as<br>$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$</p>
<p>Recall measures the percentage of items actually present in the input that were<br>correctly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as<br>$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$</p>
<p>precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.<br>In practice, we generally combine precision and recall into a single metric called<br>F-measure the F-measure that is defined as:<br>$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$</p>
<p>The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1<br>anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall.</p>
<p>Harmonic mean is used because it is a conservative metric; the harmonic mean of<br>two values is closer to the minimum of the two values than the arithmetic mean is.<br>Thus it weighs the lower of the two numbers more heavily.</p>
<p>In <strong>any-of</strong> or <strong>multi-label</strong> classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d.</p>
<p><strong>one-of</strong> or <strong>multinomial</strong> classification, multinomial classification in which the classes are mutually exclusive and each document or item appears in<br>exactly one class. Build a separate binary classifier trained on positive<br>examples from c and negative examples from all other classes. Now given a test<br>document or item d, we run all the classifiers and choose the label from the classifier with the highest score. </p>
<p>In <strong>macroaveraging</strong>, we compute the performance for each class, and then average over classes. In <strong>microaveraging</strong>, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important.</p>
<p>The only problem with cross-validation is that because all the data is used for<br>testing, we need the whole corpus to be blind; we can’t examine any of the data<br>to suggest possible features and in general see what’s going on. But looking at the<br>corpus is often important for designing the system. For this reason, it is common<br>to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set.</p>
<h5 id="Toy-Model"><a href="#Toy-Model" class="headerlink" title="Toy Model"></a>Toy Model</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> RegexpTokenizer</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processing_raw_text</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    intermediate = sentence.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        sen, label = intermediate[<span class="number">0</span>].strip(), intermediate[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">except</span> IndexError:</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">None</span>,<span class="keyword">None</span>)</span><br><span class="line">    <span class="comment"># print("sen", sen)</span></span><br><span class="line">    <span class="comment"># print(label)</span></span><br><span class="line">    <span class="keyword">return</span> sen, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processing_sentence</span><span class="params">(sentence, tokenizer=None, stop_words=True, punctuation=True, stem=False)</span>:</span></span><br><span class="line">    <span class="string">'''Given tokenizer, please deal with punctuation yourself'''</span></span><br><span class="line">    <span class="keyword">if</span> tokenizer <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> punctuation == <span class="keyword">True</span>:</span><br><span class="line">            tokenizer = RegexpTokenizer(<span class="string">r'\w+'</span>)</span><br><span class="line">            token = tokenizer.tokenize(sentence)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            token = nltk.word_tokenize(sentence)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        token = tokenizer.tokenize(sentence)</span><br><span class="line">    <span class="keyword">if</span> stop_words == <span class="keyword">True</span>:</span><br><span class="line">        sw_l = set(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">        filtered_token = [w <span class="keyword">for</span> w <span class="keyword">in</span> token <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> sw_l]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        filtered_token = token</span><br><span class="line">    <span class="keyword">if</span> stem == <span class="keyword">True</span>:</span><br><span class="line">        porter = nltk.PorterStemmer()</span><br><span class="line">        filtered_token = [porter.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> filtered_token]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filtered_token</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_class_probability</span><span class="params">(label_l)</span>:</span></span><br><span class="line">    label_distinct = set(label_l)</span><br><span class="line">    d = &#123;key: label_l.count(key) <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    d = &#123;key: d[key] / float(sum(d.values())) <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(test_example, class_p, word_p)</span>:</span></span><br><span class="line">    <span class="string">'''predict'''</span></span><br><span class="line">    predict_l = []</span><br><span class="line">    label_distinct = set(label_l)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_example)):</span><br><span class="line">        p_example = &#123;key: class_p[key] <span class="keyword">for</span> key <span class="keyword">in</span> class_p.keys()&#125;</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> test_example[i]:</span><br><span class="line">            <span class="keyword">for</span> lab <span class="keyword">in</span> label_distinct:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    p_example[lab] += np.log(word_p[lab][w])</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    p_example[lab] += np.log(word_p[lab][<span class="string">'unknown'</span>])</span><br><span class="line">                    <span class="comment"># p_example[lab] = float("inf")</span></span><br><span class="line">        predict_l.append(max(p_example, key=p_example.get))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predict_l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_dic_unknown</span><span class="params">(dic, percent=<span class="number">20</span>)</span>:</span>  <span class="comment">#The lowest 20 percent words are regarded as unknown words.</span></span><br><span class="line">    v_l = list(dic.values())</span><br><span class="line">    v_l.sort()</span><br><span class="line">    i = int(len(v_l)*percent/<span class="number">100</span>)</span><br><span class="line">    unknown_count = <span class="number">0</span></span><br><span class="line">    d = copy.deepcopy(dic)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> dic.keys():</span><br><span class="line">        <span class="keyword">if</span> dic[k] &lt;= v_l[i]:</span><br><span class="line">            unknown_count += dic[k]</span><br><span class="line">            <span class="keyword">del</span> d[k]</span><br><span class="line">    d[<span class="string">'unknown'</span>] = unknown_count</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return d</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_k_smoothing</span><span class="params">(dic, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    d = dict(map(<span class="keyword">lambda</span> i: (i[<span class="number">0</span>],(i[<span class="number">1</span>]+k)/float(len(dic.keys())*k+sum(dic.values())) ) ,dic.items()))</span><br><span class="line">    <span class="comment"># print(d)</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_labeled_vocab</span><span class="params">(example_l, label_l, dic, sort=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        smooth=False, deal_with_unknown = True)</span>:</span></span><br><span class="line">    label_distinct = set(label_l)</span><br><span class="line">    label_index = &#123;key: [<span class="number">-1</span>] <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label_l)):</span><br><span class="line">        label_index[label_l[i]].append(i)</span><br><span class="line">    label_index = &#123;k: v[<span class="number">1</span>:] <span class="keyword">for</span> k, v <span class="keyword">in</span> label_index.items()&#125;</span><br><span class="line">    dic = copy.deepcopy(dic)</span><br><span class="line">    <span class="keyword">if</span> deal_with_unknown == <span class="keyword">True</span>:</span><br><span class="line">        dic = copy.deepcopy(process_dic_unknown(dic))</span><br><span class="line"></span><br><span class="line">    d_labeled = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> lab <span class="keyword">in</span> label_distinct:</span><br><span class="line">        d = dic.fromkeys(dic.keys(), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ind <span class="keyword">in</span> label_index[lab]:</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> example_l[ind]:</span><br><span class="line">                <span class="keyword">if</span> w <span class="keyword">in</span> d.keys():</span><br><span class="line">                    d[w] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    d[w] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> sort==<span class="keyword">True</span>:</span><br><span class="line">            d_sorted = &#123;&#125;                   <span class="comment"># sort the key</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> sorted(d.keys()):</span><br><span class="line">                d_sorted[key] = d[key]</span><br><span class="line">            d_labeled[lab] = d_sorted</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            d_labeled[lab] = d</span><br><span class="line">    <span class="keyword">if</span> isinstance(smooth,int):</span><br><span class="line">        d_labeled = dict(map(<span class="keyword">lambda</span> i: (i[<span class="number">0</span>], add_k_smoothing(i[<span class="number">1</span>], smooth)),d_labeled.items()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d_labeled</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_conditional_probability</span><span class="params">(dic)</span>:</span></span><br><span class="line">    <span class="string">'''calculate p(w|c)'''</span></span><br><span class="line">    label_distinct = dic.keys()</span><br><span class="line">    p_dic = &#123;key: &#123;k: dic[key][k] / float(sum(dic[key].values())) <span class="keyword">for</span> k <span class="keyword">in</span> dic[key].keys()&#125;</span><br><span class="line">             <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    <span class="keyword">return</span> p_dic</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_l)</span>:</span></span><br><span class="line">    sen_l = []</span><br><span class="line">    label_l = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> file_l:</span><br><span class="line">        <span class="comment"># print(file)</span></span><br><span class="line">        <span class="keyword">with</span> open(file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            sen = <span class="number">1</span> <span class="comment">#dummy number</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> sen != <span class="string">''</span>:</span><br><span class="line">                sen = f.readline().strip()</span><br><span class="line">                <span class="comment"># print(sen)</span></span><br><span class="line">                <span class="comment"># print(sen)</span></span><br><span class="line">                s,lab = processing_raw_text(sen)</span><br><span class="line">                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                sen_l.append(s)</span><br><span class="line">                label_l.append(lab)</span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">    <span class="comment"># print(len(sen_l))</span></span><br><span class="line">    <span class="keyword">return</span> sen_l, label_l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_train_test_set</span><span class="params">(example, label, percent=<span class="number">0.2</span>)</span>:</span> <span class="comment"># % of data as test set</span></span><br><span class="line">    train_example = copy.deepcopy(example)</span><br><span class="line">    train_label = copy.deepcopy(label)</span><br><span class="line">    k = int(percent * len(example))</span><br><span class="line">    <span class="comment"># print(len(example))</span></span><br><span class="line">    index = random.sample(range(len(example)), k)</span><br><span class="line">    test_example, test_label = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">        test_example.append(example[i])</span><br><span class="line">        test_label.append(label[i])</span><br><span class="line">    index.sort(reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">        train_example.pop(i)</span><br><span class="line">        train_label.pop(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_example, train_label, test_example, test_label</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    file = <span class="string">'/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/imdb_labelled.txt'</span></span><br><span class="line">    file1 = <span class="string">'/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/amazon_cells_labelled.txt'</span></span><br><span class="line">    file2 = <span class="string">'/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/yelp_labelled.txt'</span></span><br><span class="line">    f_l = [file,file1,file2]</span><br><span class="line"></span><br><span class="line">    sen_l, label_l = read_data(f_l)</span><br><span class="line"></span><br><span class="line">    token_l = [processing_sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> sen_l]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filtered_token = list(map(lambda t: [w for w in t if not w in stop_words], token_l))</span></span><br><span class="line">    <span class="comment"># print(token_l)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''build training set'''</span></span><br><span class="line">    <span class="comment"># train_example = token_l[:800]</span></span><br><span class="line">    <span class="comment"># train_label = label_l[:800]</span></span><br><span class="line">    <span class="comment"># test_example = token_l[800:]</span></span><br><span class="line">    <span class="comment"># test_label = label_l[800:]</span></span><br><span class="line">    accu_l = []</span><br><span class="line">    num_iterations = <span class="number">15</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        train_example, train_label, test_example, test_label = build_train_test_set(token_l, label_l)</span><br><span class="line"></span><br><span class="line">        all_words_dic = build_vocab(train_example)</span><br><span class="line">        <span class="comment"># print(all_words_dic)</span></span><br><span class="line">        <span class="comment"># print(process_dic_unknown(all_words_dic))</span></span><br><span class="line">        label_dic = build_labeled_vocab(train_example, train_label, all_words_dic, smooth=<span class="number">1</span>)</span><br><span class="line">        class_p = calculate_class_probability(train_label)      <span class="comment">#Propability of each class</span></span><br><span class="line">        word_p = calculate_conditional_probability(label_dic)</span><br><span class="line"></span><br><span class="line">        predict_l = predict(test_example, class_p, word_p)</span><br><span class="line">        <span class="comment"># print(predict_l)</span></span><br><span class="line">        <span class="comment"># print(test_label)</span></span><br><span class="line">        accuracy = np.sum(np.array(test_label) == np.array(predict_l))/len(test_label)</span><br><span class="line">        accu_l.append(accuracy)</span><br><span class="line">        <span class="comment"># print(accuracy)</span></span><br><span class="line">    print(<span class="string">"average accuracy: "</span>, sum(accu_l)/num_iterations)</span><br></pre></td></tr></table></figure>
<p>without smoothing and dealing with unknown word: 0.555</p>
<p>without smoothing but dealing with unknown word: 0.715</p>
<p>add-1 smoothing and dealing with unknown word: 0.82</p>
<p>add-3 smoothing and dealing with unknown word: 0.785</p>
<p>add-5 smoothing and dealing with unknown word: 0.765</p>
<p>stem average accuracy:  0.784888888888889</p>
<p>without stem average accuracy:  0.7562222222222222</p>
<h2 id="Bayesian-networks"><a href="#Bayesian-networks" class="headerlink" title="Bayesian networks"></a>Bayesian networks</h2><p>Bayesian networks represent the conditional dependence in a compat way. In real life, absolute dependence is not true most time.</p>
<p>Bayesian networks is a Directed Acyclic Graph(DAG) that model local conditional dependence. It joint probability distribution, which turns conditional probability distribution to joint probability distribution. </p>
<p>Bayesian networks is assembled by causality although causality is not necessary numerically in bayesian networks. Bayesian networks that reflect causality would be simpler and easier to think about.</p>
<p>In Bayesian networks, each variable is a node. Edges between nodes are directed, which represent dependence.</p>
<p>To calculate probability, arrage the netword properly s.t. \(p(x_1,…x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)….p(x_n|x_1,x_2….x_{n-1})\). </p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="By-Emuneration"><a href="#By-Emuneration" class="headerlink" title="By Emuneration"></a>By Emuneration</h4><p>Emunerate the variables and their states, ioin the variables to build a huge table. i.e.: N variables each with d states, the table entries would be \(d^N\).</p>
<h4 id="By-Variable-Elimination"><a href="#By-Variable-Elimination" class="headerlink" title="By Variable Elimination"></a>By Variable Elimination</h4><p>Join the variables and sum out the probability to eliminate the variable immediately. Do it iteratively to avoid building a big table.</p>
<p>i.e.:  \(p(x_1,…x_n)p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)….p(x_n|x_1,x_2….x_{n-1})\). First join x1 to x2  to get \(p(x_2,x_1)\), and then sum out x1 to get \(p(x_2)\). Do it iteratively to get \(p(x_n)\).</p>
<p>Finding the optimal ordering is a NP problem.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="http://net.pku.edu.cn/~course/cs410/2015/resource/book/2012-book-StatisticalLearning-LH.pdf" target="_blank" rel="noopener">统计学习方法</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/27/Noisy-Channel/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/27/Noisy-Channel/" itemprop="url">Noisy_Channel</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-27T11:41:20+08:00">
                2018-06-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Noisy-Channel-Model"><a href="#Noisy-Channel-Model" class="headerlink" title="Noisy Channel Model"></a>Noisy Channel Model</h3><p>The intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel.</p>
<p>$$\hat{w} = \operatorname{arg\,max}_{w\in C} p(x|w)p(w)$$</p>
<p>C is the list of candidate words. Above is a kind of Bayesian inference. We see an observation x (a misspelled word) and find the word w that generated this misspelled word. Out of all possible words in the vocabulary V or just a list of candidate word C, we want to find the<br>word w such that P(w|x) is highest.</p>
<h3 id="Channel-Model-p-x-w"><a href="#Channel-Model-p-x-w" class="headerlink" title="Channel Model p(x|w)"></a>Channel Model p(x|w)</h3><p>It’s expensive to get a perfect channel model. But a pretty reasonable estimate<br>of P(x|w) could be generated just by looking at local context: the identity of the correct letter itself, the<br>misspelling, and the surrounding letters.</p>
<h3 id="confusion-matrix"><a href="#confusion-matrix" class="headerlink" title="confusion matrix"></a>confusion matrix</h3><p>Contains counts of errors. In general, each entry in a confusion matrix represents the number of times one thing was confused with another. Thus for example a substitution confusion matrix will be a square matrix of size 26×26 (or more generally |A| × |A|,<br>for an alphabet A) that represents the number of times one letter was incorrectly<br>used instead of another.</p>
<p>del[x, y]: count(xy typed as x)</p>
<p>ins[x, y]: count(x typed as xy)</p>
<p>sub[x, y]: count(x typed as y)</p>
<p>trans[x, y]: count(xy typed as yx)</p>
<p>Methods to get confusion matrix:</p>
<p>1, extract them from lists of misspellings</p>
<p>$$p(x|w) = \frac{ins[x_{i-1},w_i]}{count[w_{i-1}]},\quad \text{if insertion}$$</p>
<p>$$p(x|w) = \frac{sub[x_{i},w_i]}{count[w_{i}]},\quad \text{if substitution}$$</p>
<p>The probability of substitution is calculated by:<br>given the correct word w and error word x, the number of times that ith character wi and the ith character xi is substituted divided by total number of ith character wi.<br>wi is the ith character of the correct word w and xi is the ith character of the typo x</p>
<p>2, <em>EM algorithm</em>.<br>compute the matrices by iteratively using the spelling error correction algorithm itself. The iterative algorithm first initializes the matrices with equal values; thus, any character is equally likely to be deleted, equally likely to be substituted for any other character,<br>etc. Next, the spelling error correction algorithm is run on a set of spelling<br>errors. Given the set of typos paired with their predicted corrections, the confusion<br>matrices can now be recomputed, the spelling algorithm run again, and so on. </p>
<p>Evaluating spell correction algorithms is generally done by holding out a training,<br>development and test set from lists of errors like those on the Norvig and Mitton<br>sites mentioned above.</p>
<p><strong>Candidate words</strong>:<br>Utilize the extended minimum edit distance algorithm introduced so that in addition to insertions, deletions, and substitutions, there is transpositions, in which two letters are swapped. All words with edit distance of 1 constitutes the candidate words set.</p>
<p>For words with specific context, it is important to use larger language models than unigrams such that the pobability p(w) is extended to \(p(w_{i-1})*p(w_i)\) for bigram model to take the context into consideration.</p>
<h3 id="Real-word-spelling-errors"><a href="#Real-word-spelling-errors" class="headerlink" title="Real-word spelling errors"></a>Real-word spelling errors</h3><p>Noisy channel algorithm could also be applied. The algorithm takes the input sentence \(X = {x_1, x_2,…, x_k<br>,…, x_n}\), generates a large set of candidate correction sentences C(X),<br>then picks the sentence with the highest language model probability.</p>
<p>To generate candidate set C(X), each word is assigned the probability that would make an error. And score each sentence like the aforementioned way. The difference is that the probability that the word is indeed correct needs to be considered. Given a word w, let’s assume the correct probability is \(\alpha\). And the error probability is \(\frac{1-\alpha}{|C(x)|} \) if \(x \in C(x)\). The others are totally the same as normal noisy channel model for wrong words.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/NLP-Notation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/26/NLP-Notation/" itemprop="url">NLP_Notation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-26T18:59:05+08:00">
                2018-06-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><strong>Language models</strong> assign a probability to each sentence</p>
<p>\( w_1^n\) is a sequence of N words from word 1 to word N.</p>
<p>A <strong>held-out corpus</strong> is an additional training corpus that we use to set hyperparameters.</p>
<p><strong>Non-word spelling correction</strong> is the detection and correction of spelling errors that result in non-words (like graffe for giraffe). </p>
<p><strong>real word spelling correction</strong> is the task of detecting and correcting spelling errors even if they accidentally result in an actual word of English (real-word errors).</p>
<p><strong>edit probability</strong> The probability of typo like deletion, insertion respectively.</p>
<p><strong>text categorization</strong>: classifying an entire text by assigning it a text<br>categorization label drawn from some set of labels.</p>
<p><strong>classification</strong>: take a single observation, extract some useful<br>features, and thereby classify the observation into one of a set of discrete classes.</p>
<p><strong>Generative classifiers</strong> like naive Bayes build a model of each class. Given an observation, they return the class most likely to have generated the observation. <strong>Discriminative classifiers</strong> like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes.<br>While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role.</p>
<p><strong>bag-of-words</strong>: an unordered set of words with their position ignored, keeping only their frequency in the document.</p>
<p><strong>linear classifiers</strong>: use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression.</p>
<p><strong>period disambiguation</strong>: deciding if a period is the end of a sentence or part<br>of a word</p>
<p><strong>co-occurrence matrix</strong>: a way of representing how often words co-occur.</p>
<p><strong>term-document matrix</strong>: each row represents a word in the vocabulary and<br>each column represents a document.</p>
<p><strong>vector space model</strong>:a document is represented as a count vector(column vector), each entry is the count of a specific word.</p>
<p><strong>Information retrieval</strong>(IR): is the task of finding the document d from the D<br>documents in some collection that best matches a query q which is also a vector of length |V|.</p>
<p><strong>term-context matrix</strong>: a  |V| × |V| matrix. Each entry(i,j) represent that the number of times a word i shows in the context word j. The context could be a document or a context window of length |L|.</p>
<p><strong>WordNet</strong>(Sense-Tagged data):WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of synonyms , each expressing a distinct concept.</p>
<p><strong>part-of-speech tags</strong>:</p>
<ol>
<li>CC    Coordinating conjunction</li>
<li>CD    Cardinal number</li>
<li>DT    Determiner</li>
<li>EX    Existential there</li>
<li>FW    Foreign word</li>
<li>IN    Preposition or subordinating conjunction</li>
<li>JJ    Adjective</li>
<li>JJR    Adjective, comparative</li>
<li>JJS    Adjective, superlative</li>
<li>LS    List item marker</li>
<li>MD    Modal</li>
<li>NN    Noun, singular or mass</li>
<li>NNS    Noun, plural</li>
<li>NNP    Proper noun, singular</li>
<li>NNPS    Proper noun, plural</li>
<li>PDT    Predeterminer</li>
<li>POS    Possessive ending</li>
<li>PRP    Personal pronoun</li>
<li>PRP$    Possessive pronoun</li>
<li>RB    Adverb</li>
<li>RBR    Adverb, comparative</li>
<li>RBS    Adverb, superlative</li>
<li>RP    Particle</li>
<li>SYM    Symbol</li>
<li>TO    to</li>
<li>UH    Interjection</li>
<li>VB    Verb, base form</li>
<li>VBD    Verb, past tense</li>
<li>VBG    Verb, gerund or present participle</li>
<li>VBN    Verb, past participle</li>
<li>VBP    Verb, non-3rd person singular present</li>
<li>VBZ    Verb, 3rd person singular present</li>
<li>WDT    Wh-determiner</li>
<li>WP    Wh-pronoun</li>
<li>WP$    Possessive wh-pronoun</li>
<li>WRB    Wh-adverb</li>
</ol>
<p><strong>Lemmatization</strong> is the task of determining that two words have the same root, despite their surface differences. Usually including plural to singular, but the word form such as ing, ed would not change.</p>
<p>This naive version of morphological analysis is called <strong>stemming</strong>. Stemming usually includes turning a word to its morpheme.</p>
<p>Two broad classes of morphemes: </p>
<ul>
<li>stems: the central morpheme of the word, supplying the main meaning. i.e.: girls, girl and s are the morphemes. The central morpheme is girl.</li>
<li>affixes: adding “additional”<br>meanings of various kinds. i.e.: homegrown, ‘home’ is added before the grown. unkind: ‘un’ is added before the word kind.</li>
</ul>
<p><strong>concept drift</strong> – the gradual change over time of the con- cept underlying a class like US president from Bill Clinton to George W. Bush</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/26/Smoothing/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/26/Smoothing/" itemprop="url">Smoothing</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-26T17:47:27+08:00">
                2018-06-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing:"></a>Smoothing:</h1><p>basically discount the observed words to assign to words with zero count so as to deal with the zero probability words.</p>
<h3 id="Laplace-Smoothing"><a href="#Laplace-Smoothing" class="headerlink" title="Laplace Smoothing"></a>Laplace Smoothing</h3><p>Also called add-one smoothing.</p>
<p>the unsmoothed maximum likelihood estimate of the unigram probability of the word wi is its count ci normalized by the total number of word tokens N</p>
<p>$$ p(w_i)= \frac{c_i}{N} $$</p>
<p>Laplace smoothing merely adds one to each count. Denominator is also adjusted by adding V(number of observations). If the demoninator is still unchanged, the sum of all probability would exceed 1.</p>
<p>$$ p_{Laplace}(w_i)= \frac{c_i+1}{N+V} $$</p>
<p><strong>adjusted count</strong>: describe how a smoothing algorithm affects the numerator</p>
<p>$$ c_i^* = \frac{c_i+1}{N+V} \times{N} = p_{Laplace}(w_i) \times{N} $$</p>
<p>the ratio of the discounted counts to the original counts describe a smoothing algorithm in terms of a relative discount dc:</p>
<p>$$ d_c= \frac{c^*}{c} $$</p>
<p>Normal bigram probabilities are computed by normalizing each row of counts by the unigram count:</p>
<p>$$ p(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)}{c(w_{n-1})} $$</p>
<p>After smoothing:</p>
<p>$$ p_{Laplace}(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)+1}{c(w_{n-1})+V} $$</p>
<p>These adjusted counts can be computed by equation below to show the reconstructed counts.</p>
<p>$$ c^*(w_{n-1}w_n) = \frac{c(w_{n-1}w_n)+1}{c(w_{n-1)}+V} \times{c(w_{n-1})} = p_{Laplace}(w_n|w_{n-1}) \times{c(w_{n-1})} $$</p>
<h3 id="Add-k-smoothing"><a href="#Add-k-smoothing" class="headerlink" title="Add-k smoothing:"></a>Add-k smoothing:</h3><p>add k rather than one; </p>
<p>$$ p_{Add-k}^*(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)+k}{c(w_{n-1})+kV} $$</p>
<p>chosse k by optimizing on a devset.</p>
<p>Although add-k is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts</p>
<h3 id="Backoff-and-Interpolation"><a href="#Backoff-and-Interpolation" class="headerlink" title="Backoff and Interpolation"></a>Backoff and Interpolation</h3><p><strong>backoff</strong>: If there is zero evidence for a higher-order N-gram, use lower-order N-gram instead. </p>
<p>i.e.: If we are trying to compute $$ P(w_n|w_{n−2}w_{n−1}) $$ but we have no examples of a particular trigram wn−2wn−1wn, we can instead estimate its probability by using the bigram probability $$ P(w_n|w_{n−1}) $$.</p>
<p><strong><em>interpolation</em></strong>: we always mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts.</p>
<p>In a slightly more sophisticated version of linear interpolation, each λ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the λs for those trigrams higher and thus give that trigram more weight in the interpolation.</p>
<p>$$ \hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 (w_{n-2}^{n-1})p(w_n|w_{n-2}w_{n-1}) + \lambda_2 (w_{n-2}^{n-1})p(w_n|w_{n-1}) +\lambda_3 (w_{n-2}^{n-1})p(w_n)  $$ </p>
<p>simple interpolation:</p>
<p>$$ \hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 p(w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) +\lambda_3 p(w_n) $$ </p>
<p>In both kinds of interpolation:</p>
<p>$$ \sum_i \lambda_i = 1 $$</p>
<p>Both the simple interpolation and conditional interpolation λ are learned from a held-out corpus. choose the λ values that maximize the likelihood of the held-out corpus.There are various ways to find this optimal set of λs. One way is to use the EM algorithm, which is an iterative learning algorithm that converges on locally optimal λs</p>
<p>In terms of <strong>quality of N-gram model</strong>: quadrigram &gt; trigram &gt; bigram &gt; unigram</p>
<p>In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. In addition to this explicit discount factor, we’ll need a function α to distribute this probability mass to the lower order N-grams.</p>
<p>This kind of backoff with discounting is also called <strong>Katz backoff</strong>. In Katz backoff we rely on a discounted probability \(P^∗\) if we’ve seen this N-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probability for the shorter-history (N-1)-gram. The probability for a backoff N-gram PBO is thus computed as follows:</p>
<p>$$\begin{cases}<br>       P^*(w_n|w_{n-N+1}^{n-1})\quad \text{if}\;C(w_{n-N+1}^{n-1}) &gt; 0 \<br>       \alpha(w_{n-N+1}^{n-1})P_{BO}(w_n|w_{n-N+2}^{n-1})\quad \text{otherwise}<br>    \end{cases} $$</p>
<p>Katz backoff is often combined with a smoothing method called <strong>Good-Turing</strong>. The combined Good-Turing backoff algorithm involves quite detailed computation<br>for estimating the Good-Turing smoothing and the \(P^*\) and \(\alpha\) values.</p>
<h3 id="Kneser-Ney-Smoothing"><a href="#Kneser-Ney-Smoothing" class="headerlink" title="Kneser-Ney Smoothing"></a>Kneser-Ney Smoothing</h3><p>Intuition: Discount the probability from N gram model and assign it to phrase so as to take different word combinations into consideration.</p>
<p>Simple example:<br>Finance in _ is promissing.<br>Here you can just simply fill in a word with high probability like USA, Germany. With Kneser-Ney Smoothing, taking word combinations into consideration, it might fill in world finance centre like London and HongKong because they often appear together.</p>
<p>Novel continuation means the first time that the word \(w_{i-1}\) preceeds the word \(w_i\).</p>
<p>The number of times a word w appears as a novel continuation can be expressed as:</p>
<p>$$P_{continuation}(w_i)\propto \lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert$$</p>
<p>\(w_i\) is the current word. \(w_{i-1}\) is the words occur before the current word(also called preceeding word).</p>
<p>To turn this count into a probability, we normalize by the total number of bigram word. In summary:</p>
<p>$$P_{continuation}(w_i)=\frac{\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert}{\lvert{(w_{j-1},w_j):c(w_{j-1}w_j)&gt;0}\rvert}$$</p>
<p>Or normalized by the number of words preceding all words as follows:</p>
<p>$$P_{continuation}(w_i)=\frac{\lvert { w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert}<br>{\sum_{w’<em>i} \lvert {w’</em>{i-1}:C(w’_{i-1}w’_i)&gt;0}\rvert}$$</p>
<p>The number of words preceeding all words is:<br>given word \({w’}<em>i\),the number of word \(w’</em>{i-1}\) that preceed \({w’}_i\).<br>\({w’}_i\) are all words in the corpus. So The number of words preceeding all words = the total number of bigram word.</p>
<p>The final equation for <strong>Interpolated Kneser-Ney smoothing</strong> for bigrams is then:</p>
<p>$$P_{KN}(w_i|w_{i-1})=\frac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})}<br>+\lambda(w_{i-1})P_{continuation}(w_i)$$</p>
<p>The λ is a normalizing constant that is used to distribute the probability mass that is discounted:</p>
<p>$$\lambda(w_{i-1})=\frac{d}{C(w_{i-1})}\cdot \lvert {w:C(w_{i-1},w)&gt;0}\rvert$$</p>
<p>The first term \(\frac{d}{C(w_{i-1})}\) is the normalized discount. The second term \(\lvert {w:C(w_{i-1},w)&gt;0}\rvert\)<br>is the number of word types that can follow \(w_{i−1}\) or, equivalently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount.</p>
<p>The general recursive formulation is as follows:</p>
<p>$$P_{KN}(w_i|w_{i-n+1}\cdots w_{i-1})=\frac{max(0,C_{KN}(w_{i-n+1} \cdots w_i) - d)}{C_{KN}(w_{i-n+1}\cdots w_{i-1})}<br>+\lambda(w_{i-n+1}\cdots w_{i-1})\cdot P_{KN}(w_i|w_{i-n+2}\cdots w_{i-1})$$</p>
<p>$$\lambda(w_{i-n+1}\cdots w_{i-1})=\frac{d}{C_{KN}(w_{i-n+1}\cdots w_{i-1})}\cdot \lvert {w:C_{KN}(w_{i-n+1} \cdots w_{i-1}w)&gt;0}\rvert$$</p>
<p>where the definition of the count \(c_{KN}\) depends on whether we are counting the highest-order N-gram being interpolated (for example trigram if we are interpolat- ing trigram, bigram, and unigram) or one of the lower-order N-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram):</p>
<p>$$C_{KN}(\cdot)=\begin{cases}count(\cdot) &amp;,\text{for the highest order}\<br>continuationcount(\cdot) &amp;,\text{for all other lower orders}<br>\end{cases}$$</p>
<p>The continuation count is the number of unique single word contexts for ·.<br>At the termination of the recursion, unigrams are interpolated with the uniform<br> distribution, where the parameter ε is the empty string:</p>
<p> $$P_{KN}(w)=\frac{max(c_{KN}(w)-d,0)}{\sum_{w’}c_{KN}(w’)} +\lambda(\epsilon)\frac{1}{V}$$</p>
<p> If we want to include an unknown word <unk>, it’s just included as a regular vo-<br>cabulary entry with count zero, and hence its probability will be a lambda-weighted<br>uniform distribution \(\frac{\lambda(\epsilon)}{V}\).</unk></p>
<p>The best-performing version of Kneser-Ney smoothing is called modified Kneser- Ney smoothing. Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and \(d3_+\) for N-grams with counts of 1, 2 and three or more, respectively.</p>
<h3 id="Entropy"><a href="#Entropy" class="headerlink" title="Entropy"></a>Entropy</h3><p><strong>Entropy</strong> is the minimum number of bits(if log base 2 is used) that is needed to encode(represent) information in the optimal coding scheme.</p>
<p><strong>Entropy rate</strong> the entropy of a sequence divided by the number of words.</p>
<p>To measure entropy rate of a language, entropy rate of a infinite sequence needs to be calculated.</p>
<p>$$H(L) = −\lim_{n\to\infty} \frac{1}{n}H(w_1,w_2,…,w_n)<br>= −\lim_{n\to\infty}\sum_{W\in L} p(w_1,…,w_n)\frac{1}{n} \log p(w_1,…,w_n)$$</p>
<p>The Shannon-McMillan-Breiman theorem states that if the language is rboth stationary and ergodic</p>
<p>$$H(L) = \lim_{n\to\infty}-\frac{1}{n}\log p(w_1,…,w_n)$$</p>
<p>The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.That is, we can take a single sequence that is long enough instead of summing over all possible sequences. </p>
<p>A stochastic process is said to be stationary if the probability distribution for words at time t is the same as the probability distribution at time t + 1. Markov models, and hence N-grams, are stationary. Natural language is not stationary, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.<br>To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.</p>
<p>The cross-entropy is useful when we don’t know the actual probability distribution p that generated some data. It allows us to use some m, which is a model of p (i.e., an approximation to p) to approximate the real entropy. The<br>cross-entropy of m on p is defined by</p>
<p>$$H(p,m) = \lim_{n\to\infty}\sum_{W\in L}- p(w_1,…,w_n)\frac{1}{n} \log m(w_1,…,w_n)$$</p>
<p>That is, we draw sequences according to the probability distribution p, but sum the log of their probabilities according to m.<br>Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process:</p>
<p>$$H(p,m) = \lim_{n\to\infty}-\frac{1}{n}\log m(w_1,…,w_n)$$</p>
<p>This means that, as for entropy, we can estimate the cross-entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences.<br>What makes the cross-entropy useful is that the cross-entropy H(p,m) is an upper bound on the entropy H(p).<br>For any model m:</p>
<p>$$H(p) ≤ H(p,m)$$</p>
<p>This means that we can use some simplified model m to estimate the true entropy of a sequence of symbols drawn according to probability p. The more accurate m is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus, the difference between H(p,m) and H(p) is a measure of how accurate a model is. The lower the cross-entropy, the better is the model(better approximation of the true entropy). The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.</p>
<p>Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity. A (sufficiently long) sequence of fixed length N is employed to  approximate the cross-entropy of a model M = $ P(w_i|w_{i−N+1}…w_{i−1}) $(N is the N of a N-gram model) on a sequence of words W is</p>
<p>$$H(W) = -\frac{1}{N}\log p(w_1,…,w_n)$$</p>
<p>The perplexity of a model P on a sequence of words W is now formally defined as the exp of this cross-entropy:</p>
<p>$$Perplexity(W) = 2^{H(W)} = P(w_1,…,w_n)^{-\frac{1}<br>{N}}$$</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/25/Max-Matching/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/25/Max-Matching/" itemprop="url">Max_Matching</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T23:36:22+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Max-Matching-Algorithm"><a href="#Max-Matching-Algorithm" class="headerlink" title="Max Matching Algorithm"></a>Max Matching Algorithm</h2><p>It is a kind of greedy algorithm used to tokenize sentences. It is a baseline of tokenize algorithms.</p>
<p>It works especially well on Chinese. I think it is because that Chinese words are single word that could not be took apart while English words are sequences of characters.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_words</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(file) <span class="keyword">as</span> f:</span><br><span class="line">        l = f.readlines()</span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_matching</span><span class="params">(text, dictionary)</span>:</span></span><br><span class="line">    <span class="comment"># l = []</span></span><br><span class="line">    w = <span class="string">''</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(text),<span class="number">0</span>,<span class="number">-1</span>):</span><br><span class="line">        word = text[:i]</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> dictionary:</span><br><span class="line">            w += word+<span class="string">' '</span></span><br><span class="line"></span><br><span class="line">            w += max_matching(text[i:],dictionary) + <span class="string">' '</span></span><br><span class="line">            <span class="keyword">return</span> w</span><br><span class="line">    <span class="keyword">if</span> len(text) != <span class="number">0</span>:</span><br><span class="line">        w = text[<span class="number">0</span>]+<span class="string">' '</span></span><br><span class="line">        w += max_matching(text[<span class="number">1</span>:],dictionary) + <span class="string">' '</span></span><br><span class="line">        <span class="keyword">return</span> w</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">''</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    file = <span class="string">'/Users/lmk/Documents/NLP/word.csv'</span></span><br><span class="line">    l = read_words(file)</span><br><span class="line">    <span class="comment"># l = read_words(file)</span></span><br><span class="line">    l = list(map(<span class="keyword">lambda</span> i: i[<span class="number">3</span>:<span class="number">-1</span>], l))</span><br><span class="line">    l[<span class="number">0</span>] = l[<span class="number">0</span>][<span class="number">1</span>:]</span><br><span class="line">    dic = l</span><br><span class="line">    <span class="comment"># s = "ilovechina"</span></span><br><span class="line">    s = <span class="string">"is also possible to use a list as a queue, where the first"</span></span><br><span class="line">    sentence = max_matching(s,dic)</span><br><span class="line">    token = sentence.split()</span><br><span class="line">    <span class="comment"># flatten = lambda l: [item for sublist in l for item in sentence]</span></span><br><span class="line">    <span class="comment"># flat_list = [item for sublist in l for item in sublist]</span></span><br><span class="line">    print(sentence)</span><br><span class="line">    print(token)</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/25/Minimum-Edit-Distance/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/25/Minimum-Edit-Distance/" itemprop="url">Minimum_Edit_Distance</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T19:46:58+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Minimum-Edit-Distance"><a href="#Minimum-Edit-Distance" class="headerlink" title="Minimum Edit Distance"></a>Minimum Edit Distance</h2><p>minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. It’s useful in terms of portential splleing checking and string alignment which is significant in terms of machine translation and speech recognition.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insertion_cost</span><span class="params">(self,c1)</span>:</span></span><br><span class="line">        <span class="string">'''customize your insertion cost'''</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deletion_cost</span><span class="params">(self,c1)</span>:</span></span><br><span class="line">        <span class="string">'''customize your deletion cost'''</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">substitution_cost</span><span class="params">(self,c1,c2)</span>:</span></span><br><span class="line">        <span class="string">'''customize your substitution cost'''</span></span><br><span class="line">        <span class="keyword">if</span> c1 == c2:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(self, word1, word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type word1: str</span></span><br><span class="line"><span class="string">        :type word2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(word1)+<span class="number">1</span>    <span class="comment">#row of matrix</span></span><br><span class="line">        m = len(word2)+<span class="number">1</span>    <span class="comment">#column of matrix</span></span><br><span class="line">        l = np.zeros((m,n))</span><br><span class="line">        <span class="comment"># l = [n*[0] for i in range(m)]</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            l[i,<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            l[<span class="number">0</span>,i] = i</span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):</span><br><span class="line">                <span class="comment"># print(i,j)</span></span><br><span class="line">                l[i][j] = min(l[i<span class="number">-1</span>][j]+self.insertion_cost(word2[i<span class="number">-1</span>]), l[i][j<span class="number">-1</span>]+self.deletion_cost(word1[j<span class="number">-1</span>]), l[i<span class="number">-1</span>][j<span class="number">-1</span>]+self.substitution_cost(word1[j<span class="number">-1</span>],word2[i<span class="number">-1</span>]))</span><br><span class="line">        <span class="comment"># l = [list(range(n)) for i in range(m)]</span></span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">        <span class="keyword">return</span> l[<span class="number">-1</span>][<span class="number">-1</span>]</span><br></pre></td></tr></table></figure>
<h4 id="minimum-edit-distance-with-backtrace"><a href="#minimum-edit-distance-with-backtrace" class="headerlink" title="minimum edit distance with backtrace"></a>minimum edit distance with backtrace</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Edit_Distance</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.back_trace = <span class="keyword">None</span></span><br><span class="line">        self.n = <span class="number">0</span></span><br><span class="line">        self.m = <span class="number">0</span></span><br><span class="line">        self.l = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insertion_cost</span><span class="params">(self,c1)</span>:</span></span><br><span class="line">        <span class="string">'''customize your insertion cost'''</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">deletion_cost</span><span class="params">(self,c1)</span>:</span></span><br><span class="line">        <span class="string">'''customize your deletion cost'''</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">substitution_cost</span><span class="params">(self,c1,c2)</span>:</span></span><br><span class="line">        <span class="string">'''customize your substitution cost'''</span></span><br><span class="line">        <span class="keyword">if</span> c1 == c2:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">minDistance</span><span class="params">(self, word1, word2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type word1: str</span></span><br><span class="line"><span class="string">        :type word2: str</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        n = len(word1)+<span class="number">1</span>    <span class="comment">#row of matrix</span></span><br><span class="line">        m = len(word2)+<span class="number">1</span>    <span class="comment">#column of matrix</span></span><br><span class="line">        self.n = n</span><br><span class="line">        self.m = m</span><br><span class="line">        l = np.zeros((m,n))</span><br><span class="line">        <span class="comment"># back_trace = np.zeros((m,n))</span></span><br><span class="line">        back_trace = [n*[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> range(m)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            l[i,<span class="number">0</span>] = i</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">            l[<span class="number">0</span>,i] = i</span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,m):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>,n):</span><br><span class="line">                <span class="comment"># print(i,j)</span></span><br><span class="line">                cost = [l[i<span class="number">-1</span>][j]+self.insertion_cost(word2[i<span class="number">-1</span>]), l[i][j<span class="number">-1</span>]+self.deletion_cost(word1[j<span class="number">-1</span>]), l[i<span class="number">-1</span>][j<span class="number">-1</span>]+self.substitution_cost(word1[j<span class="number">-1</span>],word2[i<span class="number">-1</span>])]</span><br><span class="line">                min_cost = min(cost)</span><br><span class="line">                print((i,j),min_cost)</span><br><span class="line">                back_trace[i][j] = [i <span class="keyword">for</span> i,j <span class="keyword">in</span> enumerate(cost) <span class="keyword">if</span> j == min_cost]</span><br><span class="line">                l[i][j] = min_cost</span><br><span class="line">        <span class="comment"># l = [list(range(n)) for i in range(m)]</span></span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">        <span class="comment"># print(back_trace)</span></span><br><span class="line">        self.l = l</span><br><span class="line">        self.back_trace = back_trace</span><br><span class="line">        <span class="keyword">return</span> l[<span class="number">-1</span>][<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def print_direction(self,l):</span></span><br><span class="line">    <span class="comment">#     if</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">output_back_trace</span><span class="params">(self)</span>:</span></span><br><span class="line">        i = self.n<span class="number">-1</span></span><br><span class="line">        j = self.m<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span> i &gt;= <span class="number">0</span> <span class="keyword">and</span> j &gt;= <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># print((i,j))</span></span><br><span class="line">            direction_list = self.back_trace[i][j]   <span class="comment">#choose the last direction, here every direction has the same cost</span></span><br><span class="line">            print(direction_list)</span><br><span class="line">            <span class="keyword">if</span> isinstance(direction_list,list):</span><br><span class="line">                direction = direction_list[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                direction = direction_list</span><br><span class="line">            <span class="keyword">if</span> direction == <span class="number">2</span>:</span><br><span class="line">                i -= <span class="number">1</span></span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> direction == <span class="number">1</span>:</span><br><span class="line">                j -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                i -= <span class="number">1</span></span><br><span class="line">            <span class="comment"># print(self.l[i][j])</span></span><br><span class="line"></span><br><span class="line">s = Edit_Distance()</span><br><span class="line"><span class="comment"># word1 = "intention"</span></span><br><span class="line"><span class="comment"># word2 = "execution"</span></span><br><span class="line">word1 = <span class="string">"execution"</span></span><br><span class="line">word2 = <span class="string">"intention"</span></span><br><span class="line">leg = s.minDistance(word1,word2)</span><br><span class="line">print(leg)</span><br><span class="line">s.output_back_trace()</span><br><span class="line">print(s.l)</span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/25/Port-Stemmer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/25/Port-Stemmer/" itemprop="url">Port_Stemmer</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-25T14:28:00+08:00">
                2018-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Lemmatization"><a href="#Lemmatization" class="headerlink" title="Lemmatization"></a>Lemmatization</h2><p>Lemmatization is the task to determine whether two words have the same root. If proper lemmatization has been applied, number of tokens could be reduced significantly and understanding of the text could also be facilitated.</p>
<h4 id="Problem-of-suffix-stripping"><a href="#Problem-of-suffix-stripping" class="headerlink" title="Problem of suffix stripping"></a>Problem of suffix stripping</h4><p><em>probe</em> and <em>probate</em><br><em>wand</em> and <em>wander</em><br>Sometimes removal of suffix could affect the original meaning and degrade the performance.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/07/Sort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/06/07/Sort/" itemprop="url">Sort</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-07T17:37:39+08:00">
                2018-06-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Quick-Sort"><a href="#Quick-Sort" class="headerlink" title="Quick Sort"></a>Quick Sort</h2><p>Partition the array into two parts such that elements in part a &lt; that of part b. Recursively invoke quick sort to do in-place sorting. Average performance is the best in practice. O(nlogn). The worst case happens when the array is inversely sorted, which leads the subarray after partition is extremely imbalanced. Time complexity is O(n^2).</p>
<h2 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h2><p>Divide and conquer. Divide the array into two parts and then merge recursively. The process of sorting is just like building a binary tree in a top-down manner and then merge the array bottom-up.</p>
<h2 id="Heap-Sort"><a href="#Heap-Sort" class="headerlink" title="Heap Sort"></a>Heap Sort</h2><p>build a max heap so that the first element in the heap is always the largest. Sorting is just switching the largest value with the last element in the array and maintain a heap. after poping the element, reduce the heap size by one.</p>
<p>##Performance<br>Qucik sort is the fastest. After it is merge sort. And then heap sort. build a heap takes considerable time. Buble sort and insertion sort are far slower.<br>Random an array of length 1000000, quick sort took 5.30535101890564 seconds and merge sort took 5.918827056884766 seconds. Their perfomance are quite good and close to each other. Heap sort took 13.471954822540283 seconds.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Sort</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, l = None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> l <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">assert</span> type(l) == list,(<span class="string">"please input a list"</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l = self._generate()</span><br><span class="line">        self.l = l</span><br><span class="line">        self.result = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(self)</span>:</span></span><br><span class="line">        l = self.l      <span class="comment">#add new reference to self.l, change l would change self.l</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(l)):   <span class="comment">#use built-in methods to insert</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> l[i] &lt; l[j]:</span><br><span class="line">                    <span class="keyword">if</span> j == <span class="number">0</span>:</span><br><span class="line">                        l.insert(j,l[i])</span><br><span class="line">                        <span class="keyword">del</span>(l[i+<span class="number">1</span>])</span><br><span class="line">                    <span class="keyword">elif</span> l[i] &gt;= l[j<span class="number">-1</span>]:</span><br><span class="line">                        l.insert(j, l[i])</span><br><span class="line">                        <span class="keyword">del</span> (l[i + <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert_sort_v1</span><span class="params">(self)</span>:</span>   <span class="comment">#much slower than using built-in method</span></span><br><span class="line">        l = self.l      <span class="comment">#add new reference to self.l, change l would change self.l</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(l)):   <span class="comment">#switch position</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(i<span class="number">-1</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> l[j+<span class="number">1</span>] &lt; l[j]:</span><br><span class="line">                    l[j], l[j+<span class="number">1</span>] = l[j+<span class="number">1</span>], l[j]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(self)</span>:</span></span><br><span class="line">        l = self.l</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,len(l)<span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">0</span>,len(l)-i<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> l[j] &gt; l[j+<span class="number">1</span>]:</span><br><span class="line">                    l[j], l[j+<span class="number">1</span>] = l[j+<span class="number">1</span>], l[j]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(self, p=None, r=None)</span>:</span></span><br><span class="line">        <span class="string">'''extra space usage'''</span></span><br><span class="line">        <span class="string">'''divide the original into two parts such that elements in part a &lt; that of part b'''</span></span><br><span class="line">        <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> r <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            p = <span class="number">0</span></span><br><span class="line">            r = len(self.l)</span><br><span class="line">        <span class="comment"># print("p ",p)</span></span><br><span class="line">        <span class="comment"># print("r ", r)</span></span><br><span class="line">        <span class="keyword">if</span> p&lt;r<span class="number">-1</span>:</span><br><span class="line">            q = self._quick_sort_partition(p, r)</span><br><span class="line">            <span class="comment"># print("q ", q)</span></span><br><span class="line">            self.quick_sort(p,q)</span><br><span class="line">            self.quick_sort(q,r)</span><br><span class="line">            <span class="comment"># self._quick_sort_partition(p, q)</span></span><br><span class="line">            <span class="comment"># self._quick_sort_partition(q, r)</span></span><br><span class="line">        <span class="comment"># q = self._quick_sort_partition(p, r)</span></span><br><span class="line">        <span class="comment"># print(q)</span></span><br><span class="line">        <span class="comment"># while True:</span></span><br><span class="line">        <span class="comment">#     if self._quick_sort_partition(p, q) == 0:</span></span><br><span class="line">        <span class="comment">#         break</span></span><br><span class="line">        <span class="comment"># while True:</span></span><br><span class="line">        <span class="comment">#     if self._quick_sort_partition(q, r) == r-1:</span></span><br><span class="line">        <span class="comment">#         break</span></span><br><span class="line">            <span class="comment"># q = self._quick_sort_partition(0,len(self.l))</span></span><br><span class="line">        <span class="comment"># self.quick_sort(p,q)</span></span><br><span class="line">        <span class="comment"># self.quick_sort(q,r)</span></span><br><span class="line">        <span class="comment"># else:</span></span><br><span class="line">        <span class="comment">#     q = self._quick_sort_partition(p, r)</span></span><br><span class="line">        <span class="comment">#     self.quick_sort(p, q - 1)</span></span><br><span class="line">        <span class="comment">#     self.quick_sort(q, r)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_quick_sort_partition</span><span class="params">(self, p, r)</span>:</span></span><br><span class="line">        <span class="comment"># l = list(self.l)</span></span><br><span class="line">        l = self.l</span><br><span class="line">        i = p<span class="number">-1</span></span><br><span class="line">        pivot = l[r<span class="number">-1</span>]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(p,r<span class="number">-1</span>):      <span class="comment">#in-place exchange without memory overhead</span></span><br><span class="line">            <span class="keyword">if</span> l[j] &lt;= pivot:</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">                l[i], l[j] = l[j], l[i]</span><br><span class="line">        <span class="comment"># print(i)</span></span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">        l[i+<span class="number">1</span>], l[r<span class="number">-1</span>] = l[r<span class="number">-1</span>], l[i+<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">        <span class="comment"># print(self.l)</span></span><br><span class="line">        <span class="comment"># print("i ",i)</span></span><br><span class="line">        <span class="keyword">return</span> i+<span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(self, p=None, r=None)</span>:</span></span><br><span class="line">        l = self.l</span><br><span class="line">        <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> r <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            p = <span class="number">0</span></span><br><span class="line">            r = len(l)      <span class="comment">#not include r</span></span><br><span class="line">        <span class="keyword">if</span> p &lt; r<span class="number">-1</span>:</span><br><span class="line">            q = int((p+r)/<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">            self.merge_sort(p,q)</span><br><span class="line">            self.merge_sort(q, r)</span><br><span class="line">            self.merge(p,q,r)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(self, p,q,r)</span>:</span></span><br><span class="line">        l = self.l</span><br><span class="line">        L = l[p:q]</span><br><span class="line">        R = l[q:r]</span><br><span class="line">        L.append(float(<span class="string">'inf'</span>))</span><br><span class="line">        R.append(float(<span class="string">'inf'</span>))</span><br><span class="line">        i,j = <span class="number">0</span>,<span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> range(p,r):    <span class="comment">#r = len(list), not include r</span></span><br><span class="line">            <span class="keyword">if</span> L[i] &lt; R[j]:</span><br><span class="line">                l[k] = L[i]</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                l[k] = R[j]</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">merge_two_lists</span><span class="params">(self,l1,l2)</span>:</span></span><br><span class="line">        <span class="keyword">assert</span> l1 <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">or</span> l2 <span class="keyword">is</span> <span class="keyword">None</span>,(<span class="string">"empty list"</span>)</span><br><span class="line">        l = []</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        j = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> i&lt;len(l1) <span class="keyword">and</span> j&lt;len(l2):</span><br><span class="line">            <span class="keyword">if</span> l1[i] &lt; l2[j]:</span><br><span class="line">                l.append(l1[i])</span><br><span class="line">                i+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                l.append(l2[j])</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i == len(l1):</span><br><span class="line">            l.extend(l2[j:])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            l.extend(l1[i:])</span><br><span class="line">        <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">heap_sort</span><span class="params">(self)</span>:</span></span><br><span class="line">        l = self.l</span><br><span class="line">        self.heap_size = len(l)</span><br><span class="line">        self.build_heap()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(l)<span class="number">-1</span>,<span class="number">0</span>,<span class="number">-1</span>):  <span class="comment">#no need to include 0. change with itself is meaningless</span></span><br><span class="line">            l[i], l[<span class="number">0</span>] = l[<span class="number">0</span>], l[i]</span><br><span class="line">            self.heap_size -= <span class="number">1</span></span><br><span class="line">            self.max_heaplify(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build_heap</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#bottom up. the first level of the tree has no child, no need to maintain</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(int(self.heap_size/<span class="number">2</span>),<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">            self.max_heaplify(i)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">max_heaplify</span><span class="params">(self,i)</span>:</span></span><br><span class="line">        l = self.l</span><br><span class="line">        left = <span class="number">2</span>*i +<span class="number">1</span></span><br><span class="line">        right = <span class="number">2</span>*i +<span class="number">2</span></span><br><span class="line">        largest = i</span><br><span class="line">        <span class="keyword">if</span> left &lt; self.heap_size <span class="keyword">and</span> l[left] &gt; l[i]:</span><br><span class="line">            largest = left</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> right &lt; self.heap_size <span class="keyword">and</span> l[right] &gt; l[largest]:</span><br><span class="line">            largest = right</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> largest != i:</span><br><span class="line">            <span class="string">'''if position changes, need to recursively run max_heaplify to guarantee</span></span><br><span class="line"><span class="string">            the descendant are also max heap'''</span></span><br><span class="line">            l[i], l[largest] = l[largest], l[i]</span><br><span class="line">            self.max_heaplify(largest)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_evaluate</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.result <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.result = self.l</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(self.result)):</span><br><span class="line">            <span class="keyword">if</span> self.result[i] &lt; self.result[i<span class="number">-1</span>]:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_generate</span><span class="params">(self,length = None)</span>:</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">import</span> random</span><br><span class="line">            <span class="keyword">if</span> length <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                length = random.randint(<span class="number">0</span>, <span class="number">1000</span>)</span><br><span class="line">                <span class="comment"># length = random.randint(0,100000)</span></span><br><span class="line">            l = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(length):</span><br><span class="line">                l.append(random.randint(-length,length))</span><br><span class="line">            <span class="keyword">return</span> l</span><br><span class="line">        <span class="keyword">except</span> ImportError:</span><br><span class="line">            print(<span class="string">"no random module, please input a sequence"</span>)</span><br><span class="line"></span><br><span class="line">l = [<span class="number">4</span>,<span class="number">6</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">5</span>,<span class="number">-9</span>,<span class="number">0</span>,<span class="number">5</span>,]</span><br><span class="line"><span class="comment"># l = [4,6,10,23,-1,-6]</span></span><br><span class="line"><span class="comment"># s = Sort(l)</span></span><br><span class="line"><span class="comment"># print(s._quick_sort_partition(0,len(l)))</span></span><br><span class="line"><span class="comment"># print(s.l)</span></span><br><span class="line">s = Sort()</span><br><span class="line"><span class="comment"># s.quick_sort()</span></span><br><span class="line">print(<span class="string">"length "</span>,len(s.l))</span><br><span class="line">s.heap_sort()</span><br><span class="line"><span class="comment"># s.build_heap()</span></span><br><span class="line"><span class="comment"># s.merge_sort()</span></span><br><span class="line"><span class="comment"># s.insert_sort_v1()</span></span><br><span class="line"><span class="comment"># s.insert_sort()</span></span><br><span class="line">print(s._evaluate())</span><br><span class="line"><span class="comment"># s._quick_sort_partition(0,len(l))</span></span><br><span class="line">print(s.l)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compare_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    print(<span class="string">"**************compare time**************"</span>)</span><br><span class="line">    s = Sort()</span><br><span class="line">    l = s._generate(<span class="number">1000000</span>)</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    Sort(l).quick_sort()</span><br><span class="line">    print(<span class="string">"--- %s seconds ---"</span> % (time.time() - start_time))</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    Sort(l).merge_sort()</span><br><span class="line">    print(<span class="string">'--- &#123;&#125; seconds ---'</span>.format(time.time() - start_time))</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    Sort(l).heap_sort()</span><br><span class="line">    print(<span class="string">'--- &#123;&#125; seconds ---'</span>.format(time.time() - start_time))</span><br><span class="line"></span><br><span class="line">compare_time()</span><br></pre></td></tr></table></figure>
<p>###Reference: Introduction to Algorithms</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/4/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/">6</a><a class="extend next" rel="next" href="/page/6/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="MK_LEE" />
            
              <p class="site-author-name" itemprop="name">MK_LEE</p>
              <p class="site-description motion-element" itemprop="description">Passionate about NLP</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">55</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MK_LEE</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
