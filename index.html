<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="Passionate about NLP">
<meta property="og:type" content="website">
<meta property="og:title">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name">
<meta property="og:description" content="Passionate about NLP">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title">
<meta name="twitter:description" content="Passionate about NLP">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'PJWL3PD75I',
      apiKey: '5589833aa2fa703729b4e7e939ac7dec',
      indexName: 'test_index',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title></title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title"></span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/11/Decision-Tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/08/11/Decision-Tree/" itemprop="url">Decision_Tree</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-08-11T14:20:17+08:00">
                2019-08-11
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h3 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h3><p>实际上adaboost是GBDT的一种特例，loss函数为指数损失函数或者平方损失函数，可以理解为loss函数正好可以写成闭合形式，所以不需要梯度下降的方法来近似。</p>
<h3 id="GBDT"><a href="#GBDT" class="headerlink" title="GBDT"></a>GBDT</h3><p>利用梯度下降方法来近似残差，使得loss下降最快。本质上是一种greedy算法。</p>
<p>原则上来说Xgboost属于GBDT的一种，GBM也是。</p>
<h6 id="正则"><a href="#正则" class="headerlink" title="正则"></a>正则</h6><p>步长(学习率)是一种正则化，避免贪心算法每次都选择最好的树导致过拟合。步长可能通过line search来找。</p>
<h3 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h3><h6 id="为什么使用二阶导数？"><a href="#为什么使用二阶导数？" class="headerlink" title="为什么使用二阶导数？"></a>为什么使用二阶导数？</h6><ol>
<li>理论上: know what we are learning, convergence.知道在学什么，有更多信息，加速收敛</li>
<li>工程上: 对损失函数进行泰勒展开得到一阶和二阶导使得损失函数可以自定义，更多样化</li>
</ol>
<p>这个实际上就是根据模型在t-1次的地方展开来近似第t次模型的损失函数，<strong>因此并不需要loss函数可导</strong>.而GBDT是loss函数直接对模型的t-1次结果求导，因此loss函数必须可导。</p>
<h6 id="特征重要性实现："><a href="#特征重要性实现：" class="headerlink" title="特征重要性实现："></a>特征重要性实现：</h6><ol>
<li>特征在所有树中作为划分属性的次数</li>
<li>划分属性时loss平均的降低量(信息增益)</li>
<li>划分属性时对样本的覆盖度</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/26/Word-Sense-Disambiguation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/26/Word-Sense-Disambiguation/" itemprop="url">Word_Sense_Disambiguation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-26T10:28:24+08:00">
                2019-07-26
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Upper bound and lower bound are necessarily to be determined to evaluate an algorithm or a system.</p>
<p>Usually upper bound is human evaluation and lower bound is the simplest algorithm.</p>
<h4 id="Supervised-Disambiguation"><a href="#Supervised-Disambiguation" class="headerlink" title="Supervised Disambiguation"></a>Supervised Disambiguation</h4><h6 id="Bayesian-classification"><a href="#Bayesian-classification" class="headerlink" title="Bayesian classification"></a>Bayesian classification</h6><p>given a context, employ bayes classification to classify the word sense.</p>
<p>###### </p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/22/Evaluation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/22/Evaluation/" itemprop="url">Evaluation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-22T23:32:19+08:00">
                2019-07-22
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css">
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/08/Information-Retrieval/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/07/08/Information-Retrieval/" itemprop="url">Information_Retrieval</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-07-08T15:59:07+08:00">
                2019-07-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h4 id="BM25"><a href="#BM25" class="headerlink" title="BM25"></a>BM25</h4><p>It is a kind of algorithm that scoring the documents according to the query. It is modified from tf-idf.</p>
<p>$$<br>f(q, d)=\sum_{w \in q \cap d} c(w, q) \frac{(k+1) c(w, d)}{c(w, d)+k\left(1-b+b \frac{|d|}{a v d l}\right)} \log \frac{M+1}{d f(w)}<br>$$<br>c(w,q) is the term frequency. </p>
<p>$\log \frac{M+1}{d f(w)}$ is the idf where M is the number of documents and df(w) is the number of document that word w shows up.</p>
<p>$\frac{(k+1) c(w, d)}{c(w, d)+k\left(1-b+b \frac{|d|}{a v d l}\right)}$ is the transformation of term frequency. $\frac{(k+1) x}{x+k}$ inhibit term frequency x from increasing infinitely. The relevance does not increase linearly with the term frequency.</p>
<p>$1-b+b \frac{|d|}{a v d l}$  is a normalization term of document length. The longer the document length, higher the penalty. </p>
<h4 id="Term-frequency"><a href="#Term-frequency" class="headerlink" title="Term-frequency"></a>Term-frequency</h4><p>There are many kinds of <strong>term frequency</strong> tf(<em>t</em>,<em>d</em>). For example, it coule be </p>
<ol>
<li>raw count;</li>
<li>term frequency adjusted for document length : $f_{t,d}$ ÷ (number of words in d)</li>
<li>$\log \left(1+f_{t, d}\right)$</li>
</ol>
<p>There are many kinds of <strong>inverse term frequency</strong> idf(<em>t</em>,<em>d</em>). For example, it coule be</p>
<ol>
<li>$\log \frac{N}{n_{t}}=-\log \frac{n_{t}}{N}$</li>
<li>$\log \left(\frac{N}{1+n_{t}}\right)$</li>
</ol>
<h3 id="Relevance"><a href="#Relevance" class="headerlink" title="Relevance"></a>Relevance</h3><p>Relevance of information (like title and query, document and query) coule be turned into a ranking problem.</p>
<p><strong>ad-hoc retrieval task</strong>: title and query</p>
<h3 id="Natural-language-sentence-matching-NLSM"><a href="#Natural-language-sentence-matching-NLSM" class="headerlink" title="Natural language sentence matching (NLSM)"></a>Natural language sentence matching (NLSM)</h3><p>two types of deep learning frameworks[1]:</p>
<ol>
<li>“Siamese” architecture(like DSSM). Two sentences are fed into a same neural network and compute their similarity by sth like cosine similarity. Disadvantage is that there is no explicit interaction between two sentences in this approach.</li>
<li>matching aggregation. Smaller units like words are matched and the results are aggregated by a CNN or LSTM. This framework captures more interactive features rather than only general information.</li>
</ol>
<h6 id="Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences"><a href="#Bilateral-Multi-Perspective-Matching-for-Natural-Language-Sentences" class="headerlink" title="Bilateral Multi-Perspective Matching for Natural Language Sentences"></a>Bilateral Multi-Perspective Matching for Natural Language Sentences</h6><p><strong>Word Representation Layer</strong>:</p>
<p>representation of word: word embedding concat char embedding. Note that the char embedding is fed to a RNN and the hidden state of the last time step is utilised.</p>
<p>For example, <strong>“sentence matching”</strong>, the embedding is the concatenation of [sentence] word embedding and the RNN output of [s,e,b,t,e,n,c,e] char embedding.</p>
<p><strong>Context Representation Layer</strong>:</p>
<p>The embeddings are fed to a bi-lstm to compute feature vectors(context related vectors).</p>
<p>搜索召回:</p>
<ol>
<li>精确匹配：字符完全匹配</li>
<li>语义相关</li>
</ol>
<p>语义相关(相关性模型)：</p>
<ol>
<li>树模型: xgboost等。特征如bm25，word2vec向量等等，特征工程在语义相关模型不好做，因此树模型往往效果一般</li>
<li>深度学习模型. 通过深度学习自动提取特征，早期模型有DSSM, ESIM等，现在用bert，elmo等</li>
</ol>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://arxiv.org/pdf/1702.03814.pdf" target="_blank" rel="noopener">Bilateral Multi-Perspective Matching for Natural Language Sentences</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/29/Data-Structure/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/29/Data-Structure/" itemprop="url">Data_Structure</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-29T15:12:21+08:00">
                2019-05-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h6 id="heap"><a href="#heap" class="headerlink" title="heap:"></a>heap:</h6><p>Heaplify should start from the end. BUILD-MAX-HEAP starts from the last second layer because the last layer has no children.</p>
<p>If there are several same records and the records are exchanged during sorting, the sorting algorithm isn’t stable, otherwise it is stable</p>
<h6 id="Red-Black-Tree-RBTree"><a href="#Red-Black-Tree-RBTree" class="headerlink" title="Red Black Tree(RBTree)"></a>Red Black Tree(RBTree)</h6><p>RBTree is also known as self-balanced BST(binary search tree). </p>
<p>The height of a RB-tree with n non-NIL(NULL) nodes is at most $2 \log (n+1)$.</p>
<p>$\mathrm{O}(\log (\mathrm{n}))$ deterministic INSERT/DELETE/SEARCH.</p>
<p>并查集:</p>
<p>解决路径连通等问题。并查集就是一棵树，记录一个根结点下面跟它连通的节点。要查询两个节点是否连通，查询父节点是否同一个即可。</p>
<p>并查集的合并就是将一个父节点作为另一个父节点的子节点即可。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/08/HMM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/08/HMM/" itemprop="url">HMM</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-08T17:34:22+08:00">
                2019-05-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>$$<br>\text { Markov Assumption: } P\left(q_{i}=a | q_{1} \ldots q_{i-1}\right)=P\left(q_{i}=a | q_{i-1}\right)<br>$$<br>Current hidden state $q_t$ only relates to the hidden state of the last time step $q_{t-1}$.</p>
<p>Component of a HMM:</p>
<ol>
<li>$Q=q_{1} q_{2} \dots q_{N}$     a set of N states</li>
<li>$A=a_{11} \ldots a_{i j} \ldots a_{N N}$       a transition matrix A</li>
<li>$O=O_{1} O_{2} \dots O_{T}$                a sequence of observations on T time steps</li>
<li>$B=b_{i}\left(o_{t}\right)$                            a sequence of observation likelihoods, also called emission probabilities, which is given a hidden state, the probability of getting the observation.</li>
<li>$\pi=\pi_{1}, \pi_{2}, \ldots, \pi_{N}$               an initial probability distribution over states.</li>
</ol>
<h4 id="Three-fundamental-problems-of-HMM"><a href="#Three-fundamental-problems-of-HMM" class="headerlink" title="Three fundamental problems of HMM"></a>Three fundamental problems of HMM</h4><ol>
<li>Likelihood computation. Given HMM model $\lambda=(A, B)$ and an observation sequence <em>O</em>, determine probability $P(O | \lambda)$.</li>
<li>Decoding(finding the best path). Given an observation sequence <em>O</em> and an HMM $\lambda=(A, B)$, discover the best hidden state sequence <em>Q</em>.</li>
<li>Learning. Given an observation sequence <em>O</em> and the set of states<br>in the HMM, learning the HMM parameters which are the transition matrix A and emision matrix B. </li>
</ol>
<h4 id="Likelihood-Computation-The-Forward-Algorithm"><a href="#Likelihood-Computation-The-Forward-Algorithm" class="headerlink" title="Likelihood Computation: The Forward Algorithm"></a>Likelihood Computation: The Forward Algorithm</h4><ol>
<li>Initialization: $$\alpha_{1}(j)=\pi_{j} b_{j}\left(o_{1}\right) 1 \leq j \leq N$$</li>
<li>Recursion: $$\alpha_{t}(j)=\sum_{i=1}^{N} \alpha_{t-1}(i) a_{i j} b_{j}\left(o_{t}\right) ; \quad 1 \leq j \leq N, 1&lt;t \leq T$$</li>
<li>Termination: $$P(O | \lambda)=\sum_{i=1}^{N} \alpha_{T}(i)$$</li>
</ol>
<p>Where $\alpha_{t}(j)=\sum_{i=1}^{N} \alpha_{t-1}(i) a_{i j} b_{j}\left(o_{t}\right) $ means the probability of hidden state j at time t is computed from summing the probability of all the last hidden state multiplied by the transition probability $a_{ij}$ and emission probability $b_{j}\left(o_{t}\right)$.</p>
<p>$\alpha_{t}(j)$ represents the probability of being in state j after seeing the first <em>t</em> observations, given the model parameters $\lambda$ which are the transition matrix A and emision matrix B.</p>
<h4 id="Decoding-The-Viterbi-Algorithm"><a href="#Decoding-The-Viterbi-Algorithm" class="headerlink" title="Decoding: The Viterbi Algorithm"></a>Decoding: The Viterbi Algorithm</h4><ol>
<li>Initialization:$$\begin{array}{rl}{v_{1}(j)=\pi_{j} b_{j}\left(o_{1}\right)} &amp; {1 \leq j \leq N} \ {b t_{1}(j)=0} &amp; {1 \leq j \leq N}\end{array}$$.</li>
<li>Recursion: $\begin{aligned} v_{t}(j) &amp;=\max <em>{i=1}^{N} v</em>{t-1}(i) a_{i j} b_{j}\left(o_{t}\right) ; \quad 1 \leq j \leq N, 1&lt;t \leq T \ b t_{t}(j) &amp;=\underset{i=1}{\operatorname{argmax}} v_{t-1}(i) a_{i j} b_{j}\left(o_{t}\right) ; \quad 1 \leq j \leq N, 1&lt;t \leq T \end{aligned}$.</li>
<li>Termination: $\begin{aligned} \text { The best score: } &amp; P <em>=\max <em>{i=1}^{N} v</em>{T}(i) \ \text { The start of backtrace: } &amp; q_{T^{</em>}}=\underset{i=1}{\operatorname{argmax}} v_{T}(i) \end{aligned}$.</li>
</ol>
<p>The difference is that in viterbi, it is max operation. And it is sum operation in The Forward Algorithm. The max operation means the most probable path.</p>
<p>$bt_1(j)=0$ because there is no preceeding state at time step 0.</p>
<p><strong>Performance Optimization</strong></p>
<p>For large alpha matrix, for example, many time steps and many states, storing the whole apha matrix $v(j)$ may be expensive. Therefore storing the sequence of back pointers may be economic. Otherwise just store the alpha matrix and perform argmax would be faster because loop is time consuming.  </p>
<h4 id="Learning"><a href="#Learning" class="headerlink" title="Learning"></a>Learning</h4><p>$$<br>\xi_{t}(i, j)=P\left(q_{t}=S_{i}, q_{t+1}=S_{j} | O, \lambda\right)<br>$$</p>
<p>where $\xi_{t}(i, j)$ is the probability of state $S_i$ at time t and state $S_j$ at time t+1.<br>$$<br>\begin{aligned} \xi_{t}(i, j) &amp;=\frac{\alpha_{t}(i) a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j)}{P(O | \lambda)} \ &amp;=\frac{\alpha_{t}(i) a_{i} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j)}{\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t}(i) a_{i j} b_{j}\left(O_{t+1}\right) \beta_{t+1}(j)} \end{aligned}<br>$$<br>where the numeration term is $P\left(q_{t}=S_{i}, q_{t+1}=S_{j}, O | \lambda\right)$ and the demoninator is $P(O | \lambda)$.<br>$$<br>\gamma_{t}(i)=\frac{\alpha_{t}(i) \beta_{t}(i)}{P(O | \lambda)}=\frac{\alpha_{t}(i) \beta_{t}(i)}{\sum_{i=1}^{N} \alpha_{t}(i) \beta_{t}(i)}<br>$$<br>$\gamma_{t}(i)=P\left(q_{t}=s_{i} | O, \lambda\right)$ is the probability of being in state $S_i$ at time t, given the<br>observation sequence O, and the model $\lambda$. $\alpha_{t}(i)$ accounts for the partial observation sequence , an $O_{1} O_{2} \dots O_{t}$ and state $S_i$ at t, while $\beta_{t}(i)$ accounts for the remainder of the observation sequence $O_{1} O_{2} \dots O_{t}$ given state $S_i$ at t. The normalization factor ${P(O | \lambda)}={\sum_{i=1}^{N} \alpha_{t}(i) \beta_{t}(i)}$ makes $\gamma_{t}(i)$ a probability measure so that $\sum_{i=1}^{N} \gamma_{t}(i)=1$.<br>$$<br>\gamma_{t}(i)=\sum_{j=1}^{N} \xi_{t}(i, j)<br>$$</p>
<p>$\sum_{t=1}^{T-1} \gamma_{t}(i)$=expected number of transitions from $S_i$.</p>
<p>$\sum_{t=1}^{T-1} \xi_{t}(i, j)=$ expected number of transitions from $S_i$ to $S_j$.</p>
<h5 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h5><p>Matrix multiplication(inner product):</p>
<ol>
<li>The first way:</li>
</ol>
<p>$$<br>state_transition={\alpha_{t-1}^T \cdot transition_matrix}<br>$$</p>
<p>$$<br>\alpha_{t}(j)={state_transition}^T \times emission_matrix<br>$$</p>
<p>​                 where $\alpha_{t}(j)$ is the probability vector of being at state j at time step t.</p>
<ol>
<li>The second way:<br>$$<br>\alpha_{t}(i, j)=\alpha_{t-1} \cdot emission_matrix \times transition_matrix<br>$$<br>where $\alpha_{t}(i, j)$ is a matrix. Summation or maximization needed to be done for forward algorithm or backward algorithm respectively.</li>
</ol>
<p>Hadamard product<br>$$<br>transition_matrix \times emission_matrix \times \alpha_{t-1}<br>$$<br>where $\alpha_{t-1}$ is supposed to be broadcasted automatically or manually.</p>
<p>$$<br>P(X | Y, Z)=\frac{P(X, Y | Z)}{P(Y | Z)}<br>$$</p>
<h3 id="Training-of-HMM"><a href="#Training-of-HMM" class="headerlink" title="Training of HMM"></a>Training of HMM</h3><ol>
<li><p>EM algorithm. First initialize the model randomly.</p>
<ul>
<li>E step: Given the model, calculate the $\gamma_{t}(i)$ which is the posterier probability of state i at time step t. Here the latent variables are states.</li>
<li>M step: update parameters of the model.</li>
</ul>
</li>
<li><p>Gradient descent.First initialize the model randomly.</p>
<ul>
<li>Given the model, calculate the $\gamma_{t}(i)$ which is the posterier probability of state i at time step t.</li>
<li>Perform BP. The loss function is the cross-entropy of the  posterier probability and the real label. For example, there are 4 labels. In the time step t, $\gamma_{t}(i)$ is [0.1, 0.4, 0.3, 0.2] and the real label is [0, 0, 1, 0]. Here is where the cross-entropy come into play.</li>
</ul>
</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/04/search-engine/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/04/search-engine/" itemprop="url">search_engine</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-05-04T22:40:22+08:00">
                2019-05-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>紧密度：衡量切词后词在文档中出现位置的距离。紧密度高，切词后在doc出现相隔的距离不能太远。</p>
<h3 id="semantic-matching-语义匹配"><a href="#semantic-matching-语义匹配" class="headerlink" title="semantic matching(语义匹配)"></a>semantic matching(语义匹配)</h3><h5 id="表示层多粒度融合"><a href="#表示层多粒度融合" class="headerlink" title="表示层多粒度融合"></a>表示层多粒度融合</h5><p>在表示层切词的地方，同时输入基础和短语粒度的切词结果作为输入</p>
<h5 id="增加先验信息"><a href="#增加先验信息" class="headerlink" title="增加先验信息"></a>增加先验信息</h5><p>比如词语共现信息，collocation等等[1].</p>
<h4 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h4><p>“精度”的分母是所有提取了的文档数目，而“召回”的分母则是所有相关的文档数目。</p>
<p>Pointwise VS. Pairwise</p>
<p>Pointwise是给定单个输入，输出对应单个label(评分之类的)</p>
<p>pairwise是给定一对输入，输出对应是否输入的句子对是否相关</p>
<p>Listwise 是给定一堆输入，输出这堆输入对应的分数(例如相关性)，损失函数就是衡量两个分布差异的东西。</p>
<h4 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h4><p>PV:Page View, 页面浏览量</p>
<p>UV:unique visitor即独立访客数</p>
<p>QV:query view, 用户输入query次数</p>
<p><strong>转化率=转化次数/访问次数</strong></p>
<p><strong>点击量/展现量=点击率</strong></p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://www.jiqizhixin.com/articles/2017-06-15-5" target="_blank" rel="noopener">百度NLP | 神经网络语义匹配技术</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/04/30/spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/04/30/spark/" itemprop="url">spark</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-04-30T10:36:47+08:00">
                2019-04-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Support for gzip input files should work the same as it does in Hadoop. For example, <code>sc.textFile(&quot;myFile.gz&quot;)</code> should automatically decompress and read gzip-compressed files (<code>textFile()</code> is actually <a href="https://github.com/mesos/spark/blob/v0.7.0/core/src/main/scala/spark/SparkContext.scala#L239" target="_blank" rel="noopener">implemented</a> using Hadoop’s <code>TextInputFormat</code>, which supports gzip-compressed files).</p>
<p>note that if you call <code>sc.textFile()</code> on a gzipped file, Spark will give you an RDD with only 1 partition (as of 0.9.0). This is because gzipped files are <a href="http://mail-archives.apache.org/mod_mbox/spark-user/201310.mbox/" target="_blank" rel="noopener">not splittable</a>. Therefore processing of a gzipped file has to be in a worker. After processing, it could be repartitioned. If you don’t repartition the RDD somehow, any operations on that RDD will be limited to a single core.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rdd.getNumPartitions()</span><br></pre></td></tr></table></figure>
<p>Get the number of partitions of a rdd.</p>
<p>spark hdfs location <figure class="highlight plain"><figcaption><span>there is another slash after two consecutive slashes!!!</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">##### Dependency of spark</span><br><span class="line"></span><br><span class="line">It is fine to zip your dependency and use —py-file to submit the dependency. Make sure to figure out the path of the unzipped files and import them. ```sc.addFile``` or ```sc.addPyFile``` is able to broadcast the dependency files to all workers.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### Differences between coalesce and repartition</span><br><span class="line"></span><br><span class="line">The repartition algorithm does a full shuffle of the data and creates equal sized partitions of data. coalesce combines existing partitions to avoid a full shuffle.[1]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Row of pyspark resembles dict. Accessing values like dict.</span><br><span class="line"></span><br><span class="line">## Hadoop</span><br><span class="line"></span><br><span class="line">```sh</span><br><span class="line">hadoop dfs -test -d ....</span><br></pre></td></tr></table></figure></p>
<p>test whether a directory exist or not.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;"><a href="https://medium.com/@mrpowers/managing-spark-partitions-with-coalesce-and-repartition-4050c57ad5c4" target="_blank" rel="noopener">Managing Spark Partitions with Coalesce and Repartition</a><a href="#fnref:1" rev="footnote"> ↩</a></span></li></ol></div></div>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/14/Probabilistic-Graph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/14/Probabilistic-Graph/" itemprop="url">Probabilistic_Graph</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-14T23:11:10+08:00">
                2019-03-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Both Markovian random field and Conditional random field have the same type of functions. The difference is that Markovian random field is a generative model while CRF is a discriminative model.</p>
<p><strong>generative model</strong> calculate the distribution over all variables.</p>
<p><strong>discriminative model</strong> calculate the conditional probability directly.</p>
<p>EM算法本质可看作先根据参数\(\theta\)计算出关于z的概率分布Q(z)=\(p(z|x,\theta)\),然后用这个z的概率分布去计算似然函数，选择使似然函数最大的参数集\(\theta\).</p>
<p>EM算法缺点：收敛到局部极值点</p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><p>概率图的推断可采用变量消除法，变量消除的顺序对计算量有着非常重要的影响。一种改进就是消息传播法，将概率图中的变量表示成一棵树，选取一个变量作为根结点，所有其他节点将信息传播至根结点后，根结点再将信息往下传播，直到叶子节点，叶子节点是我们关心的变量。</p>
<p>变量消除法或消息传播法都是精确计算，一种替代方式是通过采样来进行近似计算。</p>
<h5 id="Markov-Chain-Monte-Carlo-MCMC"><a href="#Markov-Chain-Monte-Carlo-MCMC" class="headerlink" title="Markov Chain Monte Carlo(MCMC)"></a>Markov Chain Monte Carlo(MCMC)</h5>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/03/13/VAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/03/13/VAE/" itemprop="url">VAE</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2019-03-13T15:10:27+08:00">
                2019-03-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="Auto-Encoder"><a href="#Auto-Encoder" class="headerlink" title="Auto Encoder"></a>Auto Encoder</h2><p>Encoder is used to map input to hidden states while decoder is supposed to reconstruct input from hidden states.</p>
<p>Auto encoder is supposed to capture used information in input rather than simply copying data. It could be regarded that the decoder is simply an auxiliary task.</p>
<p>An autoencoder whose code(hidden states) dimension is less than the input dimension is called <strong>undercomplete</strong>. It is supposed to learn the most salient features of data distribution. </p>
<p>If the capacity of encoder and decoder is too much, it is possible that encoder simply map the input to an unique code(like hash with collision) while decoder simply reconstruct input from that code without leanring useful information.</p>
<p>Tricks to train auto encoder including regularizing the auto encoder such that features are sparse.</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="MK_LEE" />
            
              <p class="site-author-name" itemprop="name">MK_LEE</p>
              <p class="site-description motion-element" itemprop="description">Passionate about NLP</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">97</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MK_LEE</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
