<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP," />










<meta name="description" content="Word sense disambiguation(WSD)Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the perfor">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Word_Senses">
<meta property="og:url" content="http://yoursite.com/2018/07/06/Word-Senses/index.html">
<meta property="og:site_name" content="Home of MasKong">
<meta property="og:description" content="Word sense disambiguation(WSD)Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the perfor">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-07-06T09:48:13.686Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Word_Senses">
<meta name="twitter:description" content="Word sense disambiguation(WSD)Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the perfor">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/06/Word-Senses/"/>





  <title>Word_Senses | Home of MasKong</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Home of MasKong</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Lesson learnt from lessons</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/06/Word-Senses/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Home of MasKong">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Word_Senses</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-07-06T12:43:04+08:00">
                2018-07-06
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Word-sense-disambiguation-WSD"><a href="#Word-sense-disambiguation-WSD" class="headerlink" title="Word sense disambiguation(WSD)"></a>Word sense disambiguation(WSD)</h1><p>Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the performance of tasks such as question answering and machine translation.</p>
<p>Two kinds of WSD tasks:</p>
<ul>
<li>lexical sample</li>
<li>all-words</li>
</ul>
<p>Lexical sample: given a set of target words and with an inventory of senses from some lexicon. Supervised learning is employed to solve this problem at usual. Hand-label a small set of words and corresponding sense, and feed into a classifier.</p>
<p>All-words: given entire texts and a lexicon that contains an inventory of senses for each entry. The objective is to disambiguate every content word in the text. It is impractical to train a classifier for each term because the number of words is large.</p>
<h3 id="Supervised-Learning-Approach"><a href="#Supervised-Learning-Approach" class="headerlink" title="Supervised Learning Approach"></a>Supervised Learning Approach</h3><ol>
<li>Feature Vector Extraction. Process the sentence in a context window to extract feature vector.</li>
</ol>
<p>There are two kinds of feature vector:</p>
<ul>
<li>collocation features. Multiple words in a position-specific relationship to a target word. It is effective at encoding local lexical and grammatical information that usually isolate a given sense. High performing systems generally use POS tags and word collocations of length 1,2 and 3 from a window of words 3.</li>
<li>bag-of-words: neglecting the position information, just record number of context words in a context window. The context words could be pre-selected(frequent used words) given a target word.</li>
</ul>
<p><strong>Evaluation</strong>: most frequent sense as baseline. Most freauent sense usually choose the most frequently used sense given a context.</p>
<h3 id="Dictionary-and-Thesaurus"><a href="#Dictionary-and-Thesaurus" class="headerlink" title="Dictionary and Thesaurus"></a>Dictionary and Thesaurus</h3><h5 id="The-Lesk-Algorithm"><a href="#The-Lesk-Algorithm" class="headerlink" title="The Lesk Algorithm"></a>The Lesk Algorithm</h5><p>It is a family of dictionary-based algorithms.<br>Simplified algorithm compute the number of overlapped words between a given context and all of the word senses. Output the sense with the most overlapped words.</p>
<p>Original Lesk Algorithm compute the number of overlapped words between sense of each context word and the sense of the target word.<br>i.e.: Bank is a financial institute for deposit.<br>Simplified algorithm compute the number of overlapping words between each sense and the context [financial,institute, deposit] and output the sense with the most overlapped words.<br>The original algorithm compute all senses with all senses of each word in the context  [financial,institute, deposit] and output the sense with the most overlapped words.</p>
<p><strong>Notes</strong>:stop-words are not taken into consideration.</p>
<p>Problem: Entries of the dictionary for the target word are not enough which reduce the chance of overlapping.</p>
<p>Solution: To expand the sense, include senses of similar words which may increase the number of overlapping words.</p>
<p>The best solution is to utilize sense-tagged corpus, which is so called <strong><strong>Corpus Lesk Algorithm</strong></strong>. To expand the signature for a word sense, add all word senses with the same label. i.e.: add word sense of large, huge to big. Instead of employ a stop-list, the algorithm apply IDF to reduce the weight of function words like the,of. This algorithm is usually a baseline.</p>
<p>The bag-of-words approach could be improved by combining the Lesk algorithm. The glosses and example sentences for the target sense in WordNet would be used as words features along with <strong>sense-tagged</strong> corpus like SemCor.</p>
<h5 id="Graph-based-Methods"><a href="#Graph-based-Methods" class="headerlink" title="Graph-based Methods"></a>Graph-based Methods</h5><p>A graph is employed to represent words. Each sense is a node and relations between senses are edges. It is usually an undirected graph.</p>
<p>The correct sense is the one that is central in the graph. Use degree to decide centrality.</p>
<p>Another approach is to assign probability to nodes according to personalized page rank.</p>
<p><strong>Problem of both supervised and dictionary-based approach</strong>: need hand-built resouces</p>
<h3 id="Semi-Supervised-Bootstrapping"><a href="#Semi-Supervised-Bootstrapping" class="headerlink" title="Semi-Supervised: Bootstrapping"></a>Semi-Supervised: Bootstrapping</h3><ol>
<li>Use a small set of labeled data to train the classifier. </li>
<li>Do classification.</li>
<li>Select the result with high confidence and add to the training set.</li>
<li>Train the classifier and repeat step 2 and 3.</li>
<li>Stop until there is no untagged data left or reach a specific error rate threshold.</li>
</ol>
<p>To construct the training set, hand-label it or use heuristic to help.</p>
<p>Heuristic:</p>
<ul>
<li><strong>one sense per collocation</strong>: certain words or phrases strongly indicates a specific sense.</li>
<li><strong>one sense per discourse</strong>:a particular word appearing multiple times in a text or discourse often appeared with the same sense. This heuristic seems to hold better for coarse-grained senses and particularly for cases of homonymy rather than polysemy.</li>
</ul>
<h3 id="Unsupervised-Approach-Word-Sense-Induction-WSI"><a href="#Unsupervised-Approach-Word-Sense-Induction-WSI" class="headerlink" title="Unsupervised Approach:Word Sense Induction(WSI)"></a>Unsupervised Approach:Word Sense Induction(WSI)</h3><p>It is actually a clustering of word senses. </p>
<ol>
<li>For each token(sense) of a word in the corpus, use context word vector \(\vec{c}\) to represent it. </li>
<li>Do clustering s.t. there is N clusters and each token \(\vec{c}\) is assigned to a cluster. Each cluster defines a sense.</li>
<li>Recompute the center s.t. the center vector \(\vec{s_j}\) represent a sense.</li>
</ol>
<p>To do word sense disambiguation:</p>
<ol>
<li>compute context word vector \(\vec{c}\) to represent the sense of the word. </li>
<li>Do clustering and assign the context vector to a cluster.</li>
</ol>
<p>All we need is a clustering algorithm and a distance metric between vectors.</p>
<p>A frequently used technique in language applications is known as agglomerative<br>clustering.</p>
<p>Recent algorithms have also used topic modeling algorithms like Latent Dirichlet Allocation (LDA), another way to learn clusters of words based on their distributions.</p>
<p>It is fair to say that no evaluation metric for this task has yet become standard.</p>
<h1 id="Word-Similarity-Thesaurus-Methods"><a href="#Word-Similarity-Thesaurus-Methods" class="headerlink" title="Word Similarity(Thesaurus Methods)"></a>Word Similarity(Thesaurus Methods)</h1><p>Word similarity refers to meaning(synonyms).<br>Word relatedness characterizes relationships(antonyms,synonyms).</p>
<h5 id="Path-length-based-Approach"><a href="#Path-length-based-Approach" class="headerlink" title="Path-length based Approach"></a>Path-length based Approach</h5><p>Thesaurus could be viewed as a graph.</p>
<p>The simplest thesaurus-based algorithms use the edge between words to measure similarity.</p>
<p>pathlen(c1, c2) = 1 + edges in the shortest path between the sense nodes c1 and c2.</p>
<p>path-length based similarity:<br>$$sim_{path}(c_1,c_2)=\frac{1}{pathlen(c_1, c_2)}$$</p>
<p>Path lengths are not the same in different level of the hierarchy.<br>It is possible to refine path-based algorithms with normalizations based on depth in the hierarchy. In general an approach that independently represent the distance associated with each edge it is preferred.</p>
<p>For data without sense tag(words are not presented according to their senses), measure similarity according to senses of words. If there is a pair of senses is similar, then two words are similar.</p>
<p>$$word_{sim}(w_1,w_2)=\max_{c_1\in senses(w_1)\  c_2\in senses(w_2)} sim(c_1, c_2)$$</p>
<h5 id="information-content-word-similarity-algorithms"><a href="#information-content-word-similarity-algorithms" class="headerlink" title="information-content word-similarity algorithms"></a>information-content word-similarity algorithms</h5><p>still rely on the structure of the thesaurus but also add probabilistic information derived from a corpus.</p>
<p>The corpus is represented as a set of concepts. And it is represented as a tree  structure. The leaf nodes are basic concepts and parent nodes contain the children which is a superset of concepts. Each node is assigned a probability. So p(root) = 1.</p>
<p>$$P(c) = \frac{\sum_{w\in words(c)}count(w)}{N}$$</p>
<p>c is a specific concept. The probability of a concept is number of words in the concept divided by total number of words.</p>
<p><strong>information content</strong>: information content (IC) of a concept c</p>
<p>$$IC(c) = −\log P(c) $$</p>
<p>the lowest common subsumer(LCS) of two concepts:<br>LCS(c1, c2) = the lowest common subsumer,the lowest node in the hierarchy that subsumes both c1 and c2.(The lowest parent node of c1 and c2)</p>
<p><strong>Resnik similarity</strong>: similarity between two words is related to their common information; the more two words have in common, the more similar they are. Resnik proposes to estimate the common amount of information by the information content of the lowest common subsumer of the two nodes. </p>
<p>$$sim_{Resnik}(c_1,c_2)=-\log P(LCS(c_1,c_2))$$</p>
<p><strong>Lin similarity</strong></p>
<p>Similarity Theorem: The similarity between A and B is measured by the ratio<br>between the amount of information needed to state the commonality of A and<br>B and the information needed to fully describe what A and B are.<br>$$sim_{Lin}(A,B)=\frac{common(A,B)}{description(A,B)}$$</p>
<p>the information in common between two concepts is twice the information in the lowest common subsumer LCS(c1, c2). Adding in the above definitions of the information content of thesaurus concepts<br>$$sim_{Lin}(C_1,C_2)=\frac{2\times \log P(LCS(c_1,c_2))}{\log P(c_1)+\log P(c_2)}$$</p>
<p><strong>Jiang-Conrath distance</strong> express distance rather than similarity, work as well as or better than all the other thesaurus-based methods:</p>
<p>$$dist_{JC}(c_1, c_2) = 2\times \log P(LCS(c_1, c_2))−(\log P(c_1) +\log P(c_2)) $$</p>
<p>It could be transferred to similarity by take reciprocal.<br>$$sim_{JC}=\frac{1}{dist_{JC}}$$</p>
<p><strong>dictionary-based method</strong>: This method makes use of glosses, which are, in general, a property of dictionaries rather than thesauruses. Two concepts/senses are similar if their glosses contain overlapping words.</p>
<p>For each n-word phrase that occurs in both glosses, Extended Lesk adds in a<br>score of \(n^2\).(the relation is non-linear because of the Zipfian relationship between lengths of phrases and their corpus frequencies; longer overlaps are rare, so they should be weighted more heavily)</p>
<p>Given such an overlap function, when comparing two concepts (synsets), Extended<br>Lesk not only looks for overlap between their glosses but also between the<br>glosses of the senses that are hypernyms, hyponyms, meronyms, and other relations<br>of the two concepts. </p>
<p>If we just considered hyponyms and defined<br>gloss(hypo(A)) as the concatenation of all the glosses of all the hyponym senses of<br>A, the total relatedness between two concepts A and B might be</p>
<p>$$similarity(A,B) = overlap(gloss(A), gloss(B))+overlap(gloss(hypo(A)), gloss(hypo(B)))+overlap(gloss(A), gloss(hypo(B)))+overlap(gloss(hypo(A)),gloss(B))$$</p>
<p>Extended Lesk overlap measure:</p>
<p>$$sim_{eLESK}(c_1,c_2) =\sum_{r,q\in RELS}overlap(gloss(r(c_1)),gloss(q(c_2)))$$</p>
<p>RELS is the set of possible relations whose glosses we compare. Possible relations are  like hypernyms, hyponyms, meronyms, and other relations of the two concepts.</p>
<h2 id="Evaluating-Thesaurus-Based-Similarity"><a href="#Evaluating-Thesaurus-Based-Similarity" class="headerlink" title="Evaluating Thesaurus-Based Similarity"></a>Evaluating Thesaurus-Based Similarity</h2><p>The most common intrinsic evaluation metric computes the correlation coefficient between an algorithm’s word similarity scores and word similarity ratings assigned by humans. </p>
<p>Zipf’s law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.The most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="https://en.wikipedia.org/wiki/Zipf%27s_law" target="_blank" rel="noopener">Zipf’s law</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/05/vector-semantics/" rel="next" title="vector_semantics">
                <i class="fa fa-chevron-left"></i> vector_semantics
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">MK LEE</p>
              <p class="site-description motion-element" itemprop="description">Passionate about CS</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">tags</span>
                
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Word-sense-disambiguation-WSD"><span class="nav-number">1.</span> <span class="nav-text">Word sense disambiguation(WSD)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Supervised-Learning-Approach"><span class="nav-number">1.0.1.</span> <span class="nav-text">Supervised Learning Approach</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dictionary-and-Thesaurus"><span class="nav-number">1.0.2.</span> <span class="nav-text">Dictionary and Thesaurus</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#The-Lesk-Algorithm"><span class="nav-number">1.0.2.0.1.</span> <span class="nav-text">The Lesk Algorithm</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Graph-based-Methods"><span class="nav-number">1.0.2.0.2.</span> <span class="nav-text">Graph-based Methods</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semi-Supervised-Bootstrapping"><span class="nav-number">1.0.3.</span> <span class="nav-text">Semi-Supervised: Bootstrapping</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Unsupervised-Approach-Word-Sense-Induction-WSI"><span class="nav-number">1.0.4.</span> <span class="nav-text">Unsupervised Approach:Word Sense Induction(WSI)</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Word-Similarity-Thesaurus-Methods"><span class="nav-number">2.</span> <span class="nav-text">Word Similarity(Thesaurus Methods)</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Path-length-based-Approach"><span class="nav-number">2.0.0.0.1.</span> <span class="nav-text">Path-length based Approach</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#information-content-word-similarity-algorithms"><span class="nav-number">2.0.0.0.2.</span> <span class="nav-text">information-content word-similarity algorithms</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Evaluating-Thesaurus-Based-Similarity"><span class="nav-number">2.1.</span> <span class="nav-text">Evaluating Thesaurus-Based Similarity</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">2.1.1.</span> <span class="nav-text">Reference</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MK LEE</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  


  

  

</body>
</html>
