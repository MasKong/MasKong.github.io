<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="NLP," />










<meta name="description" content="In most text classificationapplications, however, using a stop word list doesn’t improve performance,and so it is more common to make use of the entire vocabulary and not use a stopword list Naive Bay">
<meta name="keywords" content="NLP">
<meta property="og:type" content="article">
<meta property="og:title" content="Sentiment_Classification_Naive_Bayes">
<meta property="og:url" content="http://yoursite.com/2018/06/29/Sentiment-Classification/index.html">
<meta property="og:site_name">
<meta property="og:description" content="In most text classificationapplications, however, using a stop word list doesn’t improve performance,and so it is more common to make use of the entire vocabulary and not use a stopword list Naive Bay">
<meta property="og:locale" content="default">
<meta property="og:updated_time" content="2018-08-23T01:26:57.113Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sentiment_Classification_Naive_Bayes">
<meta name="twitter:description" content="In most text classificationapplications, however, using a stop word list doesn’t improve performance,and so it is more common to make use of the entire vocabulary and not use a stopword list Naive Bay">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: 'PJWL3PD75I',
      apiKey: '5589833aa2fa703729b4e7e939ac7dec',
      indexName: 'test_index',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/06/29/Sentiment-Classification/"/>





  <title>Sentiment_Classification_Naive_Bayes | </title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title"></span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/06/29/Sentiment-Classification/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="MK_LEE">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Sentiment_Classification_Naive_Bayes</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-06-29T10:15:37+08:00">
                2018-06-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>In most text classification<br>applications, however, using a stop word list doesn’t improve performance,<br>and so it is more common to make use of the entire vocabulary and not use a stop<br>word list</p>
<h3 id="Naive-Bayes-Classifier-for-Sentiment-Analysis"><a href="#Naive-Bayes-Classifier-for-Sentiment-Analysis" class="headerlink" title="Naive Bayes Classifier for Sentiment Analysis"></a>Naive Bayes Classifier for Sentiment Analysis</h3><p>Correct estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well.</p>
<p>The assumption that terms are independent does not hold. But in terms of classification, the class that a document belongs is usually irrelevant to position. This somehow reduce the influence of the dependence.</p>
<p>i.e.: Finance industry is promising in HK.<br>HK finance is booming.</p>
<p>$$C_{NB}=\underset{c\in C}{argmax}\ logP(c)+\sum_{i \in positions}logP(w_i<br>|c)$$</p>
<p>Each word \(w_i\) could be viewed as a feature for a specific document.</p>
<h3 id="Maximum-Likelihood-to-train-Naive-Bayes"><a href="#Maximum-Likelihood-to-train-Naive-Bayes" class="headerlink" title="Maximum Likelihood to train Naive Bayes"></a>Maximum Likelihood to train Naive Bayes</h3><p>$$\hat{p}(c)=\frac{N_c}{N_{doc}}$$</p>
<p>The standard solution for an unknown word that doesn’t show up in the training set is to ignore such words—remove them from the test document and not include any probability for them at all.</p>
<p>Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization to deal with zero count.</p>
<p>$$\hat{P}(w_i|c) = \frac{count(w_i, c) +1}{<br>(\sum_{w\in V}(count(w, c)) +|V|}$$</p>
<p><strong><strong><em>Note that the vocabulary V consists of the union of all the word types in all classes, not just the words in one class. If the denominator is the word in the class \(c_i\), then given a class, the probability of the sentence shows up is 1. That is meaningless.</em></strong></strong> </p>
<h3 id="Binary-Multinominal-Naive-Bayes-The-Bernoulli-model"><a href="#Binary-Multinominal-Naive-Bayes-The-Bernoulli-model" class="headerlink" title="Binary Multinominal Naive Bayes(The Bernoulli model)"></a>Binary Multinominal Naive Bayes(The Bernoulli model)</h3><p>Whether a word occurs or not seems to matter more than its frequency.</p>
<p>The feature in this model is not term frequency anymore. It is whether a word has shown up in the sentence.</p>
<p>Smoothing could also be employed in this model.</p>
<p>First,it remove all duplicate words such that each sentence contains only one distinct word before concatenating them into the single big document. </p>
<p>Second deal with negation during text normalization. Typically adding negation prefix such as NOT_ to every word. i.e., like -&gt; Not_like.</p>
<p>In some situations we might have insufficient labeled training data. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are pre-annotated with positive or negative sentiment.</p>
<h3 id="Naive-Bayes-as-a-Language-Model"><a href="#Naive-Bayes-as-a-Language-Model" class="headerlink" title="Naive Bayes as a Language Model"></a>Naive Bayes as a Language Model</h3><p>If we use only individual word features, and we use all of the words in the text<br>(not a subset), then naive Bayes has an important similarity to language modeling.<br>Specifically, <strong><strong>a naive Bayes model can be viewed as a set of class-specific unigram language models</strong></strong>, in which the model for each class instantiates a unigram language model.</p>
<p><strong>contingency table</strong> consists of 4 entries: True Positive, True Negative, False Positive, False Negative.</p>
<p>Accuracy doesn’t work well when<br>the classes are unbalanced.</p>
<p>Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as<br>$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$</p>
<p>Recall measures the percentage of items actually present in the input that were<br>correctly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as<br>$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$</p>
<p>precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.<br>In practice, we generally combine precision and recall into a single metric called<br>F-measure the F-measure that is defined as:<br>$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$</p>
<p>The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1<br>anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall.</p>
<p>Harmonic mean is used because it is a conservative metric; the harmonic mean of<br>two values is closer to the minimum of the two values than the arithmetic mean is.<br>Thus it weighs the lower of the two numbers more heavily.</p>
<p>In <strong>any-of</strong> or <strong>multi-label</strong> classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d.</p>
<p><strong>one-of</strong> or <strong>multinomial</strong> classification, multinomial classification in which the classes are mutually exclusive and each document or item appears in<br>exactly one class. Build a separate binary classifier trained on positive<br>examples from c and negative examples from all other classes. Now given a test<br>document or item d, we run all the classifiers and choose the label from the classifier with the highest score. </p>
<p>In <strong>macroaveraging</strong>, we compute the performance for each class, and then average over classes. In <strong>microaveraging</strong>, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important.</p>
<p>The only problem with cross-validation is that because all the data is used for<br>testing, we need the whole corpus to be blind; we can’t examine any of the data<br>to suggest possible features and in general see what’s going on. But looking at the<br>corpus is often important for designing the system. For this reason, it is common<br>to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set.</p>
<h5 id="Toy-Model"><a href="#Toy-Model" class="headerlink" title="Toy Model"></a>Toy Model</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> RegexpTokenizer</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processing_raw_text</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    intermediate = sentence.split(<span class="string">'\t'</span>)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        sen, label = intermediate[<span class="number">0</span>].strip(), intermediate[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">except</span> IndexError:</span><br><span class="line">        <span class="keyword">return</span> (<span class="keyword">None</span>,<span class="keyword">None</span>)</span><br><span class="line">    <span class="comment"># print("sen", sen)</span></span><br><span class="line">    <span class="comment"># print(label)</span></span><br><span class="line">    <span class="keyword">return</span> sen, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">processing_sentence</span><span class="params">(sentence, tokenizer=None, stop_words=True, punctuation=True, stem=False)</span>:</span></span><br><span class="line">    <span class="string">'''Given tokenizer, please deal with punctuation yourself'''</span></span><br><span class="line">    <span class="keyword">if</span> tokenizer <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> punctuation == <span class="keyword">True</span>:</span><br><span class="line">            tokenizer = RegexpTokenizer(<span class="string">r'\w+'</span>)</span><br><span class="line">            token = tokenizer.tokenize(sentence)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            token = nltk.word_tokenize(sentence)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        token = tokenizer.tokenize(sentence)</span><br><span class="line">    <span class="keyword">if</span> stop_words == <span class="keyword">True</span>:</span><br><span class="line">        sw_l = set(stopwords.words(<span class="string">'english'</span>))</span><br><span class="line">        filtered_token = [w <span class="keyword">for</span> w <span class="keyword">in</span> token <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> sw_l]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        filtered_token = token</span><br><span class="line">    <span class="keyword">if</span> stem == <span class="keyword">True</span>:</span><br><span class="line">        porter = nltk.PorterStemmer()</span><br><span class="line">        filtered_token = [porter.stem(t) <span class="keyword">for</span> t <span class="keyword">in</span> filtered_token]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> filtered_token</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_class_probability</span><span class="params">(label_l)</span>:</span></span><br><span class="line">    label_distinct = set(label_l)</span><br><span class="line">    d = &#123;key: label_l.count(key) <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    d = &#123;key: d[key] / float(sum(d.values())) <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(test_example, class_p, word_p)</span>:</span></span><br><span class="line">    <span class="string">'''predict'''</span></span><br><span class="line">    predict_l = []</span><br><span class="line">    label_distinct = set(label_l)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(test_example)):</span><br><span class="line">        p_example = &#123;key: class_p[key] <span class="keyword">for</span> key <span class="keyword">in</span> class_p.keys()&#125;</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> test_example[i]:</span><br><span class="line">            <span class="keyword">for</span> lab <span class="keyword">in</span> label_distinct:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    p_example[lab] += np.log(word_p[lab][w])</span><br><span class="line">                <span class="keyword">except</span> KeyError:</span><br><span class="line">                    p_example[lab] += np.log(word_p[lab][<span class="string">'unknown'</span>])</span><br><span class="line">                    <span class="comment"># p_example[lab] = float("inf")</span></span><br><span class="line">        predict_l.append(max(p_example, key=p_example.get))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> predict_l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_dic_unknown</span><span class="params">(dic, percent=<span class="number">20</span>)</span>:</span>  <span class="comment">#The lowest 20 percent words are regarded as unknown words.</span></span><br><span class="line">    v_l = list(dic.values())</span><br><span class="line">    v_l.sort()</span><br><span class="line">    i = int(len(v_l)*percent/<span class="number">100</span>)</span><br><span class="line">    unknown_count = <span class="number">0</span></span><br><span class="line">    d = copy.deepcopy(dic)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> dic.keys():</span><br><span class="line">        <span class="keyword">if</span> dic[k] &lt;= v_l[i]:</span><br><span class="line">            unknown_count += dic[k]</span><br><span class="line">            <span class="keyword">del</span> d[k]</span><br><span class="line">    d[<span class="string">'unknown'</span>] = unknown_count</span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line">    <span class="comment"># return d</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_k_smoothing</span><span class="params">(dic, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    d = dict(map(<span class="keyword">lambda</span> i: (i[<span class="number">0</span>],(i[<span class="number">1</span>]+k)/float(len(dic.keys())*k+sum(dic.values())) ) ,dic.items()))</span><br><span class="line">    <span class="comment"># print(d)</span></span><br><span class="line">    <span class="keyword">return</span> d</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_labeled_vocab</span><span class="params">(example_l, label_l, dic, sort=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                        smooth=False, deal_with_unknown = True)</span>:</span></span><br><span class="line">    label_distinct = set(label_l)</span><br><span class="line">    label_index = &#123;key: [<span class="number">-1</span>] <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(label_l)):</span><br><span class="line">        label_index[label_l[i]].append(i)</span><br><span class="line">    label_index = &#123;k: v[<span class="number">1</span>:] <span class="keyword">for</span> k, v <span class="keyword">in</span> label_index.items()&#125;</span><br><span class="line">    dic = copy.deepcopy(dic)</span><br><span class="line">    <span class="keyword">if</span> deal_with_unknown == <span class="keyword">True</span>:</span><br><span class="line">        dic = copy.deepcopy(process_dic_unknown(dic))</span><br><span class="line"></span><br><span class="line">    d_labeled = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> lab <span class="keyword">in</span> label_distinct:</span><br><span class="line">        d = dic.fromkeys(dic.keys(), <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> ind <span class="keyword">in</span> label_index[lab]:</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> example_l[ind]:</span><br><span class="line">                <span class="keyword">if</span> w <span class="keyword">in</span> d.keys():</span><br><span class="line">                    d[w] += <span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    d[w] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> sort==<span class="keyword">True</span>:</span><br><span class="line">            d_sorted = &#123;&#125;                   <span class="comment"># sort the key</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> sorted(d.keys()):</span><br><span class="line">                d_sorted[key] = d[key]</span><br><span class="line">            d_labeled[lab] = d_sorted</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            d_labeled[lab] = d</span><br><span class="line">    <span class="keyword">if</span> isinstance(smooth,int):</span><br><span class="line">        d_labeled = dict(map(<span class="keyword">lambda</span> i: (i[<span class="number">0</span>], add_k_smoothing(i[<span class="number">1</span>], smooth)),d_labeled.items()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> d_labeled</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calculate_conditional_probability</span><span class="params">(dic)</span>:</span></span><br><span class="line">    <span class="string">'''calculate p(w|c)'''</span></span><br><span class="line">    label_distinct = dic.keys()</span><br><span class="line">    p_dic = &#123;key: &#123;k: dic[key][k] / float(sum(dic[key].values())) <span class="keyword">for</span> k <span class="keyword">in</span> dic[key].keys()&#125;</span><br><span class="line">             <span class="keyword">for</span> key <span class="keyword">in</span> label_distinct&#125;</span><br><span class="line">    <span class="keyword">return</span> p_dic</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_data</span><span class="params">(file_l)</span>:</span></span><br><span class="line">    sen_l = []</span><br><span class="line">    label_l = []</span><br><span class="line">    <span class="keyword">for</span> file <span class="keyword">in</span> file_l:</span><br><span class="line">        <span class="comment"># print(file)</span></span><br><span class="line">        <span class="keyword">with</span> open(file,<span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            sen = <span class="number">1</span> <span class="comment">#dummy number</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">while</span> sen != <span class="string">''</span>:</span><br><span class="line">                sen = f.readline().strip()</span><br><span class="line">                <span class="comment"># print(sen)</span></span><br><span class="line">                <span class="comment"># print(sen)</span></span><br><span class="line">                s,lab = processing_raw_text(sen)</span><br><span class="line">                <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                sen_l.append(s)</span><br><span class="line">                label_l.append(lab)</span><br><span class="line">        <span class="comment"># print(l)</span></span><br><span class="line">    <span class="comment"># print(len(sen_l))</span></span><br><span class="line">    <span class="keyword">return</span> sen_l, label_l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_train_test_set</span><span class="params">(example, label, percent=<span class="number">0.2</span>)</span>:</span> <span class="comment"># % of data as test set</span></span><br><span class="line">    train_example = copy.deepcopy(example)</span><br><span class="line">    train_label = copy.deepcopy(label)</span><br><span class="line">    k = int(percent * len(example))</span><br><span class="line">    <span class="comment"># print(len(example))</span></span><br><span class="line">    index = random.sample(range(len(example)), k)</span><br><span class="line">    test_example, test_label = [], []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">        test_example.append(example[i])</span><br><span class="line">        test_label.append(label[i])</span><br><span class="line">    index.sort(reverse=<span class="keyword">True</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">        train_example.pop(i)</span><br><span class="line">        train_label.pop(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_example, train_label, test_example, test_label</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    file = <span class="string">'/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/imdb_labelled.txt'</span></span><br><span class="line">    file1 = <span class="string">'/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/amazon_cells_labelled.txt'</span></span><br><span class="line">    file2 = <span class="string">'/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/yelp_labelled.txt'</span></span><br><span class="line">    f_l = [file,file1,file2]</span><br><span class="line"></span><br><span class="line">    sen_l, label_l = read_data(f_l)</span><br><span class="line"></span><br><span class="line">    token_l = [processing_sentence(s) <span class="keyword">for</span> s <span class="keyword">in</span> sen_l]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># filtered_token = list(map(lambda t: [w for w in t if not w in stop_words], token_l))</span></span><br><span class="line">    <span class="comment"># print(token_l)</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''build training set'''</span></span><br><span class="line">    <span class="comment"># train_example = token_l[:800]</span></span><br><span class="line">    <span class="comment"># train_label = label_l[:800]</span></span><br><span class="line">    <span class="comment"># test_example = token_l[800:]</span></span><br><span class="line">    <span class="comment"># test_label = label_l[800:]</span></span><br><span class="line">    accu_l = []</span><br><span class="line">    num_iterations = <span class="number">15</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(num_iterations):</span><br><span class="line">        train_example, train_label, test_example, test_label = build_train_test_set(token_l, label_l)</span><br><span class="line"></span><br><span class="line">        all_words_dic = build_vocab(train_example)</span><br><span class="line">        <span class="comment"># print(all_words_dic)</span></span><br><span class="line">        <span class="comment"># print(process_dic_unknown(all_words_dic))</span></span><br><span class="line">        label_dic = build_labeled_vocab(train_example, train_label, all_words_dic, smooth=<span class="number">1</span>)</span><br><span class="line">        class_p = calculate_class_probability(train_label)      <span class="comment">#Propability of each class</span></span><br><span class="line">        word_p = calculate_conditional_probability(label_dic)</span><br><span class="line"></span><br><span class="line">        predict_l = predict(test_example, class_p, word_p)</span><br><span class="line">        <span class="comment"># print(predict_l)</span></span><br><span class="line">        <span class="comment"># print(test_label)</span></span><br><span class="line">        accuracy = np.sum(np.array(test_label) == np.array(predict_l))/len(test_label)</span><br><span class="line">        accu_l.append(accuracy)</span><br><span class="line">        <span class="comment"># print(accuracy)</span></span><br><span class="line">    print(<span class="string">"average accuracy: "</span>, sum(accu_l)/num_iterations)</span><br></pre></td></tr></table></figure>
<p>without smoothing and dealing with unknown word: 0.555</p>
<p>without smoothing but dealing with unknown word: 0.715</p>
<p>add-1 smoothing and dealing with unknown word: 0.82</p>
<p>add-3 smoothing and dealing with unknown word: 0.785</p>
<p>add-5 smoothing and dealing with unknown word: 0.765</p>
<p>stem average accuracy:  0.784888888888889</p>
<p>without stem average accuracy:  0.7562222222222222</p>
<h2 id="Bayesian-networks"><a href="#Bayesian-networks" class="headerlink" title="Bayesian networks"></a>Bayesian networks</h2><p>Bayesian networks represent the conditional dependence in a compat way. In real life, absolute dependence is not true most time.</p>
<p>Bayesian networks is a Directed Acyclic Graph(DAG) that model local conditional dependence. It joint probability distribution, which turns conditional probability distribution to joint probability distribution. </p>
<p>Bayesian networks is assembled by causality although causality is not necessary numerically in bayesian networks. Bayesian networks that reflect causality would be simpler and easier to think about.</p>
<p>In Bayesian networks, each variable is a node. Edges between nodes are directed, which represent dependence.</p>
<p>To calculate probability, arrage the netword properly s.t. \(p(x_1,…x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)….p(x_n|x_1,x_2….x_{n-1})\). </p>
<h3 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h3><h4 id="By-Emuneration"><a href="#By-Emuneration" class="headerlink" title="By Emuneration"></a>By Emuneration</h4><p>Emunerate the variables and their states, ioin the variables to build a huge table. i.e.: N variables each with d states, the table entries would be \(d^N\).</p>
<h4 id="By-Variable-Elimination"><a href="#By-Variable-Elimination" class="headerlink" title="By Variable Elimination"></a>By Variable Elimination</h4><p>Join the variables and sum out the probability to eliminate the variable immediately. Do it iteratively to avoid building a big table.</p>
<p>i.e.:  \(p(x_1,…x_n)p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)….p(x_n|x_1,x_2….x_{n-1})\). First join x1 to x2  to get \(p(x_2,x_1)\), and then sum out x1 to get \(p(x_2)\). Do it iteratively to get \(p(x_n)\).</p>
<p>Finding the optimal ordering is a NP problem.</p>
<h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p><a href="http://net.pku.edu.cn/~course/cs410/2015/resource/book/2012-book-StatisticalLearning-LH.pdf" target="_blank" rel="noopener">统计学习方法</a></p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/06/27/Noisy-Channel/" rel="next" title="Noisy_Channel">
                <i class="fa fa-chevron-left"></i> Noisy_Channel
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/06/29/Machine-Learning/" rel="prev" title="Machine_Learning">
                Machine_Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="MK_LEE" />
            
              <p class="site-author-name" itemprop="name">MK_LEE</p>
              <p class="site-description motion-element" itemprop="description">Passionate about NLP</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">47</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">9</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes-Classifier-for-Sentiment-Analysis"><span class="nav-number">1.</span> <span class="nav-text">Naive Bayes Classifier for Sentiment Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Maximum-Likelihood-to-train-Naive-Bayes"><span class="nav-number">2.</span> <span class="nav-text">Maximum Likelihood to train Naive Bayes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Binary-Multinominal-Naive-Bayes-The-Bernoulli-model"><span class="nav-number">3.</span> <span class="nav-text">Binary Multinominal Naive Bayes(The Bernoulli model)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes-as-a-Language-Model"><span class="nav-number">4.</span> <span class="nav-text">Naive Bayes as a Language Model</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Toy-Model"><span class="nav-number">4.0.1.</span> <span class="nav-text">Toy Model</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Bayesian-networks"><span class="nav-number"></span> <span class="nav-text">Bayesian networks</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Inference"><span class="nav-number">1.</span> <span class="nav-text">Inference</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#By-Emuneration"><span class="nav-number">1.1.</span> <span class="nav-text">By Emuneration</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#By-Variable-Elimination"><span class="nav-number">1.2.</span> <span class="nav-text">By Variable Elimination</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Reference"><span class="nav-number">2.</span> <span class="nav-text">Reference</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">MK_LEE</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://yoursite.com/2018/06/29/Sentiment-Classification/';
          this.page.identifier = '2018/06/29/Sentiment-Classification/';
          this.page.title = 'Sentiment_Classification_Naive_Bayes';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://MasKong.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
