<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[test]]></title>
    <url>%2F2018%2F07%2F27%2Ftest%2F</url>
    <content type="text"><![CDATA[Test$$\left(\begin{array}{cc}0.8944272 &amp; 0.4472136\-0.4472136 &amp; -0.8944272\end{array}\right)\left(\begin{array}{cc}10 &amp; 0\0 &amp; 5\end{array}\right)$$]]></content>
  </entry>
  <entry>
    <title><![CDATA[python]]></title>
    <url>%2F2018%2F07%2F26%2Fpython%2F</url>
    <content type="text"><![CDATA[Find the key that has the largest value in a dictionary. Assume stats is the dictionary.123max(stats, key=stats.get)max_key = max(stats, key=lambda k: stats[k])]]></content>
      <tags>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear_Algebra]]></title>
    <url>%2F2018%2F07%2F26%2FLinear-Algebra%2F</url>
    <content type="text"><![CDATA[By default, vectors are column vectors. Outer Product$$\begin{array}\ A \otimes B &amp; = \begin{pmatrix}1 \\ 2\end{pmatrix}\begin{pmatrix}10 &amp; 20\end{pmatrix} \&amp; = \begin{pmatrix}110 &amp; 120 \210 &amp; 220\end{pmatrix} \&amp; = \begin{pmatrix}10 &amp; 20 \20 &amp; 40\end{pmatrix}\end{array}$$ Outer product increase the dimension of the matrix. Outer product of a (a1∗a2) matrix and a (b1∗b2) matrix is a (a1∗a2)∗(b1∗b2) matrix. $$\begin{array} \x \otimes y&amp; = \begin{bmatrix}x_1 &amp; \cdots &amp; x_{1n} \\x_2 &amp; \cdots &amp; x_{2n} \\\cdots &amp; \cdots &amp; \cdots \\x_m &amp; \cdots &amp; x_{mn}\end{bmatrix}\begin{bmatrix}y_1 &amp; \cdots &amp; y_{1q} \\y_2 &amp; \cdots &amp; y_{2q} \\\cdots &amp; \cdots &amp; \cdots \\y_p &amp; \cdots &amp; x_{pq}\end{bmatrix} \&amp; =\begin{bmatrix}x_1y_1 &amp; \cdots &amp; x_1y_{1q} &amp; x_1y_{2} &amp; \cdots &amp; x_1y_{pq} \\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \x_{1n}y_1 &amp; \cdots &amp; x_{1n}y_{1q} &amp; x_{1n}y_{2} &amp; \cdots &amp; x_{1n}y_{pq} \x_2y_1 &amp; \cdots &amp; x_2y_{1q} &amp; x_2y_{2} &amp; \cdots &amp; x_2y_{pq} \\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \x_{mn}y_1 &amp; \cdots &amp; x_{mn}y_{1q} &amp; x_{mn}y_{2} &amp; \cdots &amp; x_{mn}y_{pq}\end{bmatrix}\end{array}$$ Element-wise Product (Hadamard product)$$\begin{array}\A \odot B &amp; = \begin{pmatrix}1 &amp; 2 \\ 3 &amp; 4\end{pmatrix}\begin{pmatrix}10 &amp; 20 \\ 30 &amp; 40 \end{pmatrix} \&amp; = \begin{pmatrix}110 &amp; 220 \330 &amp; 440\end{pmatrix} \end{array}$$ NormL1 Norm: The sum of absolute values in the matrix. $$\lVert w \rVert_1 = \textstyle \sum_{i=1}^n |w_i|$$ L2 Norm: The sqrt of the sum of squared values in the matrix. $$\lVert w \rVert_2 = \sqrt {\textstyle \sum_{i=1}^n w_i^2}$$ Identity MatrixAn identity matrix of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere. $${ I_{n}={\begin{bmatrix}\ 1&amp;0&amp;0&amp;\cdots &amp;0\\ 0&amp;1&amp;0&amp;\cdots &amp;0\\ 0&amp;0&amp;1&amp;\cdots &amp;0\\ \vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \\ 0&amp;0&amp;0&amp;\cdots &amp;1\end{bmatrix}}}$$ New$$ \left[\begin{matrix} 00 &amp; 01 &amp; 02 \ 10 &amp; 11 &amp; 12 \ 20 &amp; 21 &amp; 22 \end{matrix}\right] $$ $$ \left[\begin{matrix} 00 &amp; 01 &amp; 02 \\ 10 &amp; 11 &amp; 12 \\ 20 &amp; 21 &amp; 22 \end{matrix}\right] $$ Reference机器学习中的基本数学知识 Identity matrix]]></content>
  </entry>
  <entry>
    <title><![CDATA[Numpy]]></title>
    <url>%2F2018%2F07%2F26%2FNumpy%2F</url>
    <content type="text"><![CDATA[numpy.ufunc.at. apply function at given index. np.add.at(a, [0, 1, 2, 2], 1): add 1 to array a at index [0, 1, 2, 2] numpy.random.permutation(x):Randomly permute a sequence, or return a permuted range. 即把一序列打成乱序输出 numpy.outer(a, b, out=None);Compute the outer product of two vectors. operator *: element-wise multuply Axis = 0 is column. Axis = 1 is row a[:6:2] means the first element of every two elements from 0 to 5 array dimension: count the bracket flat attribute which is an iterator over all the elements of the array: np.nditer(a, flags=[‘f_index’]) is used to iterate elements access multiple columns elements: a[b,c] while b is from 0 to n, c shows in each line which element you want to access. https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#indexing-slicing-and-iterating access nth column: a[:,n] transpose an one-dimension array: a.reshape(n,len(a)) multi-dimensional array, count the dimension from low to high np.argsort()Returns the indices that would sort an array.numpy.bincountCount number of occurrences of each value in array of non-negative ints. Np.argmax() Returns the indices of the maximum values along an axis.Np.nonzero() return tuple that consists of arrays. The first array indicates dimension, the second one indicates location(index)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Recurrent_Neural_Network]]></title>
    <url>%2F2018%2F07%2F25%2FRecurrent-Neural-Network%2F</url>
    <content type="text"><![CDATA[Gradient vanishing or exploding:Since the time step 1 would have effect on time step n, when doing back propagation, according to the chain rule, the same weight matrix W would be multiplied by n times which is W to the power of n. So the norm of W is greater than 1, there is gradient exploding. And vice versa, gradient vanishing if the norm of W is smaller than 1. Solution to gradient vanishing:Initialize weight matrix to identity matrix rather than a matrix of random values. And use Relu as activation function. The intuition of this is to take average of input and hidden state. Then start to optimize the weight matrix. Attention ModelThe input includes all or some of the hidden states in the encoder. Utilize a score function to weight importance of those hidden states and feed into the decoder with the input word.]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WordVectors]]></title>
    <url>%2F2018%2F07%2F24%2FWord2Vec%2F</url>
    <content type="text"><![CDATA[The gradient of the whole word matrix(context matrix) should be outer product rather than normal inner product because their dimensions are not matched. Normally the vectors are regarded as column vectors. Word2vecGlovePointwise Mutual Information (PMI)Normal count-based word-context matrix would provide a lot of useless information. i.e.: the entry (apple, the) is large but provide useless information. Instead we’d like context words that are particularly informative about the targetword. The best weighting or measure of association between words should tell ushow much more often than chance the two words co-occur. mutual information The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent:$$I(x,y)=\log_2\frac{P(x, y)}{p(x)p(y)}$$ The pointwise mutual information of a word w and a cotext word c is:$$PMI(w,c)=\log_2\frac{P(w, c)}{p(w)p(c)}$$ The ratio gives us an estimate ofhow much more the target and feature co-occur actually than just by chance. Positive PMI: use 0 to substitute negetive PMI. It is used commonly because negative PMI is unreliable. Negative PMI means that the two words co-occur less than expectation which could be due to the small corpora. A normal count-based co-occurrence matrix could be turned into PPMI matrix. Problem:biased toward infrequent words because the demoninator which is the frequency of the word is low. To solve this problem,change the computation for P(c). $$PPMI_\alpha(w,c)=\max(\log_2\frac{P(w, c)}{p(w)p_\alpha(c)},0)$$$$p_\alpha(c) = \frac{count(c)^\alpha}{\sum_ccount(c)^\alpha}$$ Usually \(\alpha\)=0.75 drawing on a similar weighting used for skipgrams.This increases the probability assigned to rarecontexts and solve this problem. Another solutio is Laplace smoothing. TF-IDFIDF is used to give a higher weight to rare words because they are likely to present more information. $$idf_i = \log(\frac{N}{df_i})$$ Combining term frequency with IDF results in a scheme known as tf-idf weighting of the value for word i in document j,\(w_{ij}\):\ $$w_{ij}=tf_{ij}idf_i$$ The tf-idf weighting is by far the dominant way of weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing including summarization. t-testSimilarity Measure for vectors(binary)Jaccard similarity:$$sim_{Jaccard}=\frac{\sum_{i=1}^N\min(v_i,w_i)}{\sum_{i=1}^N\max(v_i,w_i)}$$ $$J(A,B)=\frac{|A\cap B|}{|A\cup B|}$$ The min of \(v_i\) and \(w_i\)means if a feature exists in one vector but not the other vector, the result would be zero. Thedenominator can be viewed as a normalizing factor. Dice measure:$$sim_{Dice}=\frac{2 \times \sum_{i=1}^N\min(v_i,w_i)}{\sum_{i=1}^N(v_i+w_i)}$$ KL divergenceif two vectors, \(\vec{v}\) and \(\vec{w}\), each express a probabilitydistribution (their values sum to one), then they are are similar to the extent that these probability distributions are similar. Given two probability distribution or vectors P and Q: $$D(P||Q) = \sum_xP(x)\log \frac{P(x)}{Q(x)}$$ Unfortunately, the KL-divergence is undefined when Q(x) = 0 and P(x) \(\neq\) 0, which is a problem since these word-distribution vectors are generally quite sparse. Jensen-Shannon divergence solves this problem.$$JS(P||Q)=D(P|\frac{Q+P}{2})+D(Q|\frac{Q+P}{2})$$ Using syntax to define a word’s contextInstead of defining a word’s context by nearby words, we could instead define it bythe syntactic relations of these neighboring words. i.e.: the word duty could be represented by a bunch of combinations like additional,administrative(adj) or assert, assign(verb). Evaluating Vector Modelstest their performance on similarity, and in particular on computing the correlation between an algorithm’s word similarity scores and word similarity ratingsassigned by humans.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow]]></title>
    <url>%2F2018%2F07%2F23%2FTensorflow%2F</url>
    <content type="text"><![CDATA[Characteristic: Lazy evaluation like scala Two steps to build and run a model: assemble a graph use a session to execute operations in the graph Placeholder: a node to store variable which would not be modified during backpropagation Variable: a node that would be modified during backpropagation Constant: values that cannot be changed Running parts need to be encapsulated into session.run(). session.run() is evaluation of the graph. Variables could have name and scope. Use TF DType to accelerate the computation or tensorflow has to infer the type. create variables with tf.get_variable 1m = tf.get_variable(&quot;matrix&quot;, initializer=tf.constant([[0, 1], [2, 3]])) rather than 123Before running, variables need to be initialized. initializer = tf.global_variables_initializer()with tf.Session() as sess: sess.run(initializer)12Initialize only a subset of variables: with tf.Session() as sess: sess.run([a,b])12Initialize only a single variable: m=tf.get_variable(“matrix”,initializer=tf.constant([[0, 1], [2, 3]]))with tf.Session() as sess: sess.run(m.initializer)` W.assign(100) creates an assign operation that assign 100 to variable W. The operation needs to be executed in a session to take effect.]]></content>
      <tags>
        <tag>Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dependency_Parsing]]></title>
    <url>%2F2018%2F07%2F23%2FDependency-Parsing%2F</url>
    <content type="text"><![CDATA[Transition-Based Dependency ParsingLinear time, greedy algorithm, no backtracking. A configuration is a stack, a buffer of token lists and an oracle. The key is to train the oracle. The algorithm is intuitive. If there is a match in the candidate relation set, pop the words and add the relation to the result set. Push the words in the buffer into the stack. repeat until no word left. Creating an OracleSupervised learning is employed to do this. Generating Training Datagiven a reference parse and a configuration, the training oracle proceeds as follows: Choose LEFTARC if it produces a correct head-dependent relation Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent relationand (2) all of its dependents in the buffer have already been assigned. Otherwise, choose SHIFT. Featurescreate feature sets. LearningThe dominant approaches to training transition-based dependency parsers have been multinomial logistic regression and support vector machines, both of which can make effective use of large numbers of sparse features. Deep learning approaches have been applied successfully to transition-based parsing. These approaches eliminate the need for complex, hand-crafted features and have been particularly effective at overcoming the data sparsity issues normally associated training transition-basedparsers. Alternative Transition Systemsarc eager transition system is just a modified version of the oracle. The operation of the oracle: LEFTARC: Assert a head-dependent relation between the word at the front ofthe input buffer and the word at the top of the stack; pop the stack. RIGHTARC: Assert a head-dependent relation between the word on the top ofthe stack and the word at front of the input buffer; shift the word at the frontof the input buffer to the stack. SHIFT: Remove the word from the front of the input buffer and push it ontothe stack. REDUCE: Pop the stack. Beam SearchBFS with a heuristic filter that prunes the search frontier to stay within a fixed-size beam width. Graph-Based]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Back_Propagation]]></title>
    <url>%2F2018%2F07%2F21%2FBack-Propagation%2F</url>
    <content type="text"><![CDATA[Derivative of composite function$$\frac{d\frac{f(x)}{g(x)}}{dx} = \frac{f’(x)g(x)-f(x)g’(x)}{g^2(x)}$$ Derivative of Cross Entropy$$CE(\vec{y},\vec{\hat{y}}) = -\sum_iy_i\log(\hat{y_i})$$ Here \(y_i\) is an one-hot vector denotes the correct class \(c_i\). In the actual computation, it only selects out the correct class without any extra function. For a single example \(x_i\): if \(\hat{y_i}=y_i\): $$\frac{dCE(y_i,\hat{y_i})}{dx_i} = \frac{1}{\hat{y_i}}\frac{d(\hat{y_i})}{dx_i} = \frac{\sum_j^Ce^{(x_j)}}{e^{x_i}}\ \frac{e^{x_i}\sum_j^Ce^{x_j}-e^{2x_i}}{(\sum_j^Ce^{x_j})^2} = \hat{y_i}-1$$ if \(\hat{y_i}\ne y_i\): $$\frac{dCE(y_i,\hat{y_i})}{dx_i} = \frac{1}{\hat{y_i}}\frac{d(\hat{y_i})}{dx_i} = \frac{\sum_j^Ce^{(x_j)}}{e^{x_i}}\ \frac{e^{x_i}e^{x_k}}{(\sum_j^Ce^{x_j})^2}=\hat{y_i}$$ The rest is just compute the gradient layer by layer. Note that when compute the gradient with regard to activation function, there would be outer product.]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pagerank]]></title>
    <url>%2F2018%2F07%2F18%2FPagerank%2F</url>
    <content type="text"><![CDATA[A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages. PageRankIntuitionPages that are visited more are given higher weight. Employ graph to represent the internet. Higher in links means visited more. PageRank values would converge to the final value regardless of the start state and it is in the range [0,1]. So the PageRank values could also be viewed as the probability of the surfer is in the node i at any time given the PageRank value of the node i. PageRank could be regarded as a random walk in the internet. Start at any state, and then walk through the out links randomly. In case of circular routes and dead end, employ the scaled version of random walk. As the random walk proceeds, some nodes are visited more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. The idea behind PageRank is that pages visited more often in this walk are more important. Teleport:if N is the total number of nodes in the web graph, the teleport operation takes the surfer to each node with probability 1/N. Scaled version of random walk:with alpha probability take teleport operation and with 1-alpha probability take the out links of the current nodes. Interpreted in terms of the (scaled) version of PageRank, Perron’s Theorem tells us that there is a unique vector y that remains fixed under the application of the scaled update rule, and that repeated application of the update rule from any starting point will converge to y.This vector y thus corresponds to the limiting PageRank values we have been seeking. Markov chainA Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made. A Markov chain consists of N states. Each web page will correspond to a state in the Markov chain. A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1. Transition matrix P:The rows represent that given a node i, the probability of the node i transits to nodes j.(distribute the pagerank score)The columns represent that given a node j, the probability of nodes i transits to node j.(incoming pagerank score) ERGODIC MARKOV CHAIN:Definition: A Markov chain is said to be ergodic if there exists a positive integer T0 such that for all pairs of states i, j in the Markov chain, if it is started at time 0 in state i then for all t &gt; T0, the probability of being in state j at time t is greater than 0. For a Markov chain to be ergodic, two technical conditions are required of its states and the non-zero transition probabilities; these conditions are known as irreducibility and aperiodicity. irreducibility ensures that there is a sequence of transitions of non-zero probability from any state to any other. aperiodicity ensures that the states are not partitioned into sets such that all state transitions occur cyclically from one set to another. Scaled version of random walk which take teleport operation with alpha probability guarantees transition probability are greater than 0 and no loop transitions.(irreducibility and aperiodicity) Compute PageRank Iteratively compute \(\vec{x}P^t\) s.t. \(\vec{x}\) becomes unchanged. Compute the eigenvector coresponding to the largest eigenvalue 1 which is the PageRank. But compute the eigenvector could be computationally expensive. ImplementationMatrix-Multiplication version of PageRank:`pythonimport numpy as np class Pagerank(object): def init(self, N=0, alpha=0): self.alpha = alpha self.N = N self.pg_vector = None &apos;&apos;&apos;derive transition matrix from adjacency matrix&apos;&apos;&apos; def derive_transition(self,adjacency_matrix,alpha=0): if self.alpha != 0: alpha = self.alpha N = adjacency_matrix.shape[0] transition_m = np.zeros(adjacency_matrix.shape) for i in range(adjacency_matrix.shape[0]): if np.sum(adjacency_matrix[i,:]) == 0: transition_m[i,:] = np.array([1/N]*N) #this node is a dead end, transit to one of all nodes in the graph else: # with alpha probability transit to one of all nodes, 1-alpha take normal random walk transition_m[i, :] = alpha * np.array([1 / N] * N) + (1-alpha) * adjacency_matrix[i,:] / np.sum(adjacency_matrix[i,:]) return transition_m def propagate_transition(self, transition_m, pg_vector=None): #pg_vector is column vectors if pg_vector is not None: return np.dot(transition_m.T, pg_vector) else: if self.pg_vector is None: self.initialize_pg_vector() self.pg_vector = np.dot(transition_m.T, self.pg_vector) return self.pg_vector def initialize_pg_vector(self): assert self.N != 0, &quot;Please input number of Nodes N&quot; self.pg_vector = np.ones((self.N)).T self.pg_vector /= self.N print(self.pg_vector) Topic Specific PageRankStart at a random page in the topic and also end at a random page in the topic. \(\vec{x}_{sports}]\) is a topic specific pagerank vector where for pages belongs to sports, there is a topic specific pagerank and pagerank of other pages are 0. Calculate the PageRank of a page with regard to topics. The only difference is that the teleportation is different. The naive teleport go to a node in the graph with probability 1/N. The teleportation of topic specific pagerank is to go to a page in a specific topic with probability alpha. Given the number of pages S in the topic T, the probability is \(\frac{1}{S}\) rather than \(\frac{1}{N}\). i.e.:Let s be [A,B,C,D]. A belongs to topic 1. B,C and D belongs to topic 2.When calculating the topic specific pagerank, for topic 2, the formula is still \(\vec{x}P^t\). But here \(P\) for topic 2 is \(\alpha M + (1-\alpha)\frac{1}{S}\) rather than \(\alpha M + (1-\alpha)\frac{1}{N}\). Actually the formula for topic 2 is \(\alpha M + (1-\alpha)[0,\frac{1}{3},\frac{1}{3},\frac{1}{3}]\) and in normal PageRank it should be \(\alpha M + (1-\alpha)[\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]\). Personalized PageRankNow the difference is still the teleportation. The teleportation is tailored according to the users’ interest. Let’s assume the interest of a person is sports and finance. Their weights are 0.8,0.2 specifically. The teleportation is to go to a topic with the corresponding weight and then a page in the specific topic with probability alpha. This could also be viewed as a linear combination of topic specific vectors. In this example, the formula would be \(\alpha M + (1-\alpha) [0.8\vec{x_{sports}} + 0.2\vec{x_{finance}}]\). ReferenceNetworks, Crowds, and Markets: Reasoning About a Highly Connected World Introduction to Information Retrieval]]></content>
  </entry>
  <entry>
    <title><![CDATA[Parsing]]></title>
    <url>%2F2018%2F07%2F13%2FParsing%2F</url>
    <content type="text"><![CDATA[Context-Free Grammar(CFG) The symbols that correspond to words in the language (“the”, “nightclub”) are called terminal symbols. They are left nodes in a parse tree. The lexicon is the set of rules that introduce these terminal symbols. The symbols that express abstractions over these terminals are called non-terminals. Ther are the non-left Each grammar must have one designated start symbol, which is often called S &lt;\s&gt;. Formal Definition of CFGG=(T,N,S,R) T is a set of terminal symbols. N is a set of nonterminal symbols S is the start symbol (S \(\in\) N) R is a set of rules/productions of the form X -&gt; \(\gamma\). A grammar G generates a language L. Conversion to CNFChomsky Normal Form(CNF): for each non-terminal, only two non-ternimal or single terminal could be derived from it. Unit Production: Rules with a single non-terminal on the right. three situations needed to address in any generic grammar: rules that mix terminals with non-terminals on the right-hand side rules that have a single non-terminal on the right-hand side rules in which the length of the right-hand side is greater than 2 rules that mix terminals and non-terminals is to introduce a new dummy non-terminal to substitute the original terminal. For example, a rule for an infinitive verb phrase such as INF-VP → to VP would be replaced by thetwo rules INF-VP → TO VP and TO → to. To eliminate unit production:if A-&gt;&gt; B by a chain of one or more unit productions and B → \(\gamma\) is a non-unit production in our grammar, then we add A → γ for each such rule in the grammar and discard all the intervening unit productions. Rules with right-hand sides longer than 2 are normalized through the introductionof new non-terminals that substitute the longer sequences iteratively. i.e.: A → B C D. After transformation, A → X D and X -&gt;B C The entire conversion process can be summarized as follows: Copy all conforming rules to the new grammar unchanged. Convert terminals within rules to dummy non-terminals. Convert unit-productions. Make all rules binary and add them to new grammar. Learn PCFG RulesCount-based For unambiguous parsing, just count each rule showed in the resulting parsing tree and get the probability like language model. For ambiguous parsing, initialize the parsing s.t. each rule is assigned the equal probability. And then parse the sentece an calculate probability of each parsing. Update the weight. Iteratively doing this, an extension of EM algorithm. Problems of PCFG Lack of Sensitivity to Lexical Dependencies Independence Assumptions Miss Structural Dependencies Between Rules. For different location, each rule may be expanded differently. Improving PCFGs by Splitting Non-Terminals split the NP non-terminal into two versions: one for subjects, one for objects. parent annotation This method also induce new problem. More rules lead to less training which may cause overfitting. Modern models automatically search for the optimal splits. The split and merge algorithm for example, starts with a simple X-bar grammar, alternately splits the non-terminals, and merges non-terminals, finding the set of annotated nodes that maximizes the likelihood of the training set treebank.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CKY]]></title>
    <url>%2F2018%2F07%2F13%2FCKY%2F</url>
    <content type="text"><![CDATA[CKY is to initialize a matrix and fill in the upper right triangular matrix. Each word is filled into the matrix from left to right and bottom to up. There is a probability distribution about contituent for each word. And then the matrix is filled up using viterbi algorithm. Take the max probability. Transfer the rules to binary reduce the complexity.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Language_Model]]></title>
    <url>%2F2018%2F07%2F09%2FLanguage-Model%2F</url>
    <content type="text"><![CDATA[Unigram$$p(w_i) = \frac{c(w_i)}{N}$$ \(c(w_i)\) is the count of the word. N is the total number of word. Bigram$$p(w_i|w_{i-1}) = \frac{c(w_iw_{i-1})}{c(w_{i-1})}$$ N-GramGiven the previous N words, estimate the probability of the current word. $$p(w_i|w_{i-N-1}^{N-1}) = \frac{c(w_iw_{i-N-1}^{N-1})}{c(w_{i-N-1}^{N-1})}$$ Deal with Unknown WordsThere are two common ways to train the probabilities of the unknown word model . The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: Choose a vocabulary (word list) that is fixed in advance. Convert in the training set any word that is not in this set to the unknown word token in a text normalization step. Estimate the probabilities for from its counts just like any other regular word in the training set. The second alternative, replacing words in the training data by based on their frequency.i.e.: replace all words by that occur fewer than n times in the training set or choose the top V words by frequency and replace the rest by UNK. The third way:$$p(w_i)=\lambda_1p_{ML}(w_i)+(1-\lambda_1)\frac{1}{N}$$ Save some probability for unknown words \((\lambda_{unk} = 1-\lambda_1)\). This is in fact another kind of smoothing. Discount the probability of known words and assign to the unknown words. \(\lambda\) is a hyperparameter. It is 0.95 by default. N is the total number of words. CoverageThe percentage of known words in the corpus. Usually omit the symbol of the end of a sentence . Entropy of a sentence$$H(S)= -\frac{1}{N}\sum_{w\in S}\log_2 P(w)$$]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word_Senses]]></title>
    <url>%2F2018%2F07%2F06%2FWord-Senses%2F</url>
    <content type="text"><![CDATA[Word sense disambiguation(WSD)Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the performance of tasks such as question answering and machine translation. Two kinds of WSD tasks: lexical sample all-words Lexical sample: given a set of target words and with an inventory of senses from some lexicon. Supervised learning is employed to solve this problem at usual. Hand-label a small set of words and corresponding sense, and feed into a classifier. All-words: given entire texts and a lexicon that contains an inventory of senses for each entry. The objective is to disambiguate every content word in the text. It is impractical to train a classifier for each term because the number of words is large. Supervised Learning Approach Feature Vector Extraction. Process the sentence in a context window to extract feature vector. There are two kinds of feature vector: collocation features. Multiple words in a position-specific relationship to a target word. It is effective at encoding local lexical and grammatical information that usually isolate a given sense. High performing systems generally use POS tags and word collocations of length 1,2 and 3 from a window of words 3. bag-of-words: neglecting the position information, just record number of context words in a context window. The context words could be pre-selected(frequent used words) given a target word. Evaluation: most frequent sense as baseline. Most freauent sense usually choose the most frequently used sense given a context. Dictionary and ThesaurusThe Lesk AlgorithmIt is a family of dictionary-based algorithms.Simplified algorithm compute the number of overlapped words between a given context and all of the word senses. Output the sense with the most overlapped words. Original Lesk Algorithm compute the number of overlapped words between sense of each context word and the sense of the target word.i.e.: Bank is a financial institute for deposit.Simplified algorithm compute the number of overlapping words between each sense and the context [financial,institute, deposit] and output the sense with the most overlapped words.The original algorithm compute all senses with all senses of each word in the context [financial,institute, deposit] and output the sense with the most overlapped words. Notes:stop-words are not taken into consideration. Problem: Entries of the dictionary for the target word are not enough which reduce the chance of overlapping. Solution: To expand the sense, include senses of similar words which may increase the number of overlapping words. The best solution is to utilize sense-tagged corpus, which is so called Corpus Lesk Algorithm. To expand the signature for a word sense, add all word senses with the same label. i.e.: add word sense of large, huge to big. Instead of employ a stop-list, the algorithm apply IDF to reduce the weight of function words like the,of. This algorithm is usually a baseline. The bag-of-words approach could be improved by combining the Lesk algorithm. The glosses and example sentences for the target sense in WordNet would be used as words features along with sense-tagged corpus like SemCor. Graph-based MethodsA graph is employed to represent words. Each sense is a node and relations between senses are edges. It is usually an undirected graph. The correct sense is the one that is central in the graph. Use degree to decide centrality. Another approach is to assign probability to nodes according to personalized page rank. Problem of both supervised and dictionary-based approach: need hand-built resouces Semi-Supervised: Bootstrapping Use a small set of labeled data to train the classifier. Do classification. Select the result with high confidence and add to the training set. Train the classifier and repeat step 2 and 3. Stop until there is no untagged data left or reach a specific error rate threshold. To construct the training set, hand-label it or use heuristic to help. Heuristic: one sense per collocation: certain words or phrases strongly indicates a specific sense. one sense per discourse:a particular word appearing multiple times in a text or discourse often appeared with the same sense. This heuristic seems to hold better for coarse-grained senses and particularly for cases of homonymy rather than polysemy. Unsupervised Approach:Word Sense Induction(WSI)It is actually a clustering of word senses. For each token(sense) of a word in the corpus, use context word vector \(\vec{c}\) to represent it. Do clustering s.t. there is N clusters and each token \(\vec{c}\) is assigned to a cluster. Each cluster defines a sense. Recompute the center s.t. the center vector \(\vec{s_j}\) represent a sense. To do word sense disambiguation: compute context word vector \(\vec{c}\) to represent the sense of the word. Do clustering and assign the context vector to a cluster. All we need is a clustering algorithm and a distance metric between vectors. A frequently used technique in language applications is known as agglomerativeclustering. Recent algorithms have also used topic modeling algorithms like Latent Dirichlet Allocation (LDA), another way to learn clusters of words based on their distributions. It is fair to say that no evaluation metric for this task has yet become standard. Word Similarity(Thesaurus Methods)Word similarity refers to meaning(synonyms).Word relatedness characterizes relationships(antonyms,synonyms). Path-length based ApproachThesaurus could be viewed as a graph. The simplest thesaurus-based algorithms use the edge between words to measure similarity. pathlen(c1, c2) = 1 + edges in the shortest path between the sense nodes c1 and c2. path-length based similarity:$$sim_{path}(c_1,c_2)=\frac{1}{pathlen(c_1, c_2)}$$ Path lengths are not the same in different level of the hierarchy.It is possible to refine path-based algorithms with normalizations based on depth in the hierarchy. In general an approach that independently represent the distance associated with each edge it is preferred. For data without sense tag(words are not presented according to their senses), measure similarity according to senses of words. If there is a pair of senses is similar, then two words are similar. $$word_{sim}(w_1,w_2)=\max_{c_1\in senses(w_1)\ c_2\in senses(w_2)} sim(c_1, c_2)$$ information-content word-similarity algorithmsstill rely on the structure of the thesaurus but also add probabilistic information derived from a corpus. The corpus is represented as a set of concepts. And it is represented as a tree structure. The leaf nodes are basic concepts and parent nodes contain the children which is a superset of concepts. Each node is assigned a probability. So p(root) = 1. $$P(c) = \frac{\sum_{w\in words(c)}count(w)}{N}$$ c is a specific concept. The probability of a concept is number of words in the concept divided by total number of words. information content: information content (IC) of a concept c $$IC(c) = −\log P(c) $$ the lowest common subsumer(LCS) of two concepts:LCS(c1, c2) = the lowest common subsumer,the lowest node in the hierarchy that subsumes both c1 and c2.(The lowest parent node of c1 and c2) Resnik similarity: similarity between two words is related to their common information; the more two words have in common, the more similar they are. Resnik proposes to estimate the common amount of information by the information content of the lowest common subsumer of the two nodes. $$sim_{Resnik}(c_1,c_2)=-\log P(LCS(c_1,c_2))$$ Lin similarity Similarity Theorem: The similarity between A and B is measured by the ratiobetween the amount of information needed to state the commonality of A andB and the information needed to fully describe what A and B are.$$sim_{Lin}(A,B)=\frac{common(A,B)}{description(A,B)}$$ the information in common between two concepts is twice the information in the lowest common subsumer LCS(c1, c2). Adding in the above definitions of the information content of thesaurus concepts$$sim_{Lin}(C_1,C_2)=\frac{2\times \log P(LCS(c_1,c_2))}{\log P(c_1)+\log P(c_2)}$$ Jiang-Conrath distance express distance rather than similarity, work as well as or better than all the other thesaurus-based methods: $$dist_{JC}(c_1, c_2) = 2\times \log P(LCS(c_1, c_2))−(\log P(c_1) +\log P(c_2)) $$ It could be transferred to similarity by take reciprocal.$$sim_{JC}=\frac{1}{dist_{JC}}$$ dictionary-based method: This method makes use of glosses, which are, in general, a property of dictionaries rather than thesauruses. Two concepts/senses are similar if their glosses contain overlapping words. For each n-word phrase that occurs in both glosses, Extended Lesk adds in ascore of \(n^2\).(the relation is non-linear because of the Zipfian relationship between lengths of phrases and their corpus frequencies; longer overlaps are rare, so they should be weighted more heavily) Given such an overlap function, when comparing two concepts (synsets), ExtendedLesk not only looks for overlap between their glosses but also between theglosses of the senses that are hypernyms, hyponyms, meronyms, and other relationsof the two concepts. If we just considered hyponyms and definedgloss(hypo(A)) as the concatenation of all the glosses of all the hyponym senses ofA, the total relatedness between two concepts A and B might be $$similarity(A,B) = overlap(gloss(A), gloss(B))+overlap(gloss(hypo(A)), gloss(hypo(B)))+overlap(gloss(A), gloss(hypo(B)))+overlap(gloss(hypo(A)),gloss(B))$$ Extended Lesk overlap measure: $$sim_{eLESK}(c_1,c_2) =\sum_{r,q\in RELS}overlap(gloss(r(c_1)),gloss(q(c_2)))$$ RELS is the set of possible relations whose glosses we compare. Possible relations are like hypernyms, hyponyms, meronyms, and other relations of the two concepts. Evaluating Thesaurus-Based SimilarityThe most common intrinsic evaluation metric computes the correlation coefficient between an algorithm’s word similarity scores and word similarity ratings assigned by humans. Zipf’s law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.The most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word. ReferenceZipf’s law]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hierarchical_Softmax]]></title>
    <url>%2F2018%2F07%2F04%2FHierarchical-Softmax%2F</url>
    <content type="text"><![CDATA[Hierarchical softmaxTrun the normal softmax to a tree structure to avoid computing every context word with the center word. Normal softmax requires normalization which is extremely expensive. In (Goodman, 2001b) it is shown how to speed-up a maximum entropy class-based statistical language model by using the following idea. Instead of computing directly P(Y |X) (which involves normalization across all the valuesthat Y can take), one defines a clustering partition for the Y (into the word classes C, such that there is a deterministicfunction c(.) mapping Y to C), so as to write $$P(Y = y|X = x) =P(Y |C = c(Y ), X)P(C =c(Y )|X)$$ Prove:$$P(Y |X) = \sum_i P(Y, C = i|X) = \sum_i P(Y |C =i, X)P(C = i|X) = P(Y |C = c(Y ), X)P(C =c(Y )|X)$$ C(Y) is a function to classify Y into C classes. \(P(Y |C = c(Y ), X)P(C =c(Y )|X)\) is equivalent to \(\sum_i P(Y |C =i, X)P(C = i|X)\) if every Y is classified into a class and the probablities of assign a Y to a cluster are multiplied together. To utilize the architecture of binary trees, do hierarchical clustering such that each time two clusters are mergerd. Initially each word is a cluster. As a result, each level consists of a number of cluster. From the root to the leaf, multiply the probabilities together and we get the probability. Each word v must be represented by a bit vector \((b_1(v), . . . b_m(v))\) (where m depends on v) $$P(v|w_{t−1}, . . . , w_{t−n+1}) =\prod_{j=1}^mP(b_j(v)|b_1(v), . . . , b_{j−1}(v), w_{t−1}, . . . , w_{t−n+1})$$ \(b_j(v)\) is the intermediate vector in the tree. The intermediate node is in fact a cluster. Each leaf node is a word. Each intermediate note could be viewed as a group of similar-meaning words. Each node is associated with a feature vector. $$P(b = 1|node, w_{t−1}, . . . , w_{t−n+1}) =sigmoid(\alpha_{node} + \beta’\cdot tanh(c + Wx + UN_{node}))$$]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql]]></title>
    <url>%2F2018%2F07%2F03%2Fsql%2F</url>
    <content type="text"><![CDATA[RDBMS stands for Relational Database Management System. Every table is broken up into smaller entities called fields which is a column in a table designed to maintain specific information about every record in the table. A record, also called a row, is each individual entry that exists in a table. A record is a horizontal entity in a table. A column is a vertical entity in a table that contains all information associated with a specific field in a table. A database most often contains one or more tables. Each table is identified by a name. Tables contain records (rows) with data. Semicolon is the standard way to separate each SQL statement in database systems that allow more than one SQL statement to be executed in the same call to the server. Keywords COUNT SELECT COUNT(Country) FROM Customers; Return the number of country from table Customers Distinct SELECT Country FROM Customers; Selects all values from the “Country” column in the “Customers” table. WHERE The WHERE clause is used to extract only those records that fulfill a specified condition. SELECT * FROM Customers WHERE Country=’Mexico’; Selects all the customers from the country “Mexico”, in the “Customers” table. AND operator displays a record if all the conditions separated by AND is TRUE.The OR operator displays a record if any of the conditions separated by OR is TRUE.The NOT operator displays a record if the condition(s) is NOT TRUE. ORDER BY INSERT INTO IS NULL and IS NOT NULL to test null values SELECT TOP = Mysql SELECT column_name(s) FROM table_name WHERE condition LIMIT number; LIKE The LIKE operator is used in a WHERE clause to search for a specified pattern in a column. %:The percent sign represents zero, one, or multiple characters _:The underscore represents a single character IN: = multiple OR conditions. BETWEEN: selects values within a given range. The values can be numbers, text, or dates.It is inclusive: begin and end values are included. SQL aliases are used to give a table, or a column a temporary name. An alias only exists for the duration of the query. JOIN: SELECT column_name(s) FROM table1 INNER JOIN table2 ON table1.column_name = table2.column_name; UNION: combine the result-set of two or more SELECT statements. GROUP BY: used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. The HAVING clause was added to SQL because the WHERE keyword could not be used with aggregate functions. The EXISTS operator is used to test for the existence of any record in a subquery. The ANY operator returns true if any of the subquery values meet the condition. The ALL operator returns true if all of the subquery values meet the condition. The SELECT INTO statement copies data from one table into a new table. The INSERT INTO SELECT statement copies data from one table and inserts it into another table. Notes SQL keywords are NOT case sensitive: select is the same as SELECT SQL requires single quotes around text values. Numeric fields should not be enclosed in quotes: (INNER) JOIN: Returns records that have matching values in both tables LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table FULL (OUTER) JOIN: Return all records when there is a match in either left or right table Each SELECT statement within UNION must have the same number of columns.The columns must also have similar data types.The columns in each SELECT statement must also be in the same order]]></content>
      <tags>
        <tag>Database sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semantics_with_Dense_Vectors]]></title>
    <url>%2F2018%2F07%2F03%2FSemantics-with-Dense-Vectors%2F</url>
    <content type="text"><![CDATA[Skip-gram ModelGiven a center word, predict its neighbor word(context word). There are two matrices, one is context matrix and the other is word matrix. The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document.The objective of the Skip-gram model is to maximize the average log probability \(\frac{1}{T}\sum_{t=1}^T\sum_{−c≤j≤c,j\neq 0}\log p(w_{t+j}|w_t) \) The basic Skip-gram formulation defines \(p(w_{t+j} |w_t)\) using the softmax function:$$p(w_{O}|w_I) = \frac{exp({v’_{w_O}}^Tv_{w_I})}{\sum_{w=1}^Wexp({v’w}^Tv{w_I})}$$ Negative SamplingNoise Contrastive Estimation (NCE). NCE posits that a good model should be able to differentiate data from noise by means of logistic regression. While NCE can be shown to approximately maximize the log probability of the softmax, the Skipgram model is only concerned with learning high-quality vector representations, so we are free tosimplify NCE.Here is Negative Sampling, its objective funtion: $$\log\sigma({v’_{w_O}}^Tv_{w_I})+\sum_{i=1}^kE_{w_i\sim P_n(w)}[\log\sigma({-v’_{w_i}}^Tv_{w_I})]$$ The above objective function is used to replace every \(\log P(w_O|w_I )\) term in the Skip-gram objective. Thus the task is todistinguish the target word \(w_O\) from draws from the noise distribution \(P_n(w)\) using logistic regression, where there are k negative samples for each data sample. k in the range 5–20 are useful for small training datasets, while for large datasets the k 2–5. The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling uses only samples. And NCE approximately maximizes the log probability of the softmax, this property is not important for our application. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.Raise to the power of 3/4 is an emprical result. $$P(w_i) = \frac{ {f(w_i)}^{3/4} }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right) }$$ To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word \(w_i\)in the training set is discarded with probability computed by the formula $$p(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$$ where \(f(w_i)\) is the frequency of word \(w_i\) and t is a chosen threshold, typically around \(10^{−5}\). Another implementation:$$p(w_i)=(\sqrt{\frac{z(w_i)}{0.001}}+1)\cdot \frac{0.001}{z(w_i)}$$ 0.001 is the default sample value. Smaller values of ‘sample’ mean words are less likely to be kept. \(z(w_i)\) is the frequency of the word i. P(wi)=1.0 (100% chance of being kept) when z(wi)&lt;=0.0026. This means that only words which represent more than 0.26% of the total words will be subsampled. P(wi)=0.5 (50% chance of being kept) when z(wi)=0.00746. projection layer input is one-hot vector, multiplied by embedding layer, the output is the projection layer. The units in the projection layer are word vectors. Hyperparametersa context window size L CBOWcontinuous bag of words Reversed version of skip-grams, predicting the current word \(w_j\) from the context window of 2L words around it $$p(w_{c,j}|w_j) = \frac{exp(c_k\cdot v_j)}{\sum_{i\in|V|}exp(c_i\cdot v_j)}$$ \(c_k\)means the \(k_{th}\) context word. \(v_j\)means the word vector of given word \(w_j\). The computing of demoninator \(\sum_{i\in|V|}exp(c_i\cdot v_j)\) is extremely expensive because it requires computing for each given word \(w_j\). Relationship between Word VectorsWord vector matrix W and context word vector matrix C multiply together would produce a |V|*|V| matrix X.Skip-gram’s optimal value occurs whenthis learned matrix X is actually a version of the PMI matrix, with the values shiftedby logk (where k is the number of negative samples):$$WC = X^{PMI} −\log k$$ In other words, skip-gram is implicitly factorizing a (shifted version of the) PMImatrix into the two embedding matrices W and C, just as SVD did, albeit with a different kind of factorization. Once the embeddings are learned, for each word \(w_i\), there is a word vector \(v_i\) and a context word vector \(c_i\). We can choose to throw away the C matrix and just keep W, as we did with SVD.Alternatively we can add the two embeddings together, using the summed embedding\(v_i + c_i\) as the new d-dimensional embedding, or we can concatenate theminto an embedding of dimensionality 2d. As with the simple count-based methods like PPMI, the context window sizeL effects the performance of skip-gram embeddings, and experiments often tunethe parameter L on a dev set. As with PPMI, window sizing leads to qualitativedifferences: smaller windows capture more syntactic information, larger ones moresemantic and relational information. One difference from the count-based methodsis that for skip-grams, the larger the window size the more computation the algorithm requires for training. Properties of embeddingsOffsets between vector embeddingscan capture some relations between words. Brown ClusteringBrown clustering is an agglomerative clustering algorithm for deriving vector representations of words by clustering words based on their associations with the preceding or following words. Each word is initially assigned to its own cluster. merge each pair of clusters. The pair whose merger results in the smallest decrease in the likelihood of the corpus (according to the class-based language model) is merged. Clustering proceeds until all words are in one big cluster Each cluster contains words that are contextually similar(similar meaning/sense).After clustering, a word can be represented by the binary string that corresponds to its path from the root node.Each prefix represents a cluster that the word belongs to.These prefixes can then be used as a vector representation for the word; the shorter the prefix, the more abstract the cluster. (i.e.:the string 0001 represents the names of common nouns for corporate executives {chairman, president}, 1 is verbs {run, sprint, walk}, and 0 is nouns.) The length of the vector representation can thus be adjusted to fit the needs of the particular task. a 4-6 bit prefix to capture part of speech information and a full bit string to represent words. The first 8 or9-bits of a Brown clustering perform well at grammar induction. Because they arebased on immediately neighboring words, Brown clusters are most commonly usedfor representing the syntactic properties of words, and hence are commonly used asa feature in parsers. Nonetheless, the clusters do represent some semantic properties as well. Note that the naive version of the Brown clustering algorithm described above isextremely inefficient — \(O(n^5)\): at each of n iterations, the algorithm considers each of \(O(n^2)\) merges, and for each merge, compute the value of the clustering by summing over \(O(n^2)\)terms. because it has to consider every possible pair of merges. In practicewe use more efficient \(O(n^3)\) algorithms that use tables to pre-compute the values for each merge. hard clustering algorithm:each word has only one cluster class-based language modela model in which each word \(w\in V\) belongs to a class \(c\in C\) with a probability P(w|c).Class based LMs assigns a probability to a pair of words \(w_{i−1}\) and \(w_i\) by modeling the transition between classes rather than between words:$$P(w_i|w_{i−1}) = P(w_i|c_i)P(c_i|c_{i−1})$$ Given the class label of word \(c_{i-1})\), compute the transition probability and then given the new transition, the probability of current word \(w_i\). The class-based LM can be used to assign a probability to an entire corpus givena particularly clustering C as follows: $$P(corpus|C) = \prod_{i−1}^nP(c_i|c_{i−1})P(w_i|c_i)$$ Class-based language models are generally not used as a language model for applications like machine translation or speech recognition because they don’t workas well as standard n-grams or neural language models. LSA(latent semantic analysis):applying SVD to the term-document matrix LSA is a particular application of SVD to a |V| × c term-document matrix X representing |V| words and their co-occurrence with c documents or contexts.SVD factorizes any such rectangular |V| × c matrix X into the product of three matricesW, \(\sum\), and \(C^T\). In the matrix W, each of the w rows still represents a word, but each column represents one of m dimensions in a latent space, such that the m column vectors are orthogonal to each other and the columns are ordered by the amount of variance in the original dataset. The number of such dimensions m is the rank of X (the rank of a matrix is the numberof linearly independent rows). \(\sum\) is a diagonal matrix, with singular valuesalong the diagonal, expressing the importance of each dimension. The matrix\(C^T\) still represents documents or contexts, but each row now represents one of the new latent dimensions and the m row vectors are orthogonal to each other.By using only the first k dimensions instead of all m dimensions, the product of these 3 matrices becomes a least-squares approximation to the original X. Since the first dimensions encode the most variance, one way to view the reconstruction is as modeling the most important information in the original dataset. Using only the top k dimensions leads to a reduced |V| ×k matrix \(W_k\), with one k-dimensioned row per word. This row now acts as a dense k-dimensional vector (embedding) representing that word, substituting for the very high-dimensional rows of the original X. LSA implementations generally use a particular weighting of each co-occurrence cell that multiplies two weights called the local and global weights for each cell (i, j)—term i in document j. The local weight of each term i is its log frequency: log f(i, j) +1. The global weight of term i is a version of its entropy: $$1+\frac{\sum_jp(i,j)\log p(i,j)}{\log D}$$D is the number of documents. Advantages: Solve the problem of one word multiple senses or one sense multiple words by reduce the demensionality and extract the key information. Information extraction is to project the original co-occurence data to a new axes which could be viewed as a semantic space. Disadvantages: SVD requires lots of time to compute. ReferenceDistributed Representations of Words and Phrasesand their CompositionalityWord2Vec Tutorial Part 2 - Negative Sampling]]></content>
      <tags>
        <tag>NLP SVD Skip-gram CBOW Brown Clustering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural_Networks_and_Neural_Language_Model]]></title>
    <url>%2F2018%2F07%2F01%2FNeural-Networks-and-Neural-Language-Model%2F</url>
    <content type="text"><![CDATA[A neural network with one hidden layer can be shown to learn any function. Stochastic gradient descentis to choose a training example or a batch of examples to compute the gradients rather than use the entire training set. XOR Problemnot linearly separablea hidden layer rotates the axis or converts the original data to another plane. Cross Entropy Loss$$L(\hat{y}, y) = \log p(\hat{y}_i) $$If \(\hat{y}\) != y, it doesn’t contribute to the loss. Only the true class would contribute the loss. If \(p(\hat{y})\) =1, \(\log p(\hat{y}_i)=0\). If \(p(\hat{y})\) =0, \(\log p(\hat{y}_i)=\infty\).\(\hat{y}\) means output of the network. Embedding LayerCould be understood as a weight matrix. Because the one-hot vector is multiplied by the embedding layer to get the word vector, so the embedding layer is actually also a weight matrix. Via back propagation, it could also be trained simultaneously with other weights in the neural network.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logistic_Regression]]></title>
    <url>%2F2018%2F06%2F29%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[multinomial logistic regression, sometimes referred to within language processing as MaxEnt entropy modeling, MaxEnt for short. Logistic regression belongs to the family of classifiers known as the exponential or log-linear classifiers.Technically, logistic regression refers to a classifier that classifiesan observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes First, wrap the exp function around the weight-feature dot-product \(w\cdot f\) , which will make the values positive. A discriminative model discriminativemodel takes this direct approach, computing P(y|x) by discriminating among the different possible values of the class y rather than first computing a likelihood. Logistic Regression $$p(c|x) = \frac{\exp(\sum_iw_if_i(c,x))}{\sum_{c’ \in C}\exp(\sum_{i=1}^N w_if_i(c’,x)}$$ exp is to make to result greater than 0.\(f_i(c, x)\), meaning feature i for a particular class c for a given observation x. In NLP, x is usually a document. Feature i usually means whether a specific word \(x_i\) has shown in the document for a particular class. \(w_i\) is the weight corresponding to the feature i. For example, in sentiment analysis, classes are 0 and 1 corresponding to positive and negative. The word best is a feature, namely \(\ f_1\), its value for class 1 coube be +10 and -10 for class 0. Computing the actual probability rather than just choosing the best class is useful when the classifier is embedded in a larger system. Otherwise compute the nominater is sufficient to do classification. \(\hat{c}={argmax}{c\in C}\sum{i=1}^N w_if_i(c,x)\) Learning weightsThe intuition is to choose weights that make the classes of the training examples more likely. Indeed, logistic regression is trained with conditional maximum likelihood estimation. This means we choose the parameters w that maximize the (log) probability of the y labels in the training data given the observations x. $$\hat{w} = {argmax}_w\sum_jlogP(y^{(j)}|x^{(j)})$$ It means given a training set, we choose weights that maximize the probability of \(j^{th}\) training example \(x^{(j)}\) is classified to the lable \(y^{(j)}\). The objective function L that we are maximizing is thus$$L(w) = \frac{\exp(\sum_i^Nw_if_i(y^{(j)},x^{(j)}))}{\sum_{y’ \in Y}\exp(\sum_{i=1}^N w_if_i(y’^{(j)},x^{(j)})}$$]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine_Learning]]></title>
    <url>%2F2018%2F06%2F29%2FMachine-Learning%2F</url>
    <content type="text"><![CDATA[contingency table consists of 4 entries: True Positive, True Negative, False Positive, False Negative. Accuracy doesn’t work well whenthe classes are unbalanced. Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$ Recall measures the percentage of items actually present in the input that werecorrectly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$ precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.In practice, we generally combine precision and recall into a single metric calledF-measure the F-measure that is defined as:$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$ The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall. Harmonic mean is used because it is a conservative metric; the harmonic mean oftwo values is closer to the minimum of the two values than the arithmetic mean is.Thus it weighs the lower of the two numbers more heavily. In any-of or multi-label classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d. one-of or multinomial classification, multinomial classification in which the classes are mutually exclusive and each document or item appears inexactly one class. Build a separate binary classifier trained on positiveexamples from c and negative examples from all other classes. Now given a testdocument or item d, we run all the classifiers and choose the label from the classifier with the highest score. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. The only problem with cross-validation is that because all the data is used fortesting, we need the whole corpus to be blind; we can’t examine any of the datato suggest possible features and in general see what’s going on. But looking at thecorpus is often important for designing the system. For this reason, it is commonto create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set. regularization: used to penalize large weightsL1 regularization is called ‘the lasso’ or lasso regression and L2 regression is called ridge regression, and both are commonly used in language processing. L2 regularization is easier to optimize because of its simple derivative, while L1 regularization is more complex (the derivative of |w| is noncontinuousat zero). But where L2 prefers weight vectors with many small weights, L1 prefers sparse solutions with some larger weights but many more weights set to zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features. L2 regularization:$$\sum_{j=1}^Nw_j^2$$ L1 regularization:$$\sum_{j=1}^N|w_j|$$ Both L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights aredistributed according to a gaussian distribution with mean µ = 0. The regularization technique is useful for avoiding overfitting by removing or downweighting features that are unlikely to generalize well. Many kinds of classifiers including naive Bayes do not have regularization, and so instead feature selection is used to choose the important features and remove the rest. The basis of feature selection is to assign some metricof goodness to each feature, rank the features, and keep the best ones. The number of features to keep is a meta-parameter that can be optimized on a dev set. Feature SelectionFeatures are generally ranked by how informative they are about the classification decision. A very common metric is information gain. Information gain tells us how many bits of information the presence of the word gives us for guessing the class, and can be computed as follows (where ci is the ith class and ¯w means that a document does not contain the word \(w^-\)): While feature selection is important for unregularized classifiers, it is sometimesalso used in regularized classifiers in applications where speed is critical, since it is often possible to get equivalent performance with orders of magnitude fewer features. The overly strong conditional independence assumptions of Naive Bayes mean that if two features are correlated naive Bayes will multiply them, overestimating the evidence. Logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign half the weight to w1 and half to w2. naive Bayes works extremely well (even better than logistic regression or SVMs) on small datasets or short documents.Furthermore, naive Bayes is easy to implement and very fast to train. Nonetheless, algorithms like logistic regression and SVMs generally work better on larger documentsor datasets. bias-variance tradeoffThe bias of a classifier indicates how accurate it is at modeling different training sets.The variance of a classifier indicates how much its decisions are affected by small changesin training sets. Models with low bias (like SVMs with polynomial or RBF kernels) are very accurate at modeling the training data. Models with low variance (like naiveBayes) are likely to come to the same classification decision even from slightly different training data. low-bias models tend to overfit, and do not generalize well to very different test sets.low-variance models tend to generalize so well that they may not have sufficient accuracy.Thus model trades off bias and variance. Adding more features decreases bias by making it possible to more accurately model the training data, butincreases variance because of overfitting. Regularization and feature selection areways to improve (lower) the variance of classifier by downweighting or removingfeatures that are likely to overfit. In addition to the choice of a classifier, the key to successful classification is thedesign of appropriate features. Features are generally designed by examining thetraining set with an eye to linguistic intuitions and the linguistic literature on the domain. For some tasks it is especially helpful to build complex features that are combinationsof more primitive features. For logistic regression and naive Bayes thesecombination features or feature interactions have to be designed by hand. feature interactionsSome other machine learning models can automatically model the interactionsbetween features. For tasks where these combinations of features are important(especially when combination of categorical features and real-valued features mightbe helpful), the most useful classifiers may be such classifiers,including SupportSVMs Vector Machines (SVMs) with polynomial or RBF kernels, and random forests dimensionality reduction(PCA,SVD)First rotate the axes of the original dataset into a new space. The new space is chosen so that the highest order dimensioncaptures the most variance in the original dataset, the next dimension captures the next most variance, and so on. In this new space, the data if represented with a smaller number of dimensions and stillcapture much of the variation in the original data. For binary classification, sigmoid would suffice the requirement. Softmax(Logistic Regression)Linear decision boundary. Bias TermBias is significant because it allows the activation function to shift left and right. The weight is to change the steepness of the activation function. Role of Bias in Neural Networks Max-margin Objective function]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentiment_Classification_Naive_Bayes]]></title>
    <url>%2F2018%2F06%2F29%2FSentiment-Classification%2F</url>
    <content type="text"><![CDATA[In most text classificationapplications, however, using a stop word list doesn’t improve performance,and so it is more common to make use of the entire vocabulary and not use a stopword list Naive Bayes Classifier for Sentiment Analysis$$C_{NB}=\underset{c\in C}{argmax}\ logP(c)+\sum_{i \in positions}logP(w_i|c)$$ Each word \(w_i\) could be viewed as a feature for a specific document. Maximum Likelihood to train Naive Bayes$$\hat{p}(c)=\frac{N_c}{N_{doc}}$$ The standard solution for an unknown word that doesn’t show up in the training set is to ignore such words—remove them from the test document and not include any probability for them at all. Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization to deal with zero count. $$\hat{P}(w_i|c) = \frac{count(w_i, c) +1}{(\sum_{w\in V}(count(w, c)) +|V|}$$ Note that the vocabulary V consists of the union of all the word types in all classes, not just the words in one class. If the denominator is the word in the class \(c_i\), then given a class, the probability of the sentence shows up is 1. That is meaningless. Binary Multinominal Naive BayesWhether a word occurs or not seems to matter more than its frequency. First,it remove all duplicate words such that each sentence contains only one distinct word before concatenating them into the single big document. Second deal with negation during text normalization. Typically adding negation prefix such as NOT_ to every word. i.e., like -&gt; Not_like. In some situations we might have insufficient labeled training data. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are pre-annotated with positive or negative sentiment. Naive Bayes as a Language ModelIf we use only individual word features, and we use all of the words in the text(not a subset), then naive Bayes has an important similarity to language modeling.Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. contingency table consists of 4 entries: True Positive, True Negative, False Positive, False Negative. Accuracy doesn’t work well whenthe classes are unbalanced. Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$ Recall measures the percentage of items actually present in the input that werecorrectly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$ precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.In practice, we generally combine precision and recall into a single metric calledF-measure the F-measure that is defined as:$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$ The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall. Harmonic mean is used because it is a conservative metric; the harmonic mean oftwo values is closer to the minimum of the two values than the arithmetic mean is.Thus it weighs the lower of the two numbers more heavily. In any-of or multi-label classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d. one-of or multinomial classification, multinomial classification in which the classes are mutually exclusive and each document or item appears inexactly one class. Build a separate binary classifier trained on positiveexamples from c and negative examples from all other classes. Now given a testdocument or item d, we run all the classifiers and choose the label from the classifier with the highest score. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. The only problem with cross-validation is that because all the data is used fortesting, we need the whole corpus to be blind; we can’t examine any of the datato suggest possible features and in general see what’s going on. But looking at thecorpus is often important for designing the system. For this reason, it is commonto create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Noisy_Channel]]></title>
    <url>%2F2018%2F06%2F27%2FNoisy-Channel%2F</url>
    <content type="text"><![CDATA[Noisy Channel ModelThe intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. $$\hat{w} = \operatorname{arg\,max}_{w\in C} p(x|w)p(w)$$ C is the list of candidate words. Above is a kind of Bayesian inference. We see an observation x (a misspelled word) and find the word w that generated this misspelled word. Out of all possible words in the vocabulary V or just a list of candidate word C, we want to find theword w such that P(w|x) is highest. Channel Model p(x|w)It’s expensive to get a perfect channel model. But a pretty reasonable estimateof P(x|w) could be generated just by looking at local context: the identity of the correct letter itself, themisspelling, and the surrounding letters. confusion matrixContains counts of errors. In general, each entry in a confusion matrix represents the number of times one thing was confused with another. Thus for example a substitution confusion matrix will be a square matrix of size 26×26 (or more generally |A| × |A|,for an alphabet A) that represents the number of times one letter was incorrectlyused instead of another. del[x, y]: count(xy typed as x) ins[x, y]: count(x typed as xy) sub[x, y]: count(x typed as y) trans[x, y]: count(xy typed as yx) Methods to get confusion matrix: 1, extract them from lists of misspellings $$p(x|w) = \frac{ins[x_{i-1},w_i]}{count[w_{i-1}]},\quad \text{if insertion}$$ $$p(x|w) = \frac{sub[x_{i},w_i]}{count[w_{i}]},\quad \text{if substitution}$$ The probability of substitution is calculated by:given the correct word w and error word x, the number of times that ith character wi and the ith character xi is substituted divided by total number of ith character wi.wi is the ith character of the correct word w and xi is the ith character of the typo x 2, EM algorithm.compute the matrices by iteratively using the spelling error correction algorithm itself. The iterative algorithm first initializes the matrices with equal values; thus, any character is equally likely to be deleted, equally likely to be substituted for any other character,etc. Next, the spelling error correction algorithm is run on a set of spellingerrors. Given the set of typos paired with their predicted corrections, the confusionmatrices can now be recomputed, the spelling algorithm run again, and so on. Evaluating spell correction algorithms is generally done by holding out a training,development and test set from lists of errors like those on the Norvig and Mittonsites mentioned above. Candidate words:Utilize the extended minimum edit distance algorithm introduced so that in addition to insertions, deletions, and substitutions, there is transpositions, in which two letters are swapped. All words with edit distance of 1 constitutes the candidate words set. For words with specific context, it is important to use larger language models than unigrams such that the pobability p(w) is extended to \(p(w_{i-1})*p(w_i)\) for bigram model to take the context into consideration. Real-word spelling errorsNoisy channel algorithm could also be applied. The algorithm takes the input sentence \(X = {x_1, x_2,…, x_k,…, x_n}\), generates a large set of candidate correction sentences C(X),then picks the sentence with the highest language model probability. To generate candidate set C(X), each word is assigned the probability that would make an error. And score each sentence like the aforementioned way. The difference is that the probability that the word is indeed correct needs to be considered. Given a word w, let’s assume the correct probability is \(\alpha\). And the error probability is \(\frac{1-\alpha}{|C(x)|} \) if \(x \in C(x)\). The others are totally the same as normal noisy channel model for wrong words.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP_Notation]]></title>
    <url>%2F2018%2F06%2F26%2FNLP-Notation%2F</url>
    <content type="text"><![CDATA[Language models assign a probability to each sentence \( w_1^n\) is a sequence of N words from word 1 to word N. A held-out corpus is an additional training corpus that we use to set hyperparameters. Non-word spelling correction is the detection and correction of spelling errors that result in non-words (like graffe for giraffe). real word spelling correction is the task of detecting and correcting spelling errors even if they accidentally result in an actual word of English (real-word errors). edit probability The probability of typo like deletion, insertion respectively. text categorization: classifying an entire text by assigning it a textcategorization label drawn from some set of labels. classification: take a single observation, extract some usefulfeatures, and thereby classify the observation into one of a set of discrete classes. Generative classifiers like naive Bayes build a model of each class. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes.While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. bag-of-words: an unordered set of words with their position ignored, keeping only their frequency in the document. linear classifiers: use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression. period disambiguation: deciding if a period is the end of a sentence or partof a word co-occurrence matrix: a way of representing how often words co-occur. term-document matrix: each row represents a word in the vocabulary andeach column represents a document. vector space model:a document is represented as a count vector(column vector), each entry is the count of a specific word. Information retrieval(IR): is the task of finding the document d from the Ddocuments in some collection that best matches a query q which is also a vector of length |V|. term-context matrix: a |V| × |V| matrix. Each entry(i,j) represent that the number of times a word i shows in the context word j. The context could be a document or a context window of length |L|. WordNet(Sense-Tagged data):WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of synonyms , each expressing a distinct concept. part-of-speech tags: CC Coordinating conjunction CD Cardinal number DT Determiner EX Existential there FW Foreign word IN Preposition or subordinating conjunction JJ Adjective JJR Adjective, comparative JJS Adjective, superlative LS List item marker MD Modal NN Noun, singular or mass NNS Noun, plural NNP Proper noun, singular NNPS Proper noun, plural PDT Predeterminer POS Possessive ending PRP Personal pronoun PRP$ Possessive pronoun RB Adverb RBR Adverb, comparative RBS Adverb, superlative RP Particle SYM Symbol TO to UH Interjection VB Verb, base form VBD Verb, past tense VBG Verb, gerund or present participle VBN Verb, past participle VBP Verb, non-3rd person singular present VBZ Verb, 3rd person singular present WDT Wh-determiner WP Wh-pronoun WP$ Possessive wh-pronoun WRB Wh-adverb]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Smoothing]]></title>
    <url>%2F2018%2F06%2F26%2FSmoothing%2F</url>
    <content type="text"><![CDATA[Smoothing:basically discount the observed words to assign to words with zero count so as to deal with the zero probability words. Laplace SmoothingAlso called add-one smoothing. the unsmoothed maximum likelihood estimate of the unigram probability of the word wi is its count ci normalized by the total number of word tokens N $$ p(w_i)= \frac{c_i}{N} $$ Laplace smoothing merely adds one to each count. Denominator is also adjusted by adding V(number of observations). If the demoninator is still unchanged, the sum of all probability would exceed 1. $$ p_{Laplace}(w_i)= \frac{c_i+1}{N+V} $$ adjusted count: describe how a smoothing algorithm affects the numerator $$ c_i^* = \frac{c_i+1}{N+V} \times{N} = p_{Laplace}(w_i) \times{N} $$ the ratio of the discounted counts to the original counts describe a smoothing algorithm in terms of a relative discount dc: $$ d_c= \frac{c^*}{c} $$ Normal bigram probabilities are computed by normalizing each row of counts by the unigram count: $$ p(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)}{c(w_{n-1})} $$ After smoothing: $$ p_{Laplace}(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)+1}{c(w_{n-1})+V} $$ These adjusted counts can be computed by equation below to show the reconstructed counts. $$ c^*(w_{n-1}w_n) = \frac{c(w_{n-1}w_n)+1}{c(w_{n-1)}+V} \times{c(w_{n-1})} = p_{Laplace}(w_n|w_{n-1}) \times{c(w_{n-1})} $$ Add-k smoothing:add k rather than one; $$ p_{Add-k}^*(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)+k}{c(w_{n-1})+kV} $$ chosse k by optimizing on a devset. Although add-k is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts Backoff and Interpolationbackoff: If there is zero evidence for a higher-order N-gram, use lower-order N-gram instead. i.e.: If we are trying to compute $$ P(w_n|w_{n−2}w_{n−1}) $$ but we have no examples of a particular trigram wn−2wn−1wn, we can instead estimate its probability by using the bigram probability $$ P(w_n|w_{n−1}) $$. interpolation: we always mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts. In a slightly more sophisticated version of linear interpolation, each λ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the λs for those trigrams higher and thus give that trigram more weight in the interpolation. $$ \hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 (w_{n-2}^{n-1})p(w_n|w_{n-2}w_{n-1}) + \lambda_2 (w_{n-2}^{n-1})p(w_n|w_{n-1}) +\lambda_3 (w_{n-2}^{n-1})p(w_n) $$ simple interpolation: $$ \hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 p(w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) +\lambda_3 p(w_n) $$ In both kinds of interpolation: $$ \sum_i \lambda_i = 1 $$ Both the simple interpolation and conditional interpolation λ are learned from a held-out corpus. choose the λ values that maximize the likelihood of the held-out corpus.There are various ways to find this optimal set of λs. One way is to use the EM algorithm, which is an iterative learning algorithm that converges on locally optimal λs In terms of quality of N-gram model: quadrigram &gt; trigram &gt; bigram &gt; unigram In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. In addition to this explicit discount factor, we’ll need a function α to distribute this probability mass to the lower order N-grams. This kind of backoff with discounting is also called Katz backoff. In Katz backoff we rely on a discounted probability \(P^∗\) if we’ve seen this N-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probability for the shorter-history (N-1)-gram. The probability for a backoff N-gram PBO is thus computed as follows: $$\begin{cases} P^*(w_n|w_{n-N+1}^{n-1})\quad \text{if}\;C(w_{n-N+1}^{n-1}) &gt; 0 \ \alpha(w_{n-N+1}^{n-1})P_{BO}(w_n|w_{n-N+2}^{n-1})\quad \text{otherwise} \end{cases} $$ Katz backoff is often combined with a smoothing method called Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computationfor estimating the Good-Turing smoothing and the \(P^*\) and \(\alpha\) values. Kneser-Ney SmoothingNovel continuation means the first time that the word \(w_{i-1}\) preceeds the word \(w_i\). The number of times a word w appears as a novel continuation can be expressed as: $$P_{continuation}(w_i)\propto \lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert$$ \(w_i\) is the current word. \(w_{i-1}\) is the words occur before the current word(also called preceeding word). To turn this count into a probability, we normalize by the total number of bigram word. In summary: $$P_{continuation}(w_i)=\frac{\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert}{\lvert{(w_{j-1},w_j):c(w_{j-1}w_j)&gt;0}\rvert}$$ Or normalized by the number of words preceding all words as follows: $$P_{continuation}(w_i)=\frac{\lvert { w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert}{\sum_{w’i} \lvert {w’{i-1}:C(w’_{i-1}w’_i)&gt;0}\rvert}$$ The number of words preceeding all words is:given word \({w’}i\),the number of word \(w’{i-1}\) that preceed \({w’}_i\).\({w’}_i\) are all words in the corpus. So The number of words preceeding all words = the total number of bigram word. The final equation for Interpolated Kneser-Ney smoothing for bigrams is then: $$P_{KN}(w_i|w_{i-1})=\frac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})}+\lambda(w_{i-1})P_{continuation}(w_i)$$ The λ is a normalizing constant that is used to distribute the probability mass that is discounted: $$\lambda(w_{i-1})=\frac{d}{C(w_{i-1})}\cdot \lvert {w:C(w_{i-1},w)&gt;0}\rvert$$ The first term \(\frac{d}{C(w_{i-1})}\) is the normalized discount. The second term \(\lvert {w:C(w_{i-1},w)&gt;0}\rvert\)is the number of word types that can follow \(w_{i−1}\) or, equivalently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount. The general recursive formulation is as follows: $$P_{KN}(w_i|w_{i-n+1}\cdots w_{i-1})=\frac{max(0,C_{KN}(w_{i-n+1} \cdots w_i) - d)}{C_{KN}(w_{i-n+1}\cdots w_{i-1})}+\lambda(w_{i-n+1}\cdots w_{i-1})\cdot P_{KN}(w_i|w_{i-n+2}\cdots w_{i-1})$$ $$\lambda(w_{i-n+1}\cdots w_{i-1})=\frac{d}{C_{KN}(w_{i-n+1}\cdots w_{i-1})}\cdot \lvert {w:C_{KN}(w_{i-n+1} \cdots w_{i-1}w)&gt;0}\rvert$$ where the definition of the count \(c_{KN}\) depends on whether we are counting the highest-order N-gram being interpolated (for example trigram if we are interpolat- ing trigram, bigram, and unigram) or one of the lower-order N-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram): $$C_{KN}(\cdot)=\begin{cases}count(\cdot) &amp;,\text{for the highest order}\continuationcount(\cdot) &amp;,\text{for all other lower orders}\end{cases}$$ The continuation count is the number of unique single word contexts for ·.At the termination of the recursion, unigrams are interpolated with the uniform distribution, where the parameter ε is the empty string: $$P_{KN}(w)=\frac{max(c_{KN}(w)-d,0)}{\sum_{w’}c_{KN}(w’)} +\lambda(\epsilon)\frac{1}{V}$$ If we want to include an unknown word , it’s just included as a regular vo-cabulary entry with count zero, and hence its probability will be a lambda-weighteduniform distribution \(\frac{\lambda(\epsilon)}{V}\). The best-performing version of Kneser-Ney smoothing is called modified Kneser- Ney smoothing. Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and \(d3_+\) for N-grams with counts of 1, 2 and three or more, respectively. EntropyEntropy is the minimum number of bits(if log base 2 is used) that is needed to encode(represent) information in the optimal coding scheme. Entropy rate the entropy of a sequence divided by the number of words. To measure entropy rate of a language, entropy rate of a infinite sequence needs to be calculated. $$H(L) = −\lim_{n\to\infty} \frac{1}{n}H(w_1,w_2,…,w_n)= −\lim_{n\to\infty}\sum_{W\in L} p(w_1,…,w_n)\frac{1}{n} \log p(w_1,…,w_n)$$ The Shannon-McMillan-Breiman theorem states that if the language is rboth stationary and ergodic $$H(L) = \lim_{n\to\infty}-\frac{1}{n}\log p(w_1,…,w_n)$$ The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.That is, we can take a single sequence that is long enough instead of summing over all possible sequences. A stochastic process is said to be stationary if the probability distribution for words at time t is the same as the probability distribution at time t + 1. Markov models, and hence N-grams, are stationary. Natural language is not stationary, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability. The cross-entropy is useful when we don’t know the actual probability distribution p that generated some data. It allows us to use some m, which is a model of p (i.e., an approximation to p) to approximate the real entropy. Thecross-entropy of m on p is defined by $$H(p,m) = \lim_{n\to\infty}\sum_{W\in L}- p(w_1,…,w_n)\frac{1}{n} \log m(w_1,…,w_n)$$ That is, we draw sequences according to the probability distribution p, but sum the log of their probabilities according to m.Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process: $$H(p,m) = \lim_{n\to\infty}-\frac{1}{n}\log m(w_1,…,w_n)$$ This means that, as for entropy, we can estimate the cross-entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences.What makes the cross-entropy useful is that the cross-entropy H(p,m) is an upper bound on the entropy H(p).For any model m: $$H(p) ≤ H(p,m)$$ This means that we can use some simplified model m to estimate the true entropy of a sequence of symbols drawn according to probability p. The more accurate m is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus, the difference between H(p,m) and H(p) is a measure of how accurate a model is. The lower the cross-entropy, the better is the model(better approximation of the true entropy). The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity. A (sufficiently long) sequence of fixed length N is employed to approximate the cross-entropy of a model M = $ P(w_i|w_{i−N+1}…w_{i−1}) $(N is the N of a N-gram model) on a sequence of words W is $$H(W) = -\frac{1}{N}\log p(w_1,…,w_n)$$ The perplexity of a model P on a sequence of words W is now formally defined as the exp of this cross-entropy: $$Perplexity(W) = 2^{H(W)} = P(w_1,…,w_n)^{-\frac{1}{N}}$$]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Max_Matching]]></title>
    <url>%2F2018%2F06%2F25%2FMax-Matching%2F</url>
    <content type="text"><![CDATA[Max Matching AlgorithmIt is a kind of greedy algorithm used to tokenize sentences. It is a baseline of tokenize algorithms. It works especially well on Chinese. I think it is because that Chinese words are single word that could not be took apart while English words are sequences of characters. 12345678910111213141516171819202122232425262728293031323334353637def read_words(file): with open(file) as f: l = f.readlines() return ldef max_matching(text, dictionary): # l = [] w = '' for i in range(len(text),0,-1): word = text[:i] if word in dictionary: w += word+' ' w += max_matching(text[i:],dictionary) + ' ' return w if len(text) != 0: w = text[0]+' ' w += max_matching(text[1:],dictionary) + ' ' return w else: return ''if __name__ == "__main__": file = '/Users/lmk/Documents/NLP/word.csv' l = read_words(file) # l = read_words(file) l = list(map(lambda i: i[3:-1], l)) l[0] = l[0][1:] dic = l # s = "ilovechina" s = "is also possible to use a list as a queue, where the first" sentence = max_matching(s,dic) token = sentence.split() # flatten = lambda l: [item for sublist in l for item in sentence] # flat_list = [item for sublist in l for item in sublist] print(sentence) print(token)]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Minimum_Edit_Distance]]></title>
    <url>%2F2018%2F06%2F25%2FMinimum-Edit-Distance%2F</url>
    <content type="text"><![CDATA[Minimum Edit Distanceminimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. It’s useful in terms of portential splleing checking and string alignment which is significant in terms of machine translation and speech recognition. 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npclass Solution: def insertion_cost(self,c1): '''customize your insertion cost''' return 1 def deletion_cost(self,c1): '''customize your deletion cost''' return 1 def substitution_cost(self,c1,c2): '''customize your substitution cost''' if c1 == c2: return 0 return 1 def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ n = len(word1)+1 #row of matrix m = len(word2)+1 #column of matrix l = np.zeros((m,n)) # l = [n*[0] for i in range(m)] for i in range(m): l[i,0] = i for i in range(n): l[0,i] = i # print(l) for i in range(1,m): for j in range(1,n): # print(i,j) l[i][j] = min(l[i-1][j]+self.insertion_cost(word2[i-1]), l[i][j-1]+self.deletion_cost(word1[j-1]), l[i-1][j-1]+self.substitution_cost(word1[j-1],word2[i-1])) # l = [list(range(n)) for i in range(m)] # print(l) return l[-1][-1] minimum edit distance with backtrace123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import numpy as npclass Edit_Distance: def __init__(self): self.back_trace = None self.n = 0 self.m = 0 self.l = None def insertion_cost(self,c1): '''customize your insertion cost''' return 1 def deletion_cost(self,c1): '''customize your deletion cost''' return 1 def substitution_cost(self,c1,c2): '''customize your substitution cost''' if c1 == c2: return 0 return 2 def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ n = len(word1)+1 #row of matrix m = len(word2)+1 #column of matrix self.n = n self.m = m l = np.zeros((m,n)) # back_trace = np.zeros((m,n)) back_trace = [n*[0] for i in range(m)] for i in range(m): l[i,0] = i for i in range(n): l[0,i] = i # print(l) for i in range(1,m): for j in range(1,n): # print(i,j) cost = [l[i-1][j]+self.insertion_cost(word2[i-1]), l[i][j-1]+self.deletion_cost(word1[j-1]), l[i-1][j-1]+self.substitution_cost(word1[j-1],word2[i-1])] min_cost = min(cost) print((i,j),min_cost) back_trace[i][j] = [i for i,j in enumerate(cost) if j == min_cost] l[i][j] = min_cost # l = [list(range(n)) for i in range(m)] # print(l) # print(back_trace) self.l = l self.back_trace = back_trace return l[-1][-1] # def print_direction(self,l): # if def output_back_trace(self): i = self.n-1 j = self.m-1 while i &gt;= 0 and j &gt;= 0: # print((i,j)) direction_list = self.back_trace[i][j] #choose the last direction, here every direction has the same cost print(direction_list) if isinstance(direction_list,list): direction = direction_list[0] else: direction = direction_list if direction == 2: i -= 1 j -= 1 elif direction == 1: j -= 1 else: i -= 1 # print(self.l[i][j])s = Edit_Distance()# word1 = "intention"# word2 = "execution"word1 = "execution"word2 = "intention"leg = s.minDistance(word1,word2)print(leg)s.output_back_trace()print(s.l)]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Port_Stemmer]]></title>
    <url>%2F2018%2F06%2F25%2FPort-Stemmer%2F</url>
    <content type="text"><![CDATA[LemmatizationLemmatization is the task to determine whether two words have the same root. If proper lemmatization has been applied, number of tokens could be reduced significantly and understanding of the text could also be facilitated. Problem of suffix strippingprobe and probatewand and wanderSometimes removal of suffix could affect the original meaning and degrade the performance.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sort]]></title>
    <url>%2F2018%2F06%2F07%2FSort%2F</url>
    <content type="text"><![CDATA[Quick SortPartition the array into two parts such that elements in part a &lt; that of part b. Recursively invoke quick sort to do in-place sorting. Average performance is the best in practice. O(nlogn). The worst case happens when the array is inversely sorted, which leads the subarray after partition is extremely imbalanced. Time complexity is O(n^2). Merge SortDivide and conquer. Divide the array into two parts and then merge recursively. The process of sorting is just like building a binary tree in a top-down manner and then merge the array bottom-up. Heap Sortbuild a max heap so that the first element in the heap is always the largest. Sorting is just switching the largest value with the last element in the array and maintain a heap. after poping the element, reduce the heap size by one. ##PerformanceQucik sort is the fastest. After it is merge sort. And then heap sort. build a heap takes considerable time. Buble sort and insertion sort are far slower.Random an array of length 1000000, quick sort took 5.30535101890564 seconds and merge sort took 5.918827056884766 seconds. Their perfomance are quite good and close to each other. Heap sort took 13.471954822540283 seconds. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223class Sort(): def __init__(self, l = None): if l is not None: assert type(l) == list,("please input a list") else: l = self._generate() self.l = l self.result = None def insert_sort(self): l = self.l #add new reference to self.l, change l would change self.l for i in range(1,len(l)): #use built-in methods to insert for j in range(i-1,-1,-1): if l[i] &lt; l[j]: if j == 0: l.insert(j,l[i]) del(l[i+1]) elif l[i] &gt;= l[j-1]: l.insert(j, l[i]) del (l[i + 1]) def insert_sort_v1(self): #much slower than using built-in method l = self.l #add new reference to self.l, change l would change self.l for i in range(1,len(l)): #switch position for j in range(i-1,-1,-1): if l[j+1] &lt; l[j]: l[j], l[j+1] = l[j+1], l[j] def bubble_sort(self): l = self.l for i in range(0,len(l)-1): for j in range(0,len(l)-i-1): if l[j] &gt; l[j+1]: l[j], l[j+1] = l[j+1], l[j] def quick_sort(self, p=None, r=None): '''extra space usage''' '''divide the original into two parts such that elements in part a &lt; that of part b''' if p is None or r is None: p = 0 r = len(self.l) # print("p ",p) # print("r ", r) if p&lt;r-1: q = self._quick_sort_partition(p, r) # print("q ", q) self.quick_sort(p,q) self.quick_sort(q,r) # self._quick_sort_partition(p, q) # self._quick_sort_partition(q, r) # q = self._quick_sort_partition(p, r) # print(q) # while True: # if self._quick_sort_partition(p, q) == 0: # break # while True: # if self._quick_sort_partition(q, r) == r-1: # break # q = self._quick_sort_partition(0,len(self.l)) # self.quick_sort(p,q) # self.quick_sort(q,r) # else: # q = self._quick_sort_partition(p, r) # self.quick_sort(p, q - 1) # self.quick_sort(q, r) def _quick_sort_partition(self, p, r): # l = list(self.l) l = self.l i = p-1 pivot = l[r-1] for j in range(p,r-1): #in-place exchange without memory overhead if l[j] &lt;= pivot: i += 1 l[i], l[j] = l[j], l[i] # print(i) # print(l) l[i+1], l[r-1] = l[r-1], l[i+1] # print(l) # print(self.l) # print("i ",i) return i+1 def merge_sort(self, p=None, r=None): l = self.l if p is None or r is None: p = 0 r = len(l) #not include r if p &lt; r-1: q = int((p+r)/2) self.merge_sort(p,q) self.merge_sort(q, r) self.merge(p,q,r) def merge(self, p,q,r): l = self.l L = l[p:q] R = l[q:r] L.append(float('inf')) R.append(float('inf')) i,j = 0,0 for k in range(p,r): #r = len(list), not include r if L[i] &lt; R[j]: l[k] = L[i] i+=1 else: l[k] = R[j] j += 1 def merge_two_lists(self,l1,l2): assert l1 is None or l2 is None,("empty list") l = [] i = 0 j = 0 while i&lt;len(l1) and j&lt;len(l2): if l1[i] &lt; l2[j]: l.append(l1[i]) i+=1 else: l.append(l2[j]) j += 1 if i == len(l1): l.extend(l2[j:]) else: l.extend(l1[i:]) return l def heap_sort(self): l = self.l self.heap_size = len(l) self.build_heap() for i in range(len(l)-1,0,-1): #no need to include 0. change with itself is meaningless l[i], l[0] = l[0], l[i] self.heap_size -= 1 self.max_heaplify(0) def build_heap(self): #bottom up. the first level of the tree has no child, no need to maintain for i in range(int(self.heap_size/2),-1,-1): self.max_heaplify(i) def max_heaplify(self,i): l = self.l left = 2*i +1 right = 2*i +2 largest = i if left &lt; self.heap_size and l[left] &gt; l[i]: largest = left if right &lt; self.heap_size and l[right] &gt; l[largest]: largest = right if largest != i: '''if position changes, need to recursively run max_heaplify to guarantee the descendant are also max heap''' l[i], l[largest] = l[largest], l[i] self.max_heaplify(largest) def _evaluate(self): if self.result is None: self.result = self.l for i in range(1,len(self.result)): if self.result[i] &lt; self.result[i-1]: return False return True def _generate(self,length = None): try: import random if length is None: length = random.randint(0, 1000) # length = random.randint(0,100000) l = [] for i in range(length): l.append(random.randint(-length,length)) return l except ImportError: print("no random module, please input a sequence")l = [4,6,1,0,5,-9,0,5,]# l = [4,6,10,23,-1,-6]# s = Sort(l)# print(s._quick_sort_partition(0,len(l)))# print(s.l)s = Sort()# s.quick_sort()print("length ",len(s.l))s.heap_sort()# s.build_heap()# s.merge_sort()# s.insert_sort_v1()# s.insert_sort()print(s._evaluate())# s._quick_sort_partition(0,len(l))print(s.l)def compare_time(): import time print("**************compare time**************") s = Sort() l = s._generate(1000000) start_time = time.time() Sort(l).quick_sort() print("--- %s seconds ---" % (time.time() - start_time)) start_time = time.time() Sort(l).merge_sort() print('--- &#123;&#125; seconds ---'.format(time.time() - start_time)) start_time = time.time() Sort(l).heap_sort() print('--- &#123;&#125; seconds ---'.format(time.time() - start_time))compare_time() ###Reference: Introduction to Algorithms]]></content>
  </entry>
  <entry>
    <title><![CDATA[Remove_Duplicates_from_Sorted_List_II]]></title>
    <url>%2F2018%2F06%2F05%2FRemove-Duplicates-from-Sorted-List-II%2F</url>
    <content type="text"><![CDATA[##Remove Duplicates from Sorted List II: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ node = head if head == None: return None if head.next == None: return head if head.next.val != head.val: #the first two elements are not the same current_node = node node = node.next while node != None: if node.next == None: current_node.next = node return head if node.next.val != node.val: current_node.next = node current_node = node else: # when next node is the same as the current node while node != None: if node.next == None: current_node.next = None return head else: node = node.next if node.next == None: current_node.next = None return head if node.val != node.next.val: break node = node.next else: current_node = None #the first two elements are same, pointers point to None while node != None: if node.next == None: if current_node == None: # all elements includes the first few elements are same return node #return None current_node.next = node #add the last node to the linked list return head if node.next.val != node.val: #current node is to record the last node in the linked list if current_node == None: #which is identical in the list current_node = node head = current_node else: current_node.next = node current_node = node else: # when next node is the same as the current node while node != None: # if the node is the same with next node if node.next == None: # this loop is to skip the node current_node.next = None return head else: node = node.next if node.next == None: if current_node == None: return None current_node.next = None return head if node.val != node.next.val: break node = node.nexts = Solution()l = [1,1,2]# l = [1,2,3,3,4,4,5,5]# l = [2,2,3,4,5,5]LN = ListNode(l[0])# for i in range(1,len(l)):# LN.nextnode_list = list(map(ListNode,l))# print(node_list[0])# print(ListNode(l[0]))for i in range(len(node_list)-1): node_list[i].next = node_list[i+1]h = s.deleteDuplicates(node_list[0])while h!= None: print(h.val) h = h.next This version uses three pointer to do the same thing. From leetcode. 123456789101112131415161718192021222324252627282930class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ if not head or not head.next: return head fakehead = ListNode(0) fakehead.next = head prev = fakehead slow = head fast = head.next while fast: if fast.val == slow.val: while fast and fast.val == slow.val: fast = fast.next slow = prev else: prev = slow slow = slow.next slow.val = fast.val fast = fast.next slow.next = None return fakehead.next]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentence_Similarity]]></title>
    <url>%2F2018%2F05%2F27%2FSentence-Similarity%2F</url>
    <content type="text"><![CDATA[##NLP typical Step:Tokenize: separate the words in the sentence Convert the sentence to vector: the sentence has been divided into several words, and load word2vec model and convert each word to a vector ##Similarity MeasurementTF-IDF model: convert the sentence to a vector of words, use cosine similarity to measure. In this applicatoin, not very good. Bag of words: Modern way: NLP step as above. Use pre-train word vectors to represent a sentence. Compute distance. Huge amount of computation. In this application, extract several keywords first as heuristic to rule out a lot of sentences and then start using this method to reduce computation amount. The best accuracy in this application. Change pre-train vectors and brute-force computation would increase the accuracy. ##Some NotesTokenize: separate the words in the sentence Bag of words: just count the frequency of words that appear in the sentence and neglect the sequence of words Word representation: one-hot representationNot easy to extract connection because dot product between any two words are 0 Featurized representation: word embedding. Using a matrix to represent values of features, and stand for a word. It could be understood as a word embedded in a high dimensional space Analogy of word embedding. Use cosine similarity to measure whether two words are similar. Take the dot product of two words vectors and divide multiplication of their lengths. Embedding matrix: num of features * number of words. Each column is a feature vector. Use one-hot vector to multiply embedding matrix get the corresponding feature vector Word2vec:Skip-gram: randomly choose a context word and corresponding target word. Target word is in the window(like 5 words before or after the context word) of context wordTarget is to learn good word embedding rather than perform good supervised learningHow to sample context word? Employ some techniques to avoid commonly appeared word like the ,a and so on Negative sampling: one positive sample and k randomly chosen negative sample to simplify computation Content of word d: means the range of plus or minus 5 words Sequence to sequence:Machine translation: input sentence to encoder, and output is a decoder. Conditional language model. Green part is encoder, purple is decoder Beam search: a kind of approximate optimization algorithmbeam width n: track The most possible n sequences with the highest probability. Larger n, the algorithm runs better but need more memory. Smaller n: more efficient, less accurateunlike BFS and DFS, beam search do not guarantee to find the best solution. It’s not complete because it only track a few states.Start by choosing the first word for n sentences, and then second……. Maximize this probability to find the most possible sequenceMultiplication of several values that are less than one leads to numerical problem. Take Log on it solve this problem. Error analysis: working to optimize some sort of cost function/objective function that is output by a learning algorithm like RNNRNN output the p(y|x) for sentences and beam search choose argmaxP(y|x). if RNN output the correct probability but beam search choose the wrong one, beam is at fault. And vise versa, if RNN output the wrong probability for sentences, RNN is at fault. Correct probability means P(correct sentence | x) &gt; P(wrong sentence | x)Compare beam search with RNN, find which one is at fault. And then modify and improve it. Attention model:Just focus on a few words around rather than the whole sentence to translate. Divide and conquer, get better result. Speech recognition:Turn time domain signal to frequency domain signalUsually input is much larger than output CTC cost:Basic idea: collapse repeated characters not separated by “blank”]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String_Matching]]></title>
    <url>%2F2018%2F05%2F25%2FString-Matching%2F</url>
    <content type="text"><![CDATA[##String Matching: ###The Rabin-Karp algorithmIntuition:Use the values of strings as heuristic, if they are equal, they matches, else they don’t. Because the string may be very long to do the computation efficiently, we induce mod so as to reduce the computation complexity dramatically. In general, with a d-ary alphabet {0,1,2… d-1}, we choose q so that d*q fits within a computer word.d is the length of the alphabet set. 123456789101112131415161718192021'''please input pattern and string'''p = input()s = input()d = int(input())q = int(input())l_p = len(p)h = d**(l_p-1) % q'''Preprocessing'''p_sum, s_sum = 0,0for i in range(0,l_p): p_sum = (d*p_sum + int(p[i])) %q s_sum = (d * s_sum + int(s[i])) %q # p_sum = (p_sum+(d*p_sum + int(p[i]))) % q # s_sum = (s_sum+(d * s_sum + int(s[i]))) % qfor j in range(0,len(s)-l_p+1): if p_sum == s_sum: if s[j:j+l_p] == p: print(s[j:j+l_p]) if j &lt; len(s)-l_p: s_sum = (d*(s_sum-int(s[j])*h) + int(s[j+l_p])) % q Worst case: the string and pattern have all same character. O((n-m+1)*m)Best case: O(n) + O(m(v+n/q)) ###KMP algorithm Each time the given string match with the pattern, by utilizing the information contains in the pattern string, the algorithm may skip several matching without affecting the correctness. Let’s assume current position of the string is s. Length of matched string is q. There may exists a k&lt;q(if k &gt; q, we don’t know what’s them outside the pattern window) such that P[1…k] = T[s’+1…s’+k]. s’ is the new position of pointer. K is the length that the substring T[s’] starting from s’ matches with P[1..k].So s’ = s+q-k Preprocessing:Find the longest prefix of the matched string such that it is also the suffix of the matched string. Time complexity: O(n) 12345678910111213141516171819202122232425def compute_prefix(p): l = [0] * len(p) k = 0 for i in range(1,len(p)): '''#This line decrease k. because 0 &lt; k &lt; i, the total time that i has been increased &lt; len(p), so the total time that while loop execute would &lt; len(p)''' while k&gt;0 and p[i] != p[k]: k = l[k] if p[i] == p[k]: k += 1 # this increase k l[i] = k return ldef KMP_Match(s,p): l = compute_prefix(p) # print(l) q = 0 for i in range(0,len(s)): while q &gt; 0 and p[q] != s[i]: q = l[q] if p[q] == s[i]: q += 1 if q == len(p): print(s[i-q+1:i+1]) q = l[q-1]]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendation_System]]></title>
    <url>%2F2018%2F05%2F24%2FRecommendation-System%2F</url>
    <content type="text"><![CDATA[Advertisement recommendation is important! It is a significant source of income for many companies. Abuse of advertisement would annoy user. How to choose potential interested user is a challenging problem. Our application is an advertisement recommending system, which simulates the process of recommend advertisements to users who may need them. Different people have different appetites, this works same on advertisements. Packages Descriptions Xen 4.4.1 Hypervisor for virtual machines Hadoop 2.6.0 open-source software framework for storage and large scale processing of data_sets on clusters of commodity hardware. Spark 2.1.0 A fast and general engine for large-scala data processing Spark MLlib An apache Spark’s scalable machine learning library. Hive A data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage SparkSql An Apache Spark’s module for working with structured data Tomcat 8.5.29 Web server Key Algorithms Collaborative Filtering Metric Learning K-means Key Features K-means clustering with Mahalanobis Distance SQL to form a utility matrix according to user behavior Employ clustering to estimate empty entries in the matrix Recommend to user who have bought similar items Xen Configuration Pin CPUs to virtual machines Migrate some memory to virtual machines to optimize performance Optimization of PerformanceSmall dataset is employed to optimize parameters. Parameters Running Time Num-execution 5—-7 3 mins 45 sec -&gt; 3 mins 13 sec Parallelism 8—-16 3 mins 13 sec -&gt; 1 min 38 sec Executor memory 1000m-2000m 3 mins 22 sec -&gt; 2 mins 54 sec Novelty In clustering, the distance functionis Mahalanobis Distance rather than euclidean distance. Traditional collaborative filtering utilizes rows and columns of utility matrix to estimate similarity of items and users as well as empty entries in the utility matrix.In our algorithm, we use clustering to estimate the empty entries. Algorithm IntroductionClusteringTwo kinds of clustering: hierarchica and point assignment. Hierarchical:Merge clusters. Each unassigned point is a cluster.The measurements are diameter, radius, average distance, minimum distance.Radius: the maximum distance from any points to centroid.Diameter: maximum distance between any two points. Point assignment: Assign each point to the nearest cluster. K-means++:initialize several points which are as far away as possible from each other as centroid of clusters.Assign the points to these different clusters and recompute the centroids. Complexity:O(n K I * d )n = number of records, K = number of clusters,I = number of iterations, d = number of attributes Recommendation SystemThe ultimate goal of a recommendation system is to estimate those empty entries in the utility matrix. To estimate empty entry, one possible way is to find n similar users and count their normalized ratings. Another possible way is to find the rating of similar items given by the same user. Methods: Collaborative filtering: use columns and rows of utility matrix to identify similar items or users and then do recommendation Content based:Utility matrix: ranked item is 1(Boolean) or actual rating(numerical) * normalizing factor. Others are represented by 0 or blank. Association Analysis. Characteristics of different methordsUser-based: tailer-made, accurate. Not easy to measure similarity.Content-based: Reliable, wider-range of recommendation. Construction of item profiles are tedious. NotesBelow are just notes of this project, feel free to skip it.Item features could also be represented by tags. But it is tedious to get tags data. Content-based recommendation is to create both an item profile consisting of feature-value pairs and a user profile summarizing the preferences of the user, based of their row of the utility matrix. Discretize numerical data to nominal data would lose the structure implicit in numbers. That is, two ratings that are close but not identical should be considered more similar than widely differing ratings. There is no harm if some components of the vectors are Boolean and others are real-valued or integer-valued. But it is necessary to somehow normalize the data so that the scale would not affect the result too much. Machine learning technique: ML algorithm to predict the empty entries in the utility matrix. Training set is the data. Optimize the loss. i.e. classification algorithm: build a decision tree to classify the data as like or don’t like for a particular user group. Recommendation System:Our recommendation system employs a modified version of collaborative filtering. In traditional recommendation system, in order to avoid constructing user and item profile which is a tedious and tricky step, rows and columns of utility matrix are utilized to estimate similarity of item and user as well as empty entries in the utility matrix. a utility matrix, giving for each user-item pair, a value that represents what is known about the degree of preference of that user for that item. Values come from an ordered set. We assume that the matrix is sparse, meaning that most entries are “unknown.” An unknown rating implies that we have no explicit information about the user’s preference for the item. Target of recommendation system:It is not necessary to predict every blank entry in a utility matrix. Rather, it is only necessary to discover some entries in each row that are likely to be high. In most applications, the recommendation system does not offer users a ranking of all items, but rather suggests a few that the user should value highly. It may not even be necessary to find all items with the highest expected ratings, but only to find a large subset of those with the highest ratings. User Profile: aggregation of utility matrix which represent connection between users and items. After getting user profile, make recommendation according to proximity between unseen object and content in user profile. (cosine similarity) The process of identifying similar users and recommending what similar users like is called collaborative filtering.Cosine distance treat empty entries as 0.Preprocessing: Round the data: round ratings to 0 or 1. Normalize the data and apply proximity measure. Measuring similarity:Jaccard distance: may emit important information like the scale of rating.Cosine distance The Duality of Similarity To estimate an entry, for item-based, find the most similar m items, and average them to get an estimate of rating for the entry(item). After cluster, get the entry for that item clusters by averaging the item Since there is user and ad profile, try to do clustering first. Data is large, initialize small cluster first to summarize the data points. Curse of dimensionality: in high dimension, cosine similarity approaches to 0, which means that random vectors are orthogonal. Distance and density become meaningless because distance is very far away compared to low dimensionality. Clustering:Euclidean space in which the cluster could be represented by a center.Clustroid: the closest point to other points in the clusterCentroid: the arithmetic center point calculated. Mahalanobis distance is distance between x and y divided by square root of covariance, which is undistorting the ellipse to make a circle ExtentionWord2Vec to do recomendation: sequence of actions of a user in a specific time range(within 30 minutes) is regarded as a context like the context words in word2vec. The search /query/clicked item is the center. As what has been done in word2vec, train the model s.t. the center item is close to context item and far away than other items. The learnt embeddings have been shown to correctly encode information like style,price and so on. The Airbnb makes some other modification: The final booking item is added s.t. the objective is to make the center item close to final booking item too. Sample items from the same location but different style or other condition as negative samples. ClusteringClustering is employed to promote diversity of recommendation. After the vectors of products are learnt, they are clustered together. If a customer buy something in a cluster, the system would choose items in another cluster in which other or similar users usually buy items after buying items in the same cluster. This avoid the problem of recommending the similar item with the same function to users. After choosing the cluster, recommend the top k items that are similar to the purchased item. Cold Start ProblemAverage 3 geographically closest listings accoding to location or age or something like that. ReferenceApplying word2vec to Recommenders and Advertising Listing Embeddings in Search Ranking]]></content>
      <tags>
        <tag>Project</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maximum_Subarray]]></title>
    <url>%2F2018%2F05%2F24%2FMaximum-Subarray%2F</url>
    <content type="text"><![CDATA[Dynamic ProgrammingTime Complexity: O(n) Store the first value. Traverse the array. If the last value of maximum subarray l is greater than 0, add the current value to it and store it. Else store it directly. Because if the last value of maximum subarray l is less than 0, no need to add the current value to it. If the last value of maximum subarray l is greater than 0, add the current value to it. Next time we check it, if it greater than 0, we add next value to it, else we discard it because it could not be the maximum subarray. 1234567891011121314151617181920212223242526class Solution: def maxSubArray(self, nums): """ :type nums: List[int] :type k: int :rtype: float """ if len(nums) == 1: return nums[0] l = [nums[0]] for i in range(1,len(nums)): # if nums[i] &lt;0: if l[i-1] &gt;0: l.append(nums[i] + l[i-1]) else: l.append(nums[i]) return max(l) # else:s = Solution()# leg = s.maxSubArray([-1,2,3,-6,8])leg = s.maxSubArray([-2,1,-3,4,-1,2,1,-5,4])# leg = s.max_cross([-2,1,-3,4,-1,2,1,-5,4])print(leg) divide and conquerrecursively divide a problem into subproblems.Time complexity: O(nlogn)logn: Each time divide the problem into two parts, so an array of size N could construct a log2N levels tree. Each level needs at most N time to solve. Reference: Introduction to Algorithm Cha 4.1123456789101112131415161718192021222324252627282930313233343536373839404142class Solution: def maxSubArray(self, nums): """ :type nums: List[int] :type k: int :rtype: float """ left = 0 right = len(nums)-1 #index of the last item # if right == 0: # return None if right == left: #only one element return nums[0] mid = int(len(nums) / 2) # print("mid",mid, left, right) # print(self.max_cross(nums), # self.maxSubArray(nums[left:mid]), self.maxSubArray(nums[mid:right]),) m = self.max_cross(nums) l = self.maxSubArray(nums[left:mid]) r = self.maxSubArray(nums[mid:right+1]) return max(m,l,r) # return r def max_cross(self, nums): mid = int(len(nums)/2) left = 0 right = len(nums)-1 left_sum_max, right_sum_max = float("-inf"), float("-inf") left_sum, right_sum = 0,0 left_index, right_index = -1,-1 for i in range(mid-1,left-1,-1): left_sum += nums[i] if left_sum&gt;left_sum_max: left_index = i left_sum_max = left_sum for i in range(mid,right+1): right_sum += nums[i] if right_sum&gt;right_sum_max: right_index = i right_sum_max = right_sum return left_sum_max+right_sum_max]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
</search>
