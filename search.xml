<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Relational_inductive_biases]]></title>
    <url>%2F2019%2F03%2F07%2FRelational-inductive-biases%2F</url>
    <content type="text"><![CDATA[Fully connected network imposes weak relational inductive biases which is that each output is able to interact with input. CNN impose locality and translation invariance relational biases. RNN could also be regarded as a kind of CNN because the update rules are shared across all time steps. The difference is that there are Markov dependence between hidden states which is that update of \(h_t\) depends on \(h_{t-1}\) and x. This reflects the relational inductive bias of temporal invariance. sets and graphs: Invariance to ordering. Ordering is not always meaningful. There is not useful information in ordering of graphs. Graph Network relational inductive bias: invariant to permutation can express arbitrary relationships among entities per-node, per-edge functions are reused across all edges and nodes Generally, the step of a graph network : update each edge indivisually aggregate the edge information from the prospective of a single node update each node individually aggregate the edge information to be used in global attribute prediction aggregate the node information to be used in global attribute prediction Compute updated global attribute]]></content>
  </entry>
  <entry>
    <title><![CDATA[Information_Theory]]></title>
    <url>%2F2019%2F03%2F04%2FInformation-Theory%2F</url>
    <content type="text"><![CDATA[Entropy:number of bits required to encode message. Joint Entropy:$$H(X,Y) = -\sum_{x,y} p(x,y) \log p(x,y)$$ Conditional Entropy:$$H(Y|X) = -\sum_{x,y} p(x,y) \log p(y|x)$$ $$% &lt;![CDATA[\begin{equation}\begin{split}H(X,Y) &amp;= -\sum_{x,y} p(x,y) \log p(x,y) \&amp;= -\sum_{x,y} p(x,y) \log {p(x)p(y|x)} \&amp;= -\sum_{x,y} p(x,y) \log p(y|x) - \sum_{x,y} p(x,y) \log p(x) \&amp;= H(Y|X) - \sum_x p(x) \log p(x) \&amp;= H(Y|X) + H(X)\end{split}\end{equation} %]]&gt;$$ Self-information is used to quantify information in a way that less likely events should have higher information content and independent events should have additive information:$$I(x)=-\log P(x)$$ Shannon entropy$$H = - \sum_{i=1}^{N} p(x_i) \log p(x_i)$$Shannon entropy is the number of bits needed to identify a data sample drawn from the distribution. A distribution with more uncertainty, the more bits would be required to identify a data sample. KL divergence:$$D_{KL} (P || Q) = E_{x\sim P}\log\frac{P(x)}{Q(x)}=E_{x\sim P}[logP(x)-logQ(x)]$$ Note that data sample x is drawn from probability distribution P and P, Q are different probability distribution. Therefore \(D_{KL} (P || Q) \neq D_{KL} (Q || P)\). KL divergence is used to measure the amount of difference between two distributions. It could be regarded as a kind of distance except that the KL divergence is asymmetric. cross-entropy:$$H(P,Q)=-E_{x\sim P}logQ(x)$$Note: \(H(P,Q)=H(P)+D_{KL} (P || Q)\). Minimizing the cross-entropy with respect to Q is equivalent to minimizing the KL divergence, because Q does not participate in the omitted term. Mutual Information It is KL divergence of joint probability distribution and multiplication of two distributions. $$% &lt;![CDATA[\begin{equation}\begin{split}I(X, Y) &amp;= KL(p(x, y) || p(x)p(y)) \&amp;= -\sum p(x, y) \log {\frac {p(x)p(y)} {p(x, y)}}\end{split}\end{equation} %]]&gt;$$ $$% &lt;![CDATA[\begin{equation}\begin{split}I(X, Y) &amp;= -\sum p(x, y) \log {\frac {p(x)p(y)} {p(x, y)}} \&amp;= -\sum p(x, y) \log {\frac {p(x)}{p(x|y)}} \&amp;= H(X) - H(X|Y) \&amp;= I(X, Y) = H(Y) - H(Y|X)\end{split}\end{equation} %]]&gt;$$ Cross-entropy means how many bits are required to identify data samples drawn from distribution P using an approximate distribution Q. In the context of deep learning, assume the real distribution is P, and learnt distribution is Q, cross-entropy means the number of bits is required to identify a data samples drawn from real distribution P. The higher the entropy, more bits are required, the worse the learnt distribution and the model. Reference1.# 信息论基础 ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[Unsupervised_Learning]]></title>
    <url>%2F2019%2F03%2F03%2FUnsupervised-Learning%2F</url>
    <content type="text"><![CDATA[Kmeansshortage: suscepticle to noise and initialization choose of K perform poor on unbalanced data each data point belongs to only one cluster(hard clustering) Improvement: Choose K with sum squared error(SSE). Find the k that SSE decreases dramatically. Gap Statistic $$Gap(k)=E(logD_k)-logD_k$$where \(E(logD_k)\) is generated from Monte Carlo simulation. We generate the same number of data samples from uniform distribution and calculate the error. \(logD_k\) is the kmeans error. Therefore, the better the clustering, the large the Gap(k). This approach is automatic. Kmeans++Choose the first cluster center randomly, and choose other center as far as posssible. ISODATASuitable for massive amount of data. split the cluster when variance is high eliminate the cluster if number of data samples is small Expectation-Maximization Algorithm(EM)It’s used to find a set of parameters that max the probability of given data samples with latent variables. In the context of kmeans, the latent variable is the number of K. initialize \(\theta\) E step Evaluate \(Q_i(z_i)=p(z_i|x_i;\theta)\). Calculate the expectation of latent variable given data samples and parameters. M step Evaluate \(\hat \theta=argmax\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\) We employ the expectation of latent variables given data samples and parameters to substitute the real knowledge of latent variables. And do maximum likelihood to find the optimal parameter sets. $$% &lt;![CDATA[\begin{align}\sum_ilogp(x_i;\theta)&amp;=\sum_ilog\sum_{z_i}p(x_i,z_i;\theta)\&amp;=\sum_ilog\sum_{z_i}Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\&amp;\ge\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\\end{align} %]]&gt;$$where greater than and equal to results from jensen inequality. Because \(Q_i(z_i)\) is a probability distribution Q() over latent variable z, \(Q_i(z_i)\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\) could be regarded as a weighted summarization of term \(\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\). Therefore the inequality is derived from \({\varphi \left({\frac {\sum a_{i}x_{i}}{\sum a_{i}}}\right)\leq {\frac {\sum a_{i}\varphi (x_{i})}{\sum a_{i}}}\qquad \qquad}\). $$% &lt;![CDATA[\begin{align}Q_i(z_i)&amp;=\frac{p(x_i,z_i;\theta)}{\sum_zp(x_i,z;\theta)}\&amp;=\frac{p(x_i,z_i;\theta)}{p(x_i;\theta)}\&amp;=p(z_i|x_i;\theta)\\end{align} %]]&gt;$$where \(Q_i(z_i)\) is a probability distribution Q() over latent variable z. Convergence of EM Algorithm:Well, suppose \(\theta^{(t)}\) and \(\theta^{(t+1)}\) are the parameters from two successive iterations of EM.$$% &lt;![CDATA[\begin{align}L(\theta^{(t)})&amp;=\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta^{(t)})}{Q_i(z_i)}\end{align} %]]&gt;$$ The parameters \(\theta^{(t+1)}\) are then obtained by maximizing the right hand side of the equation above. $$% &lt;![CDATA[\begin{align}L(\theta^{(t+1)})&amp;\ge\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta^{(t+1)})}{Q_i(z_i)}\&amp;\ge \sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta^{(t)})}{Q_i(z_i)}\=L(\theta^{(t)})\end{align} %]]&gt;$$This first inequality comes from the fact that \(% &lt;![CDATA[\begin{align}L(\theta^{(t)})&amp;\ge \sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta^{(t)})}{Q_i(z_i)}\end{align} %]]&gt;\) holds for any values of \(Q_i\) and \(\theta\), and in particular holds for \(Q_i\) = \(Q_i^{(t)}\), \(\theta=\theta^{(t+1)}\). The second equation is derived from \(\theta^{(t+1)}=argmax\sum_i\sum_{z_i}Q_i(z_i)log\frac{p(x_i,z_i;\theta)}{Q_i(z_i)}\). Reference1.CS229 Lecture notes ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[KBQA]]></title>
    <url>%2F2019%2F03%2F01%2FKBQA%2F</url>
    <content type="text"><![CDATA[Dynamic Memory NetworkInput: if the input is a paragraph which is a sequence of sentences, the sentences are concat together with [sep] separating each sentence. The final resentation of the whole paragraph is concatenation of a set of [eos] representations. If there is only one sentence, the [eos] representation is employed to represent the whole sentence. The episodic memory unit performs multiple rounds of update to enable iterative focusing of important knowledge. TransETurn entities and relationships to embeddings. Its intuition is to maximize the loss that between positive examples and negative examples like negative sampling. The negative samples are get by replacing head or tail randomly(not simultaneously). Reference1.Ask Me Anything: Dynamic Memory Networks for Natural Language Processing ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[Optimization]]></title>
    <url>%2F2019%2F02%2F27%2FOptimization%2F</url>
    <content type="text"><![CDATA[To alliviate the gradient exploding problem, there are two methods: gradient clip. If gradient is larger than a threshold, the gradient is set to a contant value weight decay(L1, L2 regularization)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Searching]]></title>
    <url>%2F2019%2F02%2F24%2FSearching%2F</url>
    <content type="text"><![CDATA[lexical matchinglexical matching: search engine matching terms in documents with those in a search query, sort according to similarity.[1] shortage: susceptical to different language structure and words Below is improvement. Semantic modelingEarly approaches: LSA, LDA, PLSA. LSA extracts abstract semantic content using SVD Recent improvements: Go deeper: e.g., semantic hashing (Hinton and Salakhutdinov 2011) Go beyond documents: e.g., using click signals (Gao et al. 2010; Gao et al. 2011 State of the art document ranking approaches that use models trained on clickthrough data. Oriented PCA (Diamantaras et al., 1996) Word Translation Model (Gao et al. 2010) Bilingual Topic Model (Gao et al. 2011) Discriminative Projection Model (Yih et al. 2011; Gao et al. 2011) Deep learning approach:Train an auto encoder to learn internal representations through minimizing reconstruction error. During application or testing, input a query and a document and calculate the similarity directly. Like word vectors, project the word to a space and calculate the distance in that space to measure similarity. Two shortages: learning objective problem: Model is trained by reconstructing the document, not for relevance measure Scalability problem: Model size increases rapidly along the vocabulary size DSSM Tri-letter hashing to reduce the vocabulary size less susceptical to misspelling, inflection. Each query and document are represented as a feature vector and propagated to a neural network to generate new feature vectors. Compute the cos similarity between query and document feature vectors and feed them to the softmax and max the probability of the clicked document and query. Sentence SimilarityopenAI-GPT: use transformer to encode two sentences. Two sentences are concat together. There is no natural structure for two sentences. So sen1[sep]sen2 and sen2[sep]sen1 are fed to transformer to compute feature vectors and added together. And then feed to a linear layer to compute similarity. Sentence EmbeddingTwo kinds of training method: generation: pair sentences together, generate sentences given other sentence classification: pair sentences together, do classification, whether a sentence is the next sentence of given sentence. Reference1.Learning deep structured Semantic models for web search using clickthrough data ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[DL_tricks]]></title>
    <url>%2F2019%2F02%2F20%2FDL-tricks%2F</url>
    <content type="text"><![CDATA[Weight Decay(L2 regularization)Weight decay is equivalent to decrease the weight during training each time updating the parameters. Batch NormalizationBatch Normalization not reduce internal covariance shift(ICS), but make the optimization landscape smoother so that the gradient and loss are not that bumpy. BN is performed after affine functions and before activation function. $$\mu_B = \frac{1}{B} \sum_i^B \mu_i$$ $$\sigma_B^2 = \frac{1}{B - 1} \sum_i^B \sigma^2_i$$ $$\hat{X} = \frac{X - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$ $$y^{(k)} = \gamma^{k}\hat{x}^{(k)}+\beta^{(k)}$$ As you can see, the effect of batch normalisation is gone when \(\beta = \mu_B,\gamma = \sigma_B^2\). BN is performed as a layer on each neuron. For CNN, each kernel is regarded as a neuron. So the mean and variance is computed from the batch with size h*w as the kernel size. For example, in fully connected network, BN if performed on each neuron so that the computation is performed on a batch of data \(x_1, ….., x_{m-1}\) with size m. When it comes to CNN, the computation if performed on a batch of data \(x_1,….., x_{m\times p\times q-1}\) where p and q is the kernel size. The BN operation is performed within a batch of data across every data samples at different locations in a kernel. During inference, the bias and variance are computed from the mean of all batches during training.$$E[x] = E_{\beta}[\mu_\beta]$$ $$Var[x] = \frac{m}{m-1}E_\beta[\sigma^2_\beta]$$Here \(\beta\) means all batches during training. Layer NormalizationIn a standard RNN, there is a tendency for the average magnitude of the summed inputs to the recur- rent units to either grow or shrink at every time-step. Layer normalization make it invariant to it. Layer normalization performs normalization on each time step separately over all hidden units. It use inputs to the hidden units to compute \(\mu, \sigma\). It resembles batch normalization on CNN. The computation of a neural network is: $$\alpha_i^l={w_i^l}^Th^l$$ $$\quad h_i^{l+1}=f(\alpha_i^l+b_i^l)$$ where f() is an activation function. We present the computation of batch normalisation and rewrite it for comparison purpose. $$\bar{a}_i^l = g_i^{l}\frac{a_i^l - \mu_i^l}{\sigma_i^l}$$ $$\mu_i^l = \mathbb{E}[a_i^l]$$ $$\sigma_i^l = \sqrt{\mathbb{E}[(a_i^l - \mu_i^l)^2]}$$ where g is a normalisation term such that the neural network is able to recover the original distribution. The computation of layer normalisation: $$\mu^l = \frac{1}{H}\sum_{i = 1}^H a_i^l$$ $$\sigma^l = \sqrt{\frac{1}{H} \sum_{i = 1}^H (a_i^l - \mu^l)^2}$$ From above functions, you can see that the normalisation terms \(\mu \text{and} \sigma\) is computed from all the hidden units rather than a mini-batch and shared across all hidden units. With different training examples, \(\sum_{i = 1}^H a_i^l\) would be different so that the normalisation terms \(\mu\) and \(\sigma\) are also different. In the context of recurrent neural network, the equation is rewritten as: $$h^t=f[\frac{g}{\sigma^t}\odot (a^t-\mu^t+b)]$$ A standard recurrent neural network tends to ether grow or shrink the magnitude of input which leads to either exploding or vanishing gradient problem. Layer normalisation enables to model to be invariant to growth or shrink in magnitude of input. DropoutDuring training, each neuron is killed with probability p. During testing, each neuron is always presented but decrease the weight to pw. Dropout is a kind of ensemble learning because each time it train a different neural network and finally combine all of them together. Learning rate warmupDuring the first k epochs, employ small learning rate to avoid unstable gradient and parameter updates. From k+1 epoch, employ normal learning rate. constant warmup:$$\hat{\eta} = k\eta$$ TemperatureTemperature which is usually 1 is used to scale the logits that are input to softmax[1]. Reference1.Distilling the Knowledge in a Neural Network ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ensemble_Learning]]></title>
    <url>%2F2019%2F02%2F20%2FEnsemble-Learning%2F</url>
    <content type="text"><![CDATA[The base learners in ensemble learning are supposed to be good enough and diversified. But it is usually contradictive due to bias-variance trade-off. BaggingThe objective of bagging is to train multiple diversified base learners. As a result, training base learners with differenrt sub-dataset might be a good idea. In order to employ enough data to train each base learner, the dataset could have some replicated data. Bagging focus on reducing variance. It samples data samples from the original dataset and put them back. It could be parallelized. Around 63.2% data is used for training, the rest could be used for validation. Random Forest(RF)RF is just an implementation of bagging with decision tree as the base learner. A modification is that for each base learner, when splitting a node, it only choose a subset of all candidate attributes so as to induce randomness. As a result, RF converges faster than bagging. StackingStacking stacks multiple learners. For example, the first learner produces output according to the dataset. And the produced output is the input of the second learner. There could be multiple stacked learner. It is proned to overfitting. As a result, the dadaset of subsequent learner would be the validation dataset of the first learner. Increase diversity disturb data samples(bagging) disturb input attributes. create subspace for atrributes(RF) disturb output representation. Turn multi-class output to multiple binary classification output. disturb algorithm parameters. Bagging focus on reduce variance by permute training data and employ strong classifiers which are usually complex so that they have low variance and high bias while boosting focus on reduce bias by employ weak classifiers which are usually simple with high variance and low bias. That is why xgboost and GBDT are accurate even with less layers.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Graph_Neural_Networks]]></title>
    <url>%2F2019%2F02%2F15%2FGraph-Neural-Networks%2F</url>
    <content type="text"><![CDATA[Spatial-Temporal Graph: each node is associated with a feature vector that evolves over time. GCN classification: add a node that connects to all other nodes in the graph as a kind of aggregated information(global node). Run the GCN algorithm and perform classification according to the node. global pooling shallow model:each node is represented as a dense vector in the embedding matrix. A similarity function is used to map Node(u,v) to the embedding space such that$$similarity(u,v) = z_v^Tz_u$$ The key concept of GNN is how to aggregate neighborhood information. The computation in GNN is very intuitive. Each node is represented by its neighbors. This approach is the same as CBOW(word2vec) which employs a set of neighbor words to represent the center word. So features of a node is the aggregation of neighborhood information. GNN could be trained in an unsupervised manner. The key concept is that similar nodes are with similar embeddings. Model design: choose an approach to aggregate neighborhood information. define a loss function on embeddings training generate embeddings(possible to concatenate several layer embedding like ELMO) The parameters of aggregate function could be shared so as to reduce computation and generate to unseen nodes. The aggregate function could be weighted average(CNN), pooling or even RNN. Pinsage: scale the he training as well as inference of node embeddings to billions of nodesInnovations: On-the-fly graph convolutions(only part of network is used to do computation): Sample the neighborhood around a node and dynamically construct a computation graph Perform a localized graph convolution around a particular node Constructing convolutions via random walks pooling: select nodes that are more important Efficient MapReduce inference Attention mechanism enables the neural network to aggregate the information according to importance of related nodes. Knowledge graphs are heterogeneous networks.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Algorithms]]></title>
    <url>%2F2019%2F02%2F14%2FAlgorithms%2F</url>
    <content type="text"><![CDATA[Recursion:reduce a problem to one or more smaller problems and hand those problems to a black box.There is absolutely no need to care about the black box. It’s worthwhile to note that if there is impossible to simplify the problem, just solve it. And there must be an end for the recursion. heap: Heaplify should start from the end. BUILD-MAX-HEAP starts from the last second layer because the last layer has no children. If there are several same records and the records are exchanged during sorting, the sorting algorithm isn’t stable, otherwise it is stable bucket sort assumes data are uniformly distributed in [0,1) so that data are uniformly assigned to each bucket. create bucket assign data to buckets [0.1~0.2) to the 1 bucket and [0.2~0.3) to the 2 buckets sort data in each buckets merge Two sum:use hashmap to record indexes of values in the array and perform lookup. reduce time complexity to O(n) 海量数据处理 - 找出最大的n个数（top K问题)建小顶堆，如果一个数大于顶部的数，将改数代替顶部的数，进行heaplify]]></content>
  </entry>
  <entry>
    <title><![CDATA[data_warehouse]]></title>
    <url>%2F2019%2F02%2F13%2Fdata-warehouse%2F</url>
    <content type="text"><![CDATA[A data warehouse is where you store data from multiple data sources to be used for historical and trend analysis reporting. A data mart serves the same purpose but comprises only one subject area.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Machine_Learning]]></title>
    <url>%2F2019%2F01%2F18%2FMachine-Learning-1%2F</url>
    <content type="text"><![CDATA[Usually recall increase, precision decrease and vice versa. Reall measure how many examples are predicted to False if the examples are True. Precision measure how many examples are predicted to True if the examples are False. An ROC curve plots TPR vs. FPR at different classification thresholds. AUC stands for “Area under the ROC Curve.” Measures the area underneath the entire ROC curve. AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. AUCPR: area under predicsion and recall line Cross-validation is used to select model or parameters. In deep learning, it is 1-fold validation while cross-validation is k-fold validation and take the mean. Bias term is able to add information. For example, if an user likes movies a lot, then add a high bias and vice versa. If a movie is liked in general by most people, it also has a high bias. Bias-Variance tradeoffThe more complex the model, the higher the variance. For a complex model, if the data changes a bit, the model output is likely to change a lot correspondingly. A simple model with sparse connectivity rather than dense connectivity like fully-connect network is not that likely to change a lot if the data changes a bit due to sparse connectivity(please feel it intuitively). But a simple model tends to have high bias which is underfitting. In a nutshell, simple models tend to have high bias and low variance while complex models tend to have low bias(accurate) and high variance. This is so-called Bias-Variance tradeoff. KLXGBoostimportant parameters: eta:learning_rate max_depth: max_depth of tree lambda: prevent overfitting subsample: sample part of training data which prevent overfittingcolsample: subsample the columns, like random forest Induction of xgboost: greedy algorithm, choose the tree that reduce the loss as much as possible use taylor expansion to approximate the gradient calculate the optimal tree structure The xgboost is a kind of greedy algorithm because each time it add the best tree structure. It employs CART as base learner. The final output of CART are values. So the output of leaves of xgboost are actually values. It is the sum of all trees.$$\hat{y}i = \sum{k=1}^K f_k(x_i), f_k \in \mathcal{F}$$ $$w_j^\ast = -\frac{G_j}{H_j+\lambda}$$The above equation calculates the best tree structure from the first and second order gradients. It employ tailor expansion to approximate gradients so that all kinds of self-define loss functions could be supported. $$\text{obj}^\ast = -\frac{1}{2} \sum_{j=1}^T \frac{G_j^2}{H_j+\lambda} + \gamma T$$The above equation is used to measure how good is the current tree structure. $$Gain = \frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{H_L+H_R+\lambda}] - \gamma$$The above equation serves as a kind of pruning. Other tree methods usually put pruning out of the training process. other tricks that xgboost employ: subsample data: sample a portion of the whole training set to train a tree like bagging. subsample columns: sample some columns to train like random forest. parameters tuning: set depth to 3~5 number of trees to ~500 learning step \(\eta\) from 0.02~0.1 run the algorithm until overfitting if not overfitting, check the dataset. The feature engineering may not be enough. Otherwise, start pruning. Increase the \(\lambda, \gamma\) and so on. Decesion TreeThe reason that decision tree is a base classifier in boosting or bagging: unstable classifier. susceptical to training data the representation and generalization ability is easy to adjust by modifying the number of layer or leaves. easy to incorporate the weight of training samples in decision tree by adjusting training data rather than oversampling or undersampling. ID3ID3 use entropy and information gain to measure whether a split is good or not. It is a kind of greedy algorithm and tends to choose splits with a lot of features whose information gain would be greater. And this may lead to overfitting. $$Ent(D)=- \sum_{k=1}^{K} p_k\cdot log_{2}p_k$$ $$Gain(D, a)=Ent(D) - \sum_{v=1}^{V} \frac{|D^v|}{|D|}Ent(D^v)$$ C4.5It is a kind of modified ID3. It employs to be the criteria to avoid the shortage of ID3 which choose the split with more features. $$GainRatio(D,T)=\frac{Gain（D,T)}{IV(T)}$$ Classification and Regression tree(CART)CART is a binary tree. At each split, it would choose a feature and divide the dataset according to yes and no feature. It would choose the split with the least gini. GINI index is used to measure how good is the split. $$gini(T_{i})=1-\sum_{j=1}^{n} p_{j}^{2}$$ $$Gini_{split}(T)=\sum_{i=1}^{2}\frac{N_{i}}{N}gini(T_{i})$$ For regression tree, it separate the continuous feature using {x&gt;T} and {x&gt;T} where T is the threshold. The decision boundaries of the CART are staright lines. ComparisionID3 and C4.5 could only be applied to classification task while CART could be applied to both regression and classification task. ID3 and C4.5 rely on pruning. ID3 and C4.5 derive complex trees and features could not be reusing while CART derives binary trees and the features could be reusing. PruningPruning is significantly important in decision trees. Usually we would grow the DT fully and do post-pruning to avoid overfitting and a better DT with better generalization ability. Cost-Complexity Pruning(CCP)It is used in CART pruning.$$\alpha = \frac{R(T) - R(T_t)}{L(T_t)-1}$$where \(L(T_t)\) is the number of leaves in the subtree that is going to be pruned. \(R(T_t)\) and \(R(T)\) are the error rate of the tree. It is the number of samples that are classified wrong. It compares the error before and after pruning. Choose the pruning with the least error increased. The pruning could be performed on a separate pruning dataset or simply k-fold cross-validation. Reduced Error Pruning(REP)$$f(T)=-\sum_{t \in T}e(t)$$ $$f(T’)=-\sum_{t \in T’}e(t)$$where T is the original tree and T’ is the pruned tree and f() is the number of samples that are classified wrong. Its intuition is to prune the tree so that the error date decrease. If \(f(T) \leqslant f(T’)\), do pruning. Pessimistic Error Pruning(PEP)Top-down pruning. It bases on the training set to do pruning. The 1/2 is a bias to rectify the bias that the pruning is performed based on training set. $$E(T) = \sum_{t \in T}\frac{e(t)+1/2}{N(t)}$$ $$E(T’)=\sum_{t \in T, excep K}\frac{e(t)+1/2}{N(t)}$$where e(t) is the samples that are classified wrong in the subtree start from node t. N(t) is the number of samples in the subtree start from node t. Minimum Error Pruning(MEP)There are several definitions about how to calculate the error rate of pruning\(E_s\). Below is one kind of method. $$E_s=1-\frac{n_c+p_{\alpha c}m}{N+m}=\frac{N-n_c+(1-p_{\alpha c})m}{N+m}$$ $$E_{split-s}=\sum_{split=1}^M \frac{|S_{split}|}{|S|}*E_{split-s}$$ Compare the error rate of pruning and not pruning. If the pruning error rate\(E_s\) &lt; not pruning error rate, do the pruning. Linear ModelLinear regression:$$w^Tx$$ Linear classification:$$sign(w^Tx)$$if \(w^Tx\) &gt; 0, the label is one and vise versa. This would not output a probability which could be regarded as a kind of confidence. Logistic regression output a value that is confidence between [0,1] by squeeze \(w^Tx\) to sigmoid function. Why sigmoid in logistic regressionWe care about P(y=1|x). But real world data samples are only (x1, 1), (x2,-1)……]]></content>
  </entry>
  <entry>
    <title><![CDATA[shell]]></title>
    <url>%2F2019%2F01%2F13%2Fshell%2F</url>
    <content type="text"><![CDATA[注意，变量名和等号之间不能有空格. 引用变量需要在变量前加美元符号$12your_name="qinjx"echo $&#123;your_name&#125; 变量名外面的花括号是可选的，加不加都行，加花括号是为了帮助解释器识别变量的边界 以“#”开头的行就是注释，会被解释器忽略。 sh里没有多行注释，只能每一行加一个#号. 多行注释：把这一段要注释的代码用一对花括号括起来，定义成一个函数，没有地方调用这个函数，这块代码就不会执行，达到了和注释一样的效果。 字符串单引号字符串的限制： 单引号里的任何字符都会原样输出，单引号字符串中的变量是无效的 单引号字串中不能出现单引号（对单引号使用转义符后也不行） 双引号 双引号里可以有变量 双引号里可以出现转义字符]]></content>
  </entry>
  <entry>
    <title><![CDATA[Transfer_Learning]]></title>
    <url>%2F2019%2F01%2F03%2FTransfer-Learning%2F</url>
    <content type="text"><![CDATA[Transfer learning in NLP:Hypercolumns: Embeddings at different levels are used as features, concatenated either with the word embeddings or with the inputs at intermediate layers like ELMO. Unsupervised pre-training usually improves performance. And adding a language model as auxilliary task also improves performance. Semi-supervised task: perform unsupervised pre-training and perform supervised fine-tuning. steps: freeze low layers and train the high layers like the last few layers unfreeze all layers and train with discriminative learning rates which means that training low layers with low learning rate and high layer with high learning rate.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hive]]></title>
    <url>%2F2018%2F12%2F24%2FHive%2F</url>
    <content type="text"><![CDATA[Hive is a data warehousing infrastructure based on Apache Hadoop. It provides SQL for ad-hoc querying. Hive data is organized into: Databases. Tables. Partitions. Buckets. Hive supports Primitive Types. Implicit conversion is allowed for types from child to an ancestor. Complex Types is built up from primitive types. Structs. Maps (key-value tuples). Arrays (indexable lists). Case-insensitiveAll Hive keywords are case-insensitive, including the names of Hive operators and functions. MAPJOINs are processed by loading the smaller table into an in-memory hash map and matching keys with the larger table as they are streamed through. UNION 操作符用于合并两个或多个 SELECT 语句的结果。 请注意，UNION 内部的 SELECT 语句必须拥有相同数量的列。列也必须拥有相似的数据类型。同时，每条 SELECT 语句中的列的顺序必须相同。 JOIN: 如果表中有至少一个匹配，则返回行 LEFT JOIN: 即使右表中没有匹配，也从左表返回所有的行 RIGHT JOIN: 即使左表中没有匹配，也从右表返回所有的行 FULL JOIN: 只要其中一个表中存在匹配，就返回行 ${precision}美元符号$引用变量，大括号{}表示变量名字。${precision}表示引用名为precision的变量。 Reference Hive Tutorial]]></content>
  </entry>
  <entry>
    <title><![CDATA[pandas]]></title>
    <url>%2F2018%2F12%2F21%2Fpandas%2F</url>
    <content type="text"><![CDATA[1pandas.cut(x, bins) turn continuous features to categorical features.]]></content>
  </entry>
  <entry>
    <title><![CDATA[tf_visualization]]></title>
    <url>%2F2018%2F12%2F19%2Ftf_visualization%2F</url>
    <content type="text"><![CDATA[tf histrogram:x-axis represents the exact value. y-axis represents number of step. It represent frequency of a parameter.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Pytorch]]></title>
    <url>%2F2018%2F12%2F11%2FPytorch%2F</url>
    <content type="text"><![CDATA[reshape in pytorch: torch.view 1234x = torch.randn(4, 4)z = x.view(-1, 8) # the size -1 is inferred from other dimensionsz.size()### torch.Size([2, 8]) torch.view() doesn’t change the data. torch.resize() may change the data. torch.unsqueeze(input, dim, out=None) insert a dimension at specified dimension. Negative dimension means the dimension is counted backward. Converting a Torch Tensor to a NumPy Array 1b = a.numpy() Converting NumPy Array to Torch Tensor 1b = torch.from_numpy(a) Tensors can be moved onto any device or converted to another dtype using the .to method. 1x = x.to(device) torch.Tensor is the central class of the package. If you set its attribute .requires_grad as True, it starts to track all operations on it. When you finish your computation you can call .backward() and have all the gradients computed automatically. The gradient for this tensor will be accumulated into .grad attribute. 12with torch.no_grad(): y = x * 2 wrap the code in with no_grad() code block to prevent grad. To stop a tensor from tracking history, you can call .detach() to detach it from the computation history, and to prevent future computation from being tracked. To prevent tracking history (and using memory), you can also wrap the code block in with torch.no_grad():. This can be particularly helpful when evaluating a model because the model may have trainable parameters with requires_grad=True, but for which we don’t need the gradients. Each tensor has a .grad_fn attribute that references a Function that has created the Tensor Gradient in Pytorch is accumulated. use .zero_grad() to clear gradient. dtype=torch.long is for integers. Input to Pytorch must in batch. X with shape (N-sample,……) torch.nn only supports mini-batches. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample. For example, nn.Conv2d will take in a 4D Tensor of n Samples x n Channels x Height x Width. 1torchvision.datasets.ImageFolder(root=data_path,transform=torchvision.transforms.ToTensor()) Transform image dataset to tensors. Foldeds’ names are classes. Display an image in pytorch: transform the array transform to numpy array display 1234for im in images: new = transforms.ToPILImage() pilima = new(im) plt.imshow(np.array(pilima)) Images in pytorch are arranged as (N,C,H,W).Normal Images are (W,H,C). nn.Sequential is a wrapper for constructing neural network conviniently. Dimension specified as 0 but tensor has no dimensions. For CrossEntropyLoss, the target has to be one dimension tensor rather than just a value. 1torch.nn.utils.rnn.pack_padded_sequence() Pack the padded sequence. When calculating loss, we don’t want to calculate loss of token like . Pack the padded sequence would eliminate these tokens and calculate the loss. Example: 123456&gt;&gt;&gt; from torch.nn.utils.rnn import pack_sequence&gt;&gt;&gt; a = torch.tensor([1,2,3])&gt;&gt;&gt; b = torch.tensor([4,5])&gt;&gt;&gt; c = torch.tensor([6])&gt;&gt;&gt; pack_sequence([a, b, c])PackedSequence(data=tensor([ 1, 4, 6, 2, 5, 3]), batch_sizes=tensor([ 3, 2, 1])) 1torch.numel(input) return the total number of elements in the input tensor. 1torch.topk(_input_, _k_, _dim=None_, _largest=True_, _sorted=True_, _out=None) Returns the k largest elements of the given input tensor along a given dimension. RuntimeError: Expected object of scalar type Float but got scalar type Double for argument.change tensor to float with1tensor.to(torch.float) nn.CrossEntropyLoss(): The first argument is model output and the second argument is label. The label has to be shape of [batch_size]. 1torch.ne(_input_, _other_, _out=None_) perform element-wise computation between two tensors. If two elements are different, return 1,else 0. 1torch.tril(b, diagonal=0) return upper triangular matrix of a tensor. diagonal = 1 exclude more values. diagonal = -1 include more values. rnn batch_first=True doesn’t affect output hidden state. hidden states of shape (num_layers * num_directions, batch, hidden_size) ONNX does not support batch_first…… torch.reshape(), 这与 numpy.reshape 的功能类似。它大致相当于 tensor.contiguous().view() print model parameters: 123for name, param in model.named_parameters(): if param.requires_grad: print (name, param.data) From 0.4, Variable is equivalent to tensor. Pytorch prediction: each time predict an example, it would load all parameters to memory. If increase the batch size, it would reduce the times of loading parameters so as to increase prediction time. torch.matmul is basically the same as torch.bmm. Their broadcast behaviors are different. resize never copies memory.reshape always copies memory.view never copies memory. Permute is a multidimensional rotation saying somehow. It keeps the data ordering. View (which is another reshaping method) maps from one dimensionality to another sequentially reading data from the upper dimensions to the lower ones. So if you want fuse two dimensions into one, you have to apply it over contiguous dimensions or u will modify the data ordering]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scripts_python]]></title>
    <url>%2F2018%2F12%2F06%2Fsctipts_python%2F</url>
    <content type="text"><![CDATA[remove file from a directory12345678910import osimport datetimeimport shutildir_l = ['visual/train', 'visual/val']i = 0for dir_to_search in dir_l: list(map(lambda i: os.remove(os.path.join(dir_to_search, i)),os.listdir(dir_to_search)))# l = list(map(lambda i: os.path.join(dir_to_search, i),os.listdir(dir_to_search))) move newest files to another dir12345678910111213141516171819202122232425262728293031import osimport datetimeimport shutildir_l = ['summary/train', 'summary/val']i = 0for dir_to_search in dir_l: time_l = [] file_l = [] d = &#123;&#125;#dir_to_search = os.path.curdir for dirpath, dirnames, filenames in os.walk(dir_to_search): for file in filenames: curpath = os.path.join(dirpath, file) file_modified = datetime.datetime.fromtimestamp(os.path.getmtime(curpath))# print(file_modified)# print(file) time_l.append(file_modified) file_l.append(curpath) d[file_modified] = curpath time_l.sort() if i == 0: dst = 'visual/train' else: dst = 'visual/val' shutil.copyfile(d[time_l[-1]], os.path.join(dst, d[time_l[-1]].split('/')[-1] )) i += 1# '''delete'''# for i in range(len(time_l)-1):# os.remove(d[time_l[i]]) delect old files 123456789101112131415161718192021222324252627282930import osimport datetimedir_l = ['summary/train', 'summary/val']for dir_to_search in dir_l: time_l = [] file_l = [] d = &#123;&#125;#dir_to_search = os.path.curdir for dirpath, dirnames, filenames in os.walk(dir_to_search): for file in filenames: curpath = os.path.join(dirpath, file) file_modified = datetime.datetime.fromtimestamp(os.path.getmtime(curpath))# print(file_modified)# print(file) time_l.append(file_modified) file_l.append(curpath) d[file_modified] = curpath# print(time_l)# print(file_l) time_l.sort()# print(time_l) '''delete''' for i in range(len(time_l)-1): os.remove(d[time_l[i]])# print(d[time_l[-1]]) # if datetime.datetime.now() - file_modified &gt; datetime.timedelta(hours=24):# os.remove(curpath)]]></content>
      <tags>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Data Preprocessing]]></title>
    <url>%2F2018%2F12%2F04%2FData%20Preprocessing%2F</url>
    <content type="text"><![CDATA[sklearn.preprocessing.StandardScaler() transform the data to zero mean and unit variance. Features selection according to experience or something like that. Features processing. Features are in original data, some steps are necessary to preprocess the data and get corresponding features. Data cleaning. Data sampling and filtering. Not all data would be used to train a model. Then you need to sample the data points. Methods include random sampling and others. Filtering is to detect and remove outliers. Common methods are KNN, clustering. Features classification. Feature processing and analysis. Features classification: Low-level and high-level features. Low-level features are mainly original features like user ID, time, weather. High-level features are processed features like sentiment. Static features and dynamic features. Static features are stable or seldom changed like age, height. Dynamic features change frequently like distance, temperature. 0-1 features, one-hot(categorical) features, continuous features. Feature processing: features normalization features discretization missing values. Dimensionality reduction: Sometimes increasing the dimensionality of features is good for classification. Common methods to reduce dimensionality are PCA, LDA. Features selection: features generation. Evaluation. Features generation: Complete search. BFS, Branch and Bound. Heuristic. SFS, Sequential Forward Selection; SBS,Sequential Backward Selection; LRS,Plus-L Minus-R Selection. Random. RGSS, Random Generation plus Sequential Selection; SA, Simulated Annealing; GA, Genetic Algorithms. Feature normalization: if there is not feature normalization, some features with larger range like [0, +10000] compared to [0,3] would impose more influence on the model. During deployment, performance(real-time, low-delay) is signifiacnt. There would be tradeoff between the amount of data and performance(accuracy). A possible solution is to construct a tree for features like hierachical softmax so that finding a feature is more efficient. Reference# 机器学习中的数据清洗与特征处理综述]]></content>
  </entry>
  <entry>
    <title><![CDATA[Feature Cross]]></title>
    <url>%2F2018%2F12%2F02%2FFeature%20Cross%2F</url>
    <content type="text"><![CDATA[Feature cross is dot-product of two or more features. It transform linear features to non-linear features. A feature cross is a synthetic feature that encodes nonlinearity in the feature space by multiplying two or more input features together. $$x_3 = x_1x_2$$where \(x_3\) is a new feature and it is fed to the classifier as a feature. $$y = b + w_1x_1 + w_2x_2 + w_3x_3$$ Feature cross introduces non-linearity as well as richer features. Feature crossing is usually employed in categorical features rather than continuous features. Deep Cross Network models features cross as matrix multiplication. In the context of deep learning, sparse features are transformed to embeddings. Therefore dot-product of embeddings is actually a kind of feature crossing. It resembles SVM that apply a kernel to the original features. The original features are sparse features and the kernel is a look-up table. After that, sparse features are transformed to dense features. Deep cross also utilizes residual connection because it adds \(x_l\) after feature crossing. And multiple layers of feature crossing is able to model higher level of feature crossing like \([x_1,x_2……x_n]\). Detailed introduction of DCN is in Recommendation System.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Share_meeting]]></title>
    <url>%2F2018%2F11%2F26%2Fshare_meeting%2F</url>
    <content type="text"><![CDATA[MEMM CRF考虑全局信息而不是局部标记，但是计算量比较大，参数多，不容易部署 TransE algorTransH algorGAUSSIAN EMBEDDING]]></content>
  </entry>
  <entry>
    <title><![CDATA[Multi-task Learning(MTL)]]></title>
    <url>%2F2018%2F11%2F25%2FMulti-task%20Learning%2F</url>
    <content type="text"><![CDATA[In normal machine learning or deep learning task, we optimize a specific metric, which loses information from other tasks that may be desirable to improve the original task. This kind of information comes from related tasks. And this kind of shared representations may be helpful to our original task because it actually utilise much more data than the original task. Generally, as soon as you find yourself optimizing more than one loss function, you are effectively doing multi-task learning (in contrast to single-task learning). In those scenarios, it helps to think about what you are trying to do explicitly in terms of MTL and to draw insights from it. “MTL improves generalization by leveraging the domain-specific information contained in the training signals of related tasks”. Inductive BiasA model prefer some hypotheses over others. For instance, a common form of inductive bias is l1 regularization, which leads to a preference for sparse solutions. In the case of MTL, the inductive bias is introduced by other auxiliary tasks, which leads the model to prefer hypotheses that explain more than one task. This in general leads to better generalization ability. Hard parameter sharing(Multi-output)Multiple tasks share the same hidden layers. Each task has its own output. The risk of overfitting the shared parameters is an order N – where N is the number of tasks – smaller than overfitting the task-specific parameters. Soft parameter sharingEach task has its own model, parameters and output. There are constraints between parameters such that the parameters are similar. A sample constraint is L2 distance between parameters. Advantages of MTL Data Augmentation. Training multiple tasks together enables the model to employ more data in training. Data are noisy. MTL is able to regularize the model to learn efficient and effective representations that are desirable for multiple tasks. Eavesdropping. The model might be easier to learn some specific features from some specific dataset. MTL is able to help the model to learn easier from different tasks or dataset. Representation bias. MTL biases the model to prefer representations that other tasks also prefer. This will also help the model to generalize to new tasks as a hypothesis space that performs well for many tasks will also perform well for learning novel tasks as long as they are from the same environment Regularization. MTL acts as a regularizer by introducing an inductive bias. As such, it reduces the risk of overfitting as well as the Rademacher complexity of the model Technique to do MTL Block-sparse regularization. For related tasks, use regularization like L1 or other modified regularization to enforce the model learn sparse features which would be utilised to do inference for multiple tasks. Learn the relationships between tasks: clustering, KNN, Bayesian methods. In MTL for computer vision, approaches often share the convolutional layers, while learning task- specific fully-connected layers. Deep Relationship Networks add matrix priors on the fully connected layers. Auxiliary tasksUse Auxiliary tasks so that to achieve better result on the main task. Related task Adversarial task. Maximize the training error using a gradient reversal layer Hints. Predicting features as an Hints as an auxiliary task. MTL could be used to learn features that are not easy to learn. Predicting inputs. In some cases, some inputs might not be useful for the task but they might be able to guide the learning. Use them as output might help the task. Representation learning. employing a task that is known to enable a model to learn transferable representations(language modeling) There are many definitions about related tasks. use the same features to make a decision related tasks share a common optimal hypothesis class MTL in NLPadd a language modeling objective to the model that is trained jointly with the main task model. The scale of loss for different tasks are supposed to be same, otherwise task with large loss would dominate the backpropagation so that the neural network would bias to the worst task rather than learn a good representation for all tasks. Multi-Task Deep Neural Networks for Natural Language UnderstandingIt employs the same structure like bert to do pre-training. And then add a specific fine-tuning layer for each task and do fine-tuning together. For each task, the model would update the parameters including the task-specific layer and pre-training layer. Each eopch the model would be trained on all tasks separately which means their loss is not added together. Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and SemanticsIt proposes to employ the variance of outputs to be the weight of the combined loss. Core of MTL: what to share? All parameters?In conclusion, hard parameters sharing is still pervasive for neural-network based MTL. Recent advances on learning what to share, however, are promising. At the same time, our understanding of tasks – their similarity, relationship, hierarchy, and benefit for MTL – is still limited]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git]]></title>
    <url>%2F2018%2F11%2F23%2Fgit%2F</url>
    <content type="text"><![CDATA[1git checkout &lt;branch&gt; swtich to a branch. 1git add . add all to buffer 1git commit -m “comment” commit the change and add comment 1git push origin &lt;local_branch&gt;:&lt;remote_branch&gt; push local branch to remote branch. 1git pull update files from remote]]></content>
  </entry>
  <entry>
    <title><![CDATA[seq2seq_tensorflow]]></title>
    <url>%2F2018%2F11%2F23%2Fseq2seq_tensorflow%2F</url>
    <content type="text"><![CDATA[attention machanism is just an attention wrapper to wrap the RNN. Dropout is also a wrapper to wrap RNN. During training, keep_prob = p. During testing and predict, keep_prob = 1, No dropout. The output of RNN has to be fed to a fully-connected neural network to convert the final hidden states to scores of the vocabulary. After that, employ a softmax function to squeeze all scores in a probability distribution. Process of constructing seq2seq model in tensorflow: define hyperparameters. Settings of hyperparameters could be loaded from config file or parsed from command line. Both parser and tf.app.flags could be used to parse. preprocessing data. Like batching, padding and so on. construct model. build encoder, decoder and combine them together. write a train wrapper funtion to wrap the training process. feed data to model and perform training perform validation and testing Loss of seq2seq: at each step, there is only one correct token given a sentence. At each time step, use softmax to calculate the loss and optimize the sum of total loss at all time steps. Given a sentence, each time step t, there is a loss \(l_t\). The total loss \(L=\sum_0^T l_i\) where T is the number of tokens in the sentence. The optimizer is actually optimize this loss. The seq2seq could be regarded as a latent structure like auto-encoder because all information of the whole sentence is encoded in the final hidden state. Latent variable: 隐藏变量 To do: write an iterator For regression(time series data prediction) problem, just add a scaling factor with a fully-connected layer. The decoder could use the last encoder hidden state or zero or encoder input[:-1] as initial state. The RNN decoder outputs tensor with same shape as the input. The input is batched tensor, and RNN encoder takes sequence as input. Sequence is a list of tensor with shape [batch_size, dim]. The length of the sequence is the time step. When you use Dataset to train the model, during inference, there are several ways to feed the data: use dataset and feed label with idiot values create two meta graph with different input, and inference graph load parameters from the other. extract the input feature and feed with inference input. MAPE(Mean absolute percentage error):$${\displaystyle {\mbox{M}}={\frac {100\%}{n}}\sum {t=1}^{n}\left|{\frac {A{t}-F_{t}}{A_{t}}}\right|,}$$Loss of each sample is the difference divided by the sample itself. Embedding sizeAssume n is the number of categorical features, embedding size could be \(k\sqrt[4] n\) or \(\log_2 (n)\). Only normalize output but not normalize input, model diverges. There are 2 kinds of seq2seq: encoder hidden state is only fed to the first step of the decoder encoder hidden state is fed to each step of the decoder add noise to encoder is beneficial to performance]]></content>
  </entry>
  <entry>
    <title><![CDATA[tensorflow_1]]></title>
    <url>%2F2018%2F11%2F16%2Ftensorflow-1%2F</url>
    <content type="text"><![CDATA[tf.feature_column.bucketized_column divides coutinuous values into several ranges such that they are able to be represented as categorical features. 1tf.feature_column.categorical_column_with_vocabulary_file( key, vocabulary_file, vocabulary_size=None, num_oov_buckets=0, default_value=None, dtype=tf.string) maps each word in the vocabulary to an one-hot vector. tf.feature_column.categorical_column_with_hash_bucket maps features to specified number of features using hash(for example, mapping 1000 words to 100 embeddings means some words would share the same embedding.). In machine learning, this kind of hash often works well in practice. That’s because hash categories provide the model with some separation. The model can use additional features to further separate kitchenware from sports. tf.feature_column.crossed_column is able to combine features together. Somewhat counterintuitively, when creating feature crosses, you typically still should include the original (uncrossed) features in your model (as in the preceding code snippet). The independent latitude and longitude features help the model distinguish between examples where a hash collision has occurred in the crossed feature. As a rule of thumb,1embedding_dimensions = number_of_categories**0.25 1234categorical_column = ... # Create any categorical column# Represent the categorical column as an embedding column.# This means creating an embedding vector lookup table with one element for each category.embedding_column = tf.feature_column.embedding_column( categorical_column=categorical_column, dimension=embedding_dimensions) tf.train.Features is used to wrap features of input. tf.train.Example(features=features) is used to wrap examples. using TFRecord is more efficient. A parse function is used to parse a single example. RNNMost software could only process up to 120 tokens. Longer sequences are supposed to be truncated. The padded labels change the total loss, which affects the gradients.Two methods to alleviate this problem: Maintain a mask Maintain a mask (True for real, False for padded tokens) Run your model on both the real/padded tokens (model will predict labels for the padded tokens as well) Only take into account the loss caused by the real elements 12full_loss = tf.nn.softmax_cross_entropy_with_logits(preds, labels)loss = tf.reduce_mean(tf.boolean_mask(full_loss, mask)) Let your model know the real sequence length so it only predict the labels for the real tokens.1234567cell = tf.nn.rnn_cell.GRUCell(hidden_size)rnn_cells = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)output, out_state = tf.nn.dynamic_rnn(cell, seq, length, initial_state) tensorflow seq2seq resembles caffe that use a file or string to define model parameters and perform training/inference. 1tf.contrib.rnn.MultiRNNCell( [lstm_cell() for _ in range(number_of_layers)]) is a RNN cell with a number of layers. In pytorch, you only need to input the number of layers. Sample code using tf.name_scope and tf.variable_scope.12345678910111213141516171819with tf.name_scope("Train"): train_input = PTBInput(config=config, data=train_data, name="TrainInput") with tf.variable_scope("Model", reuse=None, initializer=initializer): m = PTBModel(is_training=True, config=config, input_=train_input) tf.summary.scalar("Training Loss", m.cost) tf.summary.scalar("Learning Rate", m.lr) with tf.name_scope("Valid"): valid_input = PTBInput(config=config, data=valid_data, name="ValidInput") with tf.variable_scope("Model", reuse=True, initializer=initializer): mvalid = PTBModel(is_training=False, config=config, input_=valid_input) tf.summary.scalar("Validation Loss", mvalid.cost) with tf.name_scope("Test"): test_input = PTBInput( config=eval_config, data=test_data, name="TestInput") with tf.variable_scope("Model", reuse=True, initializer=initializer): mtest = PTBModel(is_training=False, config=eval_config, input_=test_input) 12with tf.variable_scope(name, reuse=None, initializer=initializer, reuse=tf.AUTO_REUSE): pass The above code enable reusing of variables such that subgraphs would not be constructed. 1tf.identity( input, name=None) return a tensor that is identical with input tensor. 1tf.placeholder_with_default( input, shape, name=None) If there is no value fed to placeholder, the placeholder would take input value as input. The input is actually a default value. 1tf.math.sign( x, name=None) return a tensor to indicate whether x is greater than 0 or not. 1tf.math.reduce_max( input_tensor, axis=None, keepdims=None, name=None, reduction_indices=None, keep_dims=None) Computes the maximum of elements across dimensions of a tensor. Attention mechanism in tensorflow is to use an attention_wrapper to wrap the RNNcell. tf.layers.dense() apply a fully connected layer to given inputs and return result.tf.layers.Dense() return an abstract layer. 1tf.tile( input, multiples, name=None) create a tensor that repeats input multiples times. The multiples argument has to specify how many times you want to repeat for each dimension. 1tf.concat() shape 0 of tensors must be equal. [tf.nn.dynamic_rnn] is used for sequence with unknown length. 1tf.map_fn() The output should have the same shape as input. If input is a sequence, the output has to be a sequence.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Recommendation_System]]></title>
    <url>%2F2018%2F11%2F16%2FRecommendation-System-1%2F</url>
    <content type="text"><![CDATA[CTR(Click-Through-Rate). Recommendation system could be regarded as a search-ranking problem. First the system retrives a list of candidate items according to the a query. Second the system ranks the candidate items and return some items with high scores. Memorization is loosely defined as recommeding according to co-occurence of items or features. It could recommend items directly(some one buy a medical app after a sport app, then recommend a medical app if someone buy a sport app) or with data mining of those co-occurence(some one with features \(x_m\)buy a medical app after a sport app, then only recommend a medical app to an user who bought a sport app if his/her user profile matches \(x_m\)). Generalization is loosely defined as discovering of underlying features from existing co-occurence so that new feature combinations could be inferred from existing co-occurence. data for Web-scale recommender systems is mostly discrete and categorical, leading to a large and sparse feature space that is challenging for feature exploration. For continuous data, a common method is to divide the continuous range into several continuous range such that one-hot vector could be employed to represent a range. AUCAUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease. Wide&amp;DeepFactorization machine and DNN requires no or less feature engineering because it is able to learn dense and low-dimension embedding. But it would over-generate so that recommendation would be unrelevent. Wide refers to logistic regression. It is able to simply remember some rules. Wide&amp;Deep combines them together. For wide part, features are transformed to one-hot sparse feature and fed to a linear transformation. The wide part is actually a logistic regression. Here the feature transformation is actually a set of hand-craft feature combination. Feature transformation:$$\varnothing_k(x)=\prod^d_{i=1}x_i^{c_{ki}}$$where \(\varnothing_k(x)\) is feature transformations.For binary features, a cross-product transformation (e.g., “AND(gender=female, language=en)”) is 1 if and only if the constituent features (“gender=female” and “language=en”) are all 1, and 0 otherwise. Linear transformation:$$\mathbf{w}_{wide}^T[ \mathbf{x},\phi ( \mathbf{x} )]$$where \([ \mathbf{x},\phi ( \mathbf{x} )]\) is concatenation of original features and transformmed features. At last, values of wide part and deep part are added together and fed to a sigmoid function to do binary classification. The result is in [0,1] which is the probability that the user would click the recommended item.$$P(Y=1| \mathbf{x})=\sigma (\mathbf{w}{wide}^T[\mathbf{x},\phi ( \mathbf{x} )] + \mathbf{w}{deep}^Ta^{( l_f )}+b)$$ DeepFMFactorization Machines(FM)Naive version of feature cross is like below formula:$$y = w_0 + \sum_{i=1}^nw_ix_i + \sum_{i=1}^{n}\sum_{j=i+1}^nw_{ij}x_ix_j$$where \(x_i, x_j\) are features. But this kind of computation is expensive. Factorization Machines project the feature to vector space so as to reduce computation complexity. $$y = w_0 + \sum_{i=1}^nw_ix_i + \sum_{i=1}^{n}\sum_{j=i+1}^n&lt;V_i,V_j&gt; x_ix_j$$$$\sum_{i=1}^{n}\sum_{j=i+1}^n&lt;V_i,V_j&gt; x_ix_j$$$$=\frac{1}{2}\sum_{i=1}^{n}\sum_{j=1}^n&lt;V_i,V_j&gt; x_ix_j -\frac{1}{2}\sum_{i=1}^n&lt;V_i,V_i&gt;x_ix_i$$$$=\frac{1}{2}(\sum_{i=1}^n\sum_{j=1}^{n}\sum_{f=1}^kv_{if}v_{jf}x_ix_j - \sum_{i=1}^{n}\sum_{f=1}^kv_{if}v_{if}x_ix_i)$$$$=\frac{1}{2}\sum_{f=1}^{k}((\sum_{i=1}^nv_{if}x_i)(\sum_{j=1}^nv_{jf}x_j) - \sum_{i=1}^nv_{if}^2x_i^2)$$$$=\frac{1}{2}\sum_{f=1}^{k}((\sum_{i=1}^nv_{if}x_i)^2 - \sum_{i=1}^nv_{if}^2x_i^2))$$ The above deduction reduces the computation complexity from \(O(kn^2)\) to \(O(kn)\)[1] where k is the dimensionality of vectors. \(\sum_{i=1}^{n}\sum_{j=i+1}^n&lt;V_i,V_j&gt; x_ix_j\) models feature interaction and \(&lt;V_i,V_j&gt;\),which are the vectors of the original input, serve as the weight matrix \(w_{ij}\) in the original equation becasue they are also parameters. As a result, the feature interaction is actually the multiplication of the original features with the learnt parameters. Field-aware Factorization Machines(FFM)Each latent vector is associated with several field which means other kinds of knowledge.$$y = w_0 + \sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n&lt;V_{i,f_j},V_{j,f_i}&gt;x_ix_j$$ For example, \(\phi_{FM}（V,x） = &lt;V_{ESPN},V_{Nike}&gt;+&lt;V_{ESPN},V_{Male}&gt;+&lt;V_{Nike},V_{Male}&gt;\) is the feature cross of FM while \(\phi_{FFM}（V,x） = &lt;V_{ESPN,A},V_{Nike,P}&gt;+&lt;V_{ESPN,G},V_{Male,P}&gt;+&lt;V_{Nike,G},V_{Male,A}&gt;\) is the feature cross of FFM. DeepFM substitutes the wide part in wide&amp;deep with Factorization Machines(FM). It is able to model high-level feature combination by transforming the sparse feature to dense and low-dimension features. It models feature interaction as inner product of embeddings. A feature \(x_i\) is associated with a weight vector(embedding) \(v_i\) and feature interaction or feature combination is computed as \(&lt;v_i,v_j&gt;\). Intuitively, in stead of computing feature combination directly, it insert an embedding layer to the network so as to convert features to embeddings. Compuing the dot product of embedding to model feature combination. FM and FFM are low-rank models. They are able to capture low-order(first and second order) feature interactions. DNN are high-rank models that are able to capture high-order feature interactions. DeepFM combines low-rank and high-rank models together to capture both low-order and high-order feature interactions. Wide&amp;Deep requires manual feature engineering for wide part and DeepFM is an end-to-end model. DCN(Deep Cross Network)The features are usually sparse, it is unable to represent those sparse features with one-hot vector if the size of vocabulary is huge. Therefore embedding layer is employed to convert sparse features to dense features which are real-value vectors like Natural Language Processing. After that, those vectors are stacked or concatenated with other dense features. It also utilises matrix multiplication to implement feature combination.$$x_{l+1} = x_0 x_l^T w_l + b_l + x_l = f(x_l, w_l, b_l) + x_l$$ $$x_0 = [x^T_{embed,1}, . . . , x^T_{embed,k}, x^T_{dense}]$$ where \(x_0\) is the concatenation of embedding features and dense features and \(^T_{embed,k}\) means the embedding of the kth feature. \(f(x_l, w_l, b_l)\) is the feature cross between feature \(x_l\) and the original feature \(x_0\). After feature cross, the layer feature \(x_l\) is added to the next layer feature so that the feature cross \(f(x_l, w_l, b_l)\) is a kind of residual connection. The degress of cross features is growing with the number of layers. At last, outputs of the deep network and the cross network are also concatenated together and fed to the logistic regression. In a nutshell, the cross network models linear features interactions between high-order features and the original features while deep network models non-linear and only high-order features interactions. They are combined together at the end. The cross network could be regarded as a kind of generalization of FM. wide&amp;deep, deepFM and Deep&amp;Cross are multi-task learning. Real-time Personalization using Embeddings for Search Ranking at AirbnbAirbnb proposed to do real time personalization. They achieved this by train embeddings in a booking session. The approach is totally the same as word2vec except that they focus on predicting the final booking. As a result, in each booking session there are several sliding windows and they try to max the probability of predicting the final booking listing(here listing could be regarded as an item). Adapting Training for Congregated SearchThey added negative samples from the same location(e.g. Paris) so that the embeddings are able to capture location information. Cold startTo deal with cold start problem, they identified the most similar several items within a certain range and averaged them. User-type &amp; Listing-type EmbeddingsUser-type Embeddings are used to capture the long-time interest. But most user books only several or even one time. So there are far less training data. They proposed to employ rule-based mapping to map different used to the same used embedding so as to get enough training data. They did the same thing to Listing-type Embeddings. Given learnt embeddings, use cos similarities to do recommendation. Reference1.# CTR预估算法之FM, FFM, DeepFM及实践 ↩]]></content>
  </entry>
  <entry>
    <title><![CDATA[SVM_2]]></title>
    <url>%2F2018%2F10%2F09%2FSVM-2%2F</url>
    <content type="text"><![CDATA[$$margin={1\over \left|{\boldsymbol {w}}\right|} = \sqrt{w_1^2+w_2^2+…+w_n^2}=\left|{\boldsymbol {w}}\right|_{2}$$ optimal hyperplane: $$sign(w_1x_1+w_2x_2+…+w_nx_n+b)=sign(\boldsymbol {w}^Tx)$$ Quadratic Programmingminimize a (convex) quadratic function, subject to linear inequality constraints.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hyper_Para_Search]]></title>
    <url>%2F2018%2F10%2F05%2FHyper-Para-Search%2F</url>
    <content type="text"><![CDATA[Hyper-parameters SearchingRandom SearchIt is heavily used in deep learning. There are tons of parameters in deep learning. Grid search is computation-intensive because it tunes parameters one by one which change only a parameter and keep the others unchanged. Each time for each parameter, random search samples a value from normal distribution. It is always the case that some parameters like leanring rate are more important than other parameters. Random search could discover a pretty good setting of hyperparameters. For example, let’s assume there are only 3 parameters. The algorithm has been run for 9 times both random search and grid search. For random search, each parameter has been tested for 9 times. For grid search, each parameter has been tested for only 3 times because it keep the other two parameters unchanged when tuning a parameter. Advantages:efficient Grid SearchGrid search is to generate a list of candidate parameters for each parameter. Each time only changes a parameter and keep other parameters invariant. It is systematic but it is only appropriate for low dimension. When the dimension increase, the computation would also increase dramatically. It would waste computation resource because each time only a parameter is changed. Advantages:Systematic.Precise. Bayes OptimizationThe third one is Bayes Optimization. Bayes optimization is powerful but it is not widely employed because it is application-specific. There is not a good software to facilitate the coding process until now. Its general idea is to do some experiments on a few settings of parameters. After that choose the setting that has high variance to evaluate so as to achieve better performance. High variance means that the parameters has the potential to be better or worse. If the performance is degraded, it doesn’t matter. It resemble the greedy algorithm to some extend. This method assumes that the similar inputs gives similar outputs. It’s a kind of weak prior. Its intuition is that to evaluate those unknown but possibly better combinations of parameters rather than try all of them. Reference深度学习模型超参数搜索实用指南]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Technical_Analysis]]></title>
    <url>%2F2018%2F10%2F05%2FTechnical-Analysis%2F</url>
    <content type="text"><![CDATA[Price-based: Moving average lines are heavily used to smooth the short-term fluctuations in the price charts. It is regarded as a support line where the price would not go lower than it in an uptrend. Points where the longer-term moving average crosses below the short-term moving average reveals an emerging uptrend. Bollinger bands are constructed from n standard deviation of closing prices. The high and low bands are the prices that moving average plus or minus n standard deviation. The prices in the area between the high and low bands are considered as resonable fluctuation of prices. If the price go beyond the area, it reveals overbought or oversold which means that the price is likely to decrease and increase respectively in the near future. It is the time where contrarian strategy comes into play. It is basically a strategy to buy or sell when most investors sell or buy. Oscillators: Oscillators are a set of indicators that oscillate around a specific value or in a range. Oscillator charts could be employed to detect convergence or divergence between oscillator and market prices. Convergence reveals that the current trend would continue and divergence suggests the change of the current trend. Sample indicators: Relative Strength Index(RSI): the rations of increase in price to decrease in price in a period of time. High values indicates overbought and low values suggest oversold. Moving Average Convergence Divergence(MACD): It is constructed from two lines. The MACD line is the difference between two expoentially moving average lines while the signal line is the expoentially moving average of the MACD line. The MACD could be employed to show both convergence or divergence and overbought or oversold. A buy signal is gerated when the MACD line cross above the signal line. Williams %R:It indicates the relation between the closing price and the highest and lowest price in the previous days. It also reveals overbought or oversold. Accumulation/Distribution Line(AD):Calculated from Money Flow Multiplier and Money Flow Volume. It takes volume into consideration. Volume is a significant indicator in technical analysis. It basically measure the money flow of the underlying asset. A pending revesal trend is indicated when divergence occurs between price and AD. Bullish Crosses occur when AD crosses above zero. Non-Price-Based IndicatorsPut/call ratio Volatility Index]]></content>
  </entry>
  <entry>
    <title><![CDATA[BLEU]]></title>
    <url>%2F2018%2F10%2F04%2FBLEU%2F</url>
    <content type="text"><![CDATA[Bilingual Evaluation Understudy(BLEU)$$p_{n}=\frac{\sum_{c_{\in candidates}}\sum_{n-gram_{\in c}}Count_{clip}(n-gram)}{\sum_{c^{‘}{\in candidates}}\sum{n-gram^{‘}_{\in c^{‘}}}Count(n-gram^{‘})}$$ First count the n-gram match sentence by sentence. After that truncate the count which is to clip the count by maximum reference count. Finally add all the n-gram for all sentences together and divided by all the n-gram for all reference sentences. $$Count_{clip}=min(Count,Max_Ref_Count)$$ Max_Ref_Count is the largest count of a word in a single reference sentence. The machine could cheat by output short sentences. So penalty has to be added for short sentence. $$BP= \begin{cases}1 &amp; \text{if c&gt;r}\e^{1-r/c} &amp; {if}\ c \le r\end{cases}$$ If the length of the output sentence c is greater than the length of the reference, no penalty. The final version of BLEU: $$BLEU=BP\cdot exp(\sum_{n=1}^N w_n logP_n)$$ BLEU computes the geometric average of the modified n-gram precisions and penalized short sentences. Usually N = 4 and the weight \(w_n=\frac{1}{N}\)]]></content>
  </entry>
  <entry>
    <title><![CDATA[Evaluation_NLP]]></title>
    <url>%2F2018%2F10%2F04%2FEvaluation-NLP%2F</url>
    <content type="text"><![CDATA[Automatic Evaluation:Word Error Rate(WER)It is the Levenshtein distance between output and reference divided by the length of the reference. Dynamic programming is employed to compute the Levenshtein distance so as to determine the best alignment between the output and the reference. Deletion is a reference word aligned to nothing and insertion is a output word align to nothing. Substitution is word that dose not match the reference word. WER is actually edit distance normalized by length. $${\displaystyle {\mathit {WER}}={\frac {S+D+I}{N}}={\frac {S+D+I}{S+D+C}}}$$ Position-independent Error Rate(PER)It treats the reference and the output as bag of words so that words could be alighed directly without alighment of two sentences. It is definitely smaller than or equal to WER.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN_NLP]]></title>
    <url>%2F2018%2F10%2F03%2FCNN-NLP%2F</url>
    <content type="text"><![CDATA[CNN is good at capture semantic information in a contextual window due to local connectivity. It requires lots of data to train because it includes many parameters. Inability to model long-distance contextual information.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Math]]></title>
    <url>%2F2018%2F10%2F01%2FMath%2F</url>
    <content type="text"><![CDATA[Gamma Functionthe gamma function is an extension of the factorial function to real and complex numbers. If n is a positive integer $$\Gamma(n)=(n-1)!$$ For complex number z whose real part is greater than 0, the below integral, which is known as the Euler integral of the second kind, converges bsolutely. The gamma function is defined as: $${\displaystyle \Gamma (z)=\int _{0}^{\infty }{\frac {t^{z-1}}{\mathrm {e} ^{t}}}\,{\rm {d}}t}$$ This integral function is extended by analytic continuation to all complex numbers except the non-positive integers (where the function has simple poles), yielding the meromorphic function we call the gamma function. Properties$${\displaystyle \Gamma (x+1)=x\Gamma (x)}$$ Read part of complex number:\({\displaystyle \Re (z)}\) Imaginary part of complex number:\({\displaystyle \Im (z)}\) Beta FunctionDefined by the Euler integral of the first kind. $${\displaystyle \mathrm {\mathrm {B} } (x,y)=\int _{0}^{1}t^{x-1}(1-t)^{y-1}\,dt!}$$ where \({\displaystyle {\textrm {Re}}(x),{\textrm {Re}}(y)&gt;0\,}\). $${\displaystyle \mathrm {B} (x,y)={\dfrac {\Gamma (x)\,\Gamma (y)}{\Gamma (x+y)}}!}$$ when x,y are integers. the beta distribution is a family of continuous probability distributions defined on the interval [0, 1] parametrized by two positive shape parameters, denoted by \(\alpha\) and \(\beta\), that appear as exponents of the random variable and control the shape of the distribution. The beta distribution has been applied to model the behavior of random variables limited to intervals of finite length in a wide variety of disciplines. In Bayesian inference, the beta distribution is the conjugate prior probability distribution for the Bernoulli, binomial, negative binomial and geometric distributions. For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions. Conjugate prior distributionIf the posterior distributions \(p(\theta | x)\) are in the same probability distribution family as the prior probability distribution \(p(\theta)\), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function. For example, the Gaussian family is conjugate to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure that the posterior distribution is also Gaussian. This means that the Gaussian distribution is a conjugate prior for the likelihood that is also Gaussian. Advantage of Conjugate prior distribution:A conjugate prior is an algebraic convenience, giving a closed-form expression for the posterior; otherwise numerical integration may be necessary. Further, conjugate priors may give intuition, by more transparently showing how a likelihood function updates a prior distribution. The Dirichlet distribution of order k ≥ 2 with parameters \(\alpha_1, …, \alpha_K &gt; 0\) has a probability density function with respect to Lebesgue measure on the Euclidean space \(\mathbb {R}^{K−1}\) given by $${\displaystyle f(x_{1},\dots ,x_{K};\alpha _{1},\dots ,\alpha _{K})={\frac {1}{\mathrm {B} (\alpha )}}\prod {i=1}^{K}x{i}^{\alpha _{i}-1}}$$ where \(\sum_k\theta_k=1 \) and \(\theta_k&gt;0\) or in other words, \({x_k}_{k=1}^{k=K}\) belong to the standard K-1 simplex. Euclidean spaceEuclidean space’s points are vectors of real numbers. A Euclidean space is a real vector space on which is defined a fixedsymmetric bilinear form whose associated quadratic form is positive definite. The vector space itself will be denoted as a rule by L, and the fixed symmetricbilinear form will be denoted by (x, y). Such an expression is also called the innerproduct of the vectors x and y. A Euclidean space is a real vector space L in which to every pair of vectors xand y there corresponds a real number (x, y) such that the following conditions aresatisfied: \(\forall x1, x2, y ∈ L\) (1) \((x_1 + x_2, y) = (x_1, y) + (x_2, y) \ .\) (2) \((\alpha x, y) = \alpha(x, y) α\in \mathbb {R}.\) (3) \((x, y) = (y, x)\) (4) \((x, x) &gt; 0 \ \ \forall x = 0.\) Properties (1)–(3) show that the function (x, y) is a symmetric bilinear form onL, and in particular, that (0, y) = 0 for every vector \(y \in L\). It is only property (4) that expresses the specific character of a Euclidean space. The expression (x, x) is frequently denoted by \((x^2)\); it is called the scalar squareof the vector x. Thus property (4) implies that the quadratic form corresponding tothe bilinear form (x, y) is positive definite. SimplexIn geometry, a simplex is a generalization of the notion of a triangle or tetrahedron to arbitrary dimensions. Specifically, a k-simplex is a k-dimensional polytope which is the convex hull of its k + 1 vertices. More formally, suppose the k + 1 points \({\displaystyle u_{0},\dots ,u_{k}\in \mathbb {R} ^{k}}\)are affinely independent, which means \({\displaystyle u_{1}-u_{0},\dots ,u_{k}-u_{0}}\) u1−u0,…,uk−u0 are linearly independent. Then, the simplex determined by them is the set of points $${\displaystyle C=\left{\theta {0}u{0}+\dots +\theta {k}u{k}~{\bigg |}~\sum _{i=0}^{k}\theta _{i}=1{\mbox{ and }}\theta _{i}\geq 0{\mbox{ for all }}i\right}.}$$ Hilbert spaceA Hilbert space H is a real or complex inner product space that is also a complete metric space with respect to the distance function induced by the inner product. To say that H is a complex inner product space means that H is a complex vector space on which there is an inner product ⟨x,y⟩ associating a complex number to each pair of elements x, y of H that satisfies the following properties: The inner product of a pair of elements is equal to the complex conjugate of the inner product of the swapped elements: $${\displaystyle \langle y,x\rangle ={\overline {\langle x,y\rangle }}\,.}$$ The inner product is linear in its first argument. For all complex numbers a and b, $${\displaystyle \langle ax_{1}+bx_{2},y\rangle =a\langle x_{1},y\rangle +b\langle x_{2},y\rangle \,.}$$ The inner product of an element with itself is positive definite: $${\displaystyle \langle x,x\rangle \geq 0}$$ where the case of equality holds precisely when x = 0. The Completeness of H is expressed using a form of the Cauchy criterion for sequences in H: a pre-Hilbert space H is complete if every Cauchy sequence converges with respect to this norm to an element in the space. Completeness can be characterized by the following equivalent condition: if a series of vectors $${\displaystyle \sum {k=0}^{\infty }u{k}}$$ converges absolutely in the sense that $${\displaystyle \sum {k=0}^{\infty }|u{k}|&lt;\infty \,,}$$ then the series converges in H, in the sense that the partial sums converge to an element of H. 泰勒公式通过展开估算领域的值。机器学习中将损失函数进行展开，估算更新后的参数的损失。 Jensen’s inequality$${\varphi \left({\frac {\sum a_{i}x_{i}}{\sum a_{i}}}\right)\leq {\frac {\sum a_{i}\varphi (x_{i})}{\sum a_{i}}}\qquad \qquad}$$Equality holds if and only if \(x_{1}=x_{2}=\cdots =x_{n} or \varphi\) is linear.]]></content>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Probability]]></title>
    <url>%2F2018%2F09%2F30%2FProbability%2F</url>
    <content type="text"><![CDATA[Conditional Probability$$P(C,B|A) = P(C|B,A)\times P(B|A)$$ The Binomial Distribution$${\displaystyle f(k;n,p)=\Pr(X=k)={n \choose k}p^{k}(1-p)^{n-k}}$$ n trails, each trail produce one result, the result is that whether a certain event E happen or not. The event happens with probability p. The Binomial Distribution is denoted as Bin(n,p) The Multinomial Distribution$$P(x_1,…,x_k|n,p_1,..p_k) = Multi(x_1,…,x_k|n,p_1,..p_k) \= \dfrac{N!}{x_1!\cdots x_k!}p_i^{x_i}\=\dfrac{N!}{\prod_{i=1}^kx_i!}p_i^{x_i},\where\ \sum_i x_i=N,x_i&gt;0 \prod_{i=1}^d \mu_i^{m_i}$$ n trails, each trail also produce one result, the result is that one of K events \(E_j\) happen. Each event happen with probability \(\pi_j\), j=1,2,….k where \(\pi_1 + \pi_2 +…..+\pi_k=1\). \(X_k\) is number of trials in which \(E_k\) occurs. \(X_1+X_2+….+X_k=n\). Multinomial models the distribution of the histogram vector which indicates how many time each outcome was observed over N trials of experiments. The individual components of a multinomial random vector are binomial and have a binomial distribution, \(X_1\sim Bin(n,\pi_1)\). The notation is \(X\sim Mult(n,\pi)\). Poisson Distribution$${\displaystyle P(X=k)={\frac {e^{-\lambda }\lambda ^{k}}{k!}}}$$ Poisson distribution is the limit of binomial distribution where \(\lambda =np\). \(Bin(n,p) \sim possion(\lambda)\) when n is large and p is small. Poisson distribution is to model in a fixed time period or a fixed space, counts or events that occur randomly such as number of people buy something, number of goals in a round of match. \(\lambda \) is the parameter describing the rate, that is the mean of the distribution. In poission distribution, \(E(X) = Var(X) = \lambda\). Usually the variance is larger than \(\lambda\) because of overdispersion. Estimation of \(\lambda\): point estimation or interal estimation. Gaussian processA Gaussian process is a stochastic process such that each combination of random variables has a multivariate normal distribution. Hypothesis TestingTo judge whether a distribution match the hypothetic distribution according to samples from the real distribution. If the calculated probability is very small, we prone to refuse the hypothesis.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Topic_Model]]></title>
    <url>%2F2018%2F09%2F30%2FTopic-Model%2F</url>
    <content type="text"><![CDATA[LDA$$p(\theta,z,w|\alpha,\beta)=p(\theta|\alpha) \prod_{n=1}^N p(z_n|\theta)p(w_n|z_n,\beta)$$ where \(p(\theta|\alpha)\) is the estimation of parameters \(\theta\), \(p(z_n|\theta)\) is probability distribution of topics \(z_n\) given parameters \(\theta\), \(p(w_n|z_n,\beta)\) is probability distribution of words \(w_n\) over topics z. It is actually a bayes network, compute the probability distribution of topics first, and given the topics distribution, each word would belongs to one or more topics. Compute the probability distribution over words given topics distribution.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Clustering]]></title>
    <url>%2F2018%2F09%2F30%2FClustering%2F</url>
    <content type="text"><![CDATA[Two kinds of clustering: parametric: K-means non-parametric: Dense-based hree types of clustering: Hard Clustering. Each instance belongs to only one cluster Hierachical clustering. Nested clusters. Soft/Fuzzy Clustering DBSCANIn center-based approach, density is defined as: given a radius and a central point, the number of points in the circle including the central points. core points: Within an area that contains multiple points. The area has to satisfy some requirement with user specified parameters. For example, with radius(Eps) 5, if the number of points in the area exceeds like 7, then the core point is defined as a core point. border points: falls within the neighborhood of a core point. Border points could not be core points. noise points: not core points nor border points. Algorithm: label all points as core, border, or noise points. Eliminate noise points. Put an edge between all core points that are within Eps of each other. Merge each group of connected core points into a separate cluster. Assign each border point to one of the clusters of its associatedcore points. Selection of DBSCAN Parametersk-dist: the distance between the core point and the kth nearest point. plot the the k-dist graph and select the k as MinPts(min points in a cluster) and the dramatically change point as distance. Strength: resistant to noise and can handle clusters of arbitrary shapesand size Weakness: has trouble when the clusters have widely varying densities. has trouble with high-dimensional data because density is more difficult to definefor such data. DBSCAN can be expensive when the computation of nearest neighbors requires computing all pairwise proximities, as is usually the case for high-dimensional data. HDBSCAN Transform the space. Increase the distance between noise points and core points by mutual reachability distance. Build the minimum spanning tree. The points could be viewed as a graph. The points are vertices and weighted edges are mutual reachability distance. The longer the distance, the higher the weight. Build the cluster hierarchy. At beginning, each point is in the same cluster. After that, remove edges one by one according to the weights of edges in decreasing order.(remove long distence edge first) If there are ties, remove them all together. After removal, assign a new cluster label to a component if it still has at least one edge, else assign it a null label (“noise”). Extract the clusters ReferenceIntroduction to Data Mining Hierarchical density estimates for data clustering, visualization, and outlier detection]]></content>
  </entry>
  <entry>
    <title><![CDATA[TextRank]]></title>
    <url>%2F2018%2F09%2F29%2FTextRank%2F</url>
    <content type="text"><![CDATA[TextRank is actuaaly the same as PageRank. The only difference is that the unit in the PageRank is a webpage while the unit in the TextRank is word or sentence. Keyword ExtractionEach word is a vertex in the graph. Set up a context window. If two words co-occur in a context window, there is a link between those two words. The rest is the same as PageRank. Sentence ExtractionEach sentence is a vertex in the graph. The difference with keyword extraction is that now all sentences are considered connected with a weight. The weight is sentence similarity. The graph becomes a weighted graph which means there is a weight associated with each edge.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview]]></title>
    <url>%2F2018%2F09%2F20%2FInterview%2F</url>
    <content type="text"><![CDATA[Find the max n number:Like quick sort, find a random number. Divide the original array into two parts, one is less than the random number, the other is greater than the random number. Pick another random number in the greater part, iterate aforementioned steps until there are n numbers in the greater array.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Paper_review]]></title>
    <url>%2F2018%2F09%2F18%2FPaper-review%2F</url>
    <content type="text"><![CDATA[Bidirectional LSTM-CRF for sequence tagging: CRF utilize sentence-level information. In CRF input and output are connected directly. Bidirectional LSTM-CRF connect all output together with a transition matrix from one state to another state. A Deep reinforced model for abstractive summarization: Use reinforcement leanring to derive a policy. Some critirion such as BLEU could not back-propagate directly. Employing reinforcement leanring could alleviate this problem.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Writing]]></title>
    <url>%2F2018%2F09%2F18%2FWriting%2F</url>
    <content type="text"><![CDATA[Writting is to explain an idea clearly and convince others. Presentation basically is trying to tell a story. Is the sentence easy to understand?Is the sentence enjoyable and interesting to read? Principles of writing: Cut unnecessary words and phrases; learn to part with your words! Use the active voice (subject + verb + object) Write with verbs: use strong verbs, avoid turning verbs into nouns, and don’t bury the main verb! Cut unnecessary words Dead weight words and phrases Empty words and phrases Long words or phrases that could be short Unnecessary jargon and acronyms Repetitive words or phrases Adverbs Eliminate negatives Eliminate superfluous uses of “thereare/there is” Omit needless prepositions Use active voiceMore clearly.In method section, it’s fine to use passive voice.]]></content>
  </entry>
  <entry>
    <title><![CDATA[scrapy]]></title>
    <url>%2F2018%2F09%2F12%2Fscrapy%2F</url>
    <content type="text"><![CDATA[12class StatSpider(scrapy.Spider): name = "stat_chart" name is used to identify the spider name.]]></content>
      <tags>
        <tag>Scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux_Management]]></title>
    <url>%2F2018%2F09%2F06%2FLinux-Management%2F</url>
    <content type="text"><![CDATA[remotely run sth after disconnection 1nohup command ...... check runing process 1ps -ef check runing process and find a specific process 1ps -ef | grep python kill a process 1kill pid count number of files 1find . -type f | wc -l 1stat print information like creation time of a file install trash packages1sudo apt-get install trash-cli 1trash-put somefiles move files to trash 1restore-trash restore files from trash print environmental variable:12printenv abcprintenv | grep &quot;.*JAVA.*&quot; “printenv abc” print the variable named abc“printenv | grep “.JAVA.“” print variables whose name match the pattern .JAVA. ~/.bash_profile 用户登录时被读取，其中包含的命令被执行 ~/.bashrc 启动新的shell时被读取，并执行 ~/.bash_logout shell 登录退出时被读取]]></content>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[QA]]></title>
    <url>%2F2018%2F09%2F05%2FQA%2F</url>
    <content type="text"><![CDATA[Two kinds of QA: IR-based question answering knowledge-based question answering Factoid questions: questions that can be answered with simple facts expressed in short text answers. IR-based question answering: Question Processing: extract focus, classify the question type Answer Type Detection(Question Classification) Query Formulation: form a query from keywords and send to an information retrieval system. Passage Retrieval: retrive documents and rank them according to features such as relevance, number of keywords and so on. Answer Processing pattern-extraction: use pattern to extract the answer from retrived document N-gram tiling]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM]]></title>
    <url>%2F2018%2F08%2F30%2FLSTM%2F</url>
    <content type="text"><![CDATA[LSTMForget gate, input gate and output gate controls the last hidden state, current input, and current output respectively. The cell state propagates the information through the network. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged. Forget gate controls how much the network forgets about the previous cell state. The output of forget gate do element-wise multiplication with the last cell state. $$f_t=\sigma(W_f\cdot [h_{t-1},x_t]+b_f)$$ The input gate controls how much the network utilize the current cell state to update the last cell state. The current cell state is also compute by the last hidden state and current input with just the different weight and bias matrix. $$i_t=\sigma(W_i\cdot [h_{t-1},x_t]+b_i)$$ $$\tilde{C_t}=tanh(W_C\cdot [h_{t-1},x_t]+b_C)$$ Update of the cell state: $$C_t=f_tC_{t-1}+i_t\tilde$$ The final output gate controls hou much the network outputs the current cell state after novation. The network puts the cell state through tanh (to push the values to be between −1 and 1) and multiply it by the output of the sigmoid gate, so that it only outputs the parts we decided to. $$o_t=\sigma(W_o\cdot [h_{t-1},x_t]+b_o)\ h_t = o_t*tanh(C_t)$$ Variant of LSTM$$C_t=f_tC_{t-1}+(1-f_t)\tilde$$ Utilize \(1-f_t\) to substitutes the input gate. GRUWith a single update gate to control whether the last hidden state or the current state is the output. A reset gate is used to control how much the past hidden units affect the current output. $$z_t=\sigma(W_z\cdot [h_{t-1},x_t]+b_z)\ r_t=\sigma(W_r\cdot [h_{t-1},x_r]+b_i)\ \tilde{h_t}=tanh(W\cdot [r_th_{t-1},x_t]) \ h_t = (1-z_t)h_{t-1}+z_t*\tilde{h_t}$$ Here reset gate resembles the forget gate in LSTM. It decides how much the last hidden state contributes to the current hidden state. And the final output hidden state is controlled by the update gate which resembles the ouput gate in LSTM. Simpler than LSTM. Peephole connectionThe value of gate is now controled by cell state, hidden state and current input. $$o_t=\sigma(W_o\cdot [C_t,h_{t-1},x_t]+b_o)\ i_t=\sigma(W_i\cdot [C_{t-1},h_{t-1},x_t]+b_i)\ _t=\sigma(W_f\cdot [C_{t-1},h_{t-1},x_t]+b_f)$$]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention]]></title>
    <url>%2F2018%2F08%2F27%2FAttention%2F</url>
    <content type="text"><![CDATA[Attention is a distribution which describe how much the network should focus on each unit. The decoder computes the probability of a sequence of words\({y_{1},…,y_{t}}\)$$\begin{equation}p(\textbf{y}) = \prod_{t=1}^{T}p(y_{t} \bracevert {y_{1},…,y_{t-1}},c)\end{equation}$$ For each \(y_t\),$$\begin{equation}p(y_{t} \bracevert {y_{1},…,y_{t-1}},c) = g(y_{t-1},s_{t},c)\end{equation}$$where \(s_t\) is the decoder hidden state,$$s_i=f(s_{i−1},y_{i−1},c_i)$$ \(y_{t-1}\) is the output of the last time step, \(h_j\) is the encoder hidden state at time step j. c is the context vector, which is the weighted sum of all encoder outputs.$$\begin{equation}c_{i} = \sum_{j=1}^{T_{x}}\alpha_{ij}h_{j}\end{equation}$$where \(a_{ij}\) is computed from$$\begin{equation}\alpha_{ij}= \frac{\exp(e_{ij})}{\sum_{k=1}^{T_{x}}\exp(e_{ik})}\end{equation}$$ Bahdanau et al. modelglobal and local attention: the “attention”is placed on all source positions or on only a fewsource positions. $$e_{ij} = a(s_{i−1}, h_j)$$a() is a feed forward neural network. Luong et al. models$$a_t(s) = aligh(h_t, \overline{h_s})=\frac{exp(score(h_t,\overline{h_s})}{\sum_{s’}score(h_t,\overline{h_{s’}})}$$where \(h_t\) is decoder hidden state and \(\overline{h_s}\) is encoder hidden state. This align model computes the scores between the current hidden state with each encoder hidden state and put it in a softmax. There are three kinds of score functions, please refer to the paper. After that, the context vector is computed as$$c_t = a_t \cdot \overline{h_{s}}$$a weighted sum of encoder hidden state. At the end, the attentional vector is computed as:$$\widetilde{h_t}=tanh(W_c[c_t;h_t])$$concat context vector and hidden state, then feed it to transformations. $$p(y_t|y_{&lt;t}, x)=softmax(W_s,\widetilde{h_t})$$ content-basedThe attending RNN generates a query describing what it wants to focus on. Each item is dot-producted with the query to produce a score, describing how well it matches the query. The scores are fed into a softmax to create the attention distribution. location-basedNeural Turing MachinesNeural Turing Machines combine a RNN with an external memory bank. Since vectors are the natural language of neural networks, the memory is an array of vectors. Every step, NTMs read and write everywhere, just to different extents. Read operation is a weighted sum of the distribution. Similarly, NTMs write everywhere at once to different extents. Again, an attention distribution describes how much NTMs write at every location. The new value of a position in memory is a convex combination of the old memory content and the new write value. Attention Mechanism The RNN controller gives a query vector. Each memory entry is scored for similarity with the query(dot product). The scores are then converted into a distribution using softmax. Interpolate the attention from the previous time step with the current distribution controlled by intepolation amount. Convolve the attention with a shift filter which allows the controller to move its focus. Sharpen the attention distribution. This final attention distribution is fed to the read or write operation. Attentional InterfacesBetween connection of RNN or (RNN,CNN), there would be attentional interfaces which decides which units in the previous layer should pay attention to. Attention is all you need(Transformer)The encoder is composed of N=6 identical layers. Each layer has two sub-layers AttentionAttention function could be described as a function to map query, key and values to a weighted sum. Attention is actually a query. The output is a weighted sum of values and each weight is the dot product of the query and the key. Scaled Dot-Product AttentionIn the paper, scaled dot-product attention is actually the attention function scaled by \(\frac{1}{\sqrt d_k}\) Mask operation is for masking the values in decoder outputs so that the model can only pay attention to leftward information/ Multi-Head AttentionLinearly project query, key, value with learnable weights. After that apply attention function to projected query, key, value. This is one head. MultiHead is concatenation of MultiHead and apply a linear transformation to the concatenation. Multi-Head Attention allows the model to jointly attend to information from different representation subspaces at different positions. Multi-head attention simulates convolutional neural network. Each head employs different weights such that different heads learn different relationship. In terms of convolution, different positions learn different features. Multi-head attention actually enrichs the features and enable a word focus on different parts of a sentence. Normal attention allows the word to focus on only a specific part. Each head focus on different kinds of information. For example, one head focus on subject, one head focus on object, so on and so forth[2]. Position-wise Feed-Forward NetworksIncrease or decrease the dimention of representation. Positional EmbeddingThe paper propose to use positional embedding to encode position information. Compare RNN with transformerself-attention Complexity \(O(n^2\cdot d)\) Because self-attention compute score according to, say encoder self-attention, encoder input(key) and encoder input(query). Each query has to do dot-product computation with every key. So the complexity is \(O(n^2\cdot d)\) where n is the sequence length and d is the dimension(n keys and n querys). RNN Complexity \(O(n\cdot d^2)\) RNN compute hidden states according to the previous hidden states and the current hidden states. There n times computation. So the complexity is \(O(n\cdot d^2)\) where n is the sequence length and d is the dimension(n times computation and each computation is d*d). During decoding, transformer would output BERT is ‘real’ bidirectional because self-attention mechanism utilize information from both sides. The traditional bi-LSTM just concat computation result from both directions. BERT performs fine-tuning for each indivisual task. Multi-Task Deep Neural Networks for Natural Language UnderstandingThis paper performs multi-task learning(MTL) for multiple tasks which is the main difference between it and BERT. Universal Transformeradd adaptive computation time(ACT) to the transformer so that the model is able to spent more computation on encoding the ambiguous words. Adaptive Computation TimeEmploy one activation function to decide the probability of continuing computation. If some steps are stopped earlier, their states are copied to the final output time step. This allow the neural network to spent more computation on those ambiguous words. Reference Attention is all you need [2] Self-Attention For Generative Models]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NN_Architecture]]></title>
    <url>%2F2018%2F08%2F25%2FNN-Architecture%2F</url>
    <content type="text"><![CDATA[GoogleNet Auxiliary classifier layer is to solve the gradient vanishing problem In the higher part of neural network, the filter would concentrate on detail or high-level feature of the images. As a result, the local region should large which is why the numbers of 3 3 and 5 5 filters have to be increased. 1 * 1 convolution: dimensionality reduction. If not, there would be many parameters. 3 3 and 5 5 is employed because with different stride and padding, their output dimension would be the same. Different convonlution kernel means different receptive field, fileter concatenation is concatenation of features. Residual NetworkLearn the mapping(original function) directly would be hard, denoted by degration of accuracy if layers are simply added. Learn the residual function would nbe much easier. If the mapping is worse, the network would learn to discard the learnt mapping and simply use the skip connection. The performance would be at least as good as the shallow network. ReferenceDeep Residual Learning for Image Recognition GoogLeNet系列解读]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[training_problem]]></title>
    <url>%2F2018%2F08%2F25%2Ftraining-problem%2F</url>
    <content type="text"><![CDATA[Cuda Out of MemorySolution: reduce batch size reduce model size(reduce number of activation, increase filter size, decrease filter number) For RNN, reduce the length Validation accuracy aigzagging Data is not normalized, taking some steps would deviate from the local minimum training step size is too large. NLPAssign high probability to and . ber:]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gradient_descent]]></title>
    <url>%2F2018%2F08%2F22%2Fgradient-descent%2F</url>
    <content type="text"><![CDATA[Vanilla gradient descentCalculate the gradient of the whole dataset and average the gradient by the size of the dataset. Drawback: Time-consuming, compute gradients redundantly. Mini-batch gradient descentTake a batch of examples and calculate the gradient. After that, average the gradient by the batch size. Stochastic gradient descentTake a single example and calculate the gradient. Drawback: High variance, fluctuate a lot.In high dimension, some directions would have larger gradients and zigzag a lot. The direction of gradient would deviate from the best direction. MomentumMomentum would accumulate gradient if the direction is the same and reduce the gradient if the direction zigzag a lot such that the overall direction would point to the local/global minimum with less zigzagging. $$v_t = \gamma v_{t-1} - \eta \nabla_\theta J( \theta)$$ $$\theta = \theta + v_t$$ \(\theta\) is the gradient and \(\gamma\) is the momentum which is usually 0.9. \(\eta\) is the learning rate. Nesterov accelerated gradientIt is a lookahead version of momentum. The gradient would be updated using momentum, and a estimate of the future gradient after normal momentum update\(\nabla_\theta J( \theta - \gamma v_{t-1} )\) would be added to the current update of gradient so as to accelerate convergence. It significantly increased the performance of RNNs. $$v_t = \gamma v_{t-1} - \eta \nabla_\theta J( \theta + \gamma v_{t-1} ) \\theta = \theta + v_t$$ AdagradIt adapts the learning rate to the parameters, performing larger updates for infrequent and smaller updates for frequentparameters. $$\theta_{t+1, i} = \theta_{t, i} - \dfrac{\eta}{\sqrt{G_{t, ii} + \epsilon}} \cdot g_{t, i}$$ \(G_{t} \in \mathbb{R}^{d \times d}\) is a diagonal matrix where each diagonal element i,i is the sum of the squares of the gradients w.r.t. \(\theta_i\) up to time step t, while \(\epsilon\) is a smoothing term that avoids division by zero (usually on the order of 1e−8). Interestingly, without the square root operation, the algorithm performs much worse. Drawback: The accumulated sum keeps growing during training and causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge. Adadelta$$\Delta \theta_t = - \dfrac{RMS[\Delta \theta]{t-1}}{RMS[g]{t}} g_{t} \\theta_{t+1} = \theta_t + \Delta \theta_t$$ RMSpropApproximate the learning rate with the RMS of parameter updates until the previous time step. $$E[g^2]t = 0.9 E[g^2]{t-1} + 0.1 g^2_t \\theta_{t+1} = \theta_{t} - \dfrac{\eta}{\sqrt{E[g^2]t + \epsilon}} g{t}$$ It still accumulates the gradient while it also depends on the current gradient and the updates do not get monotonically smaller.RMSProp still modulates the learning rate of each weight based on the magnitudes of its gradients, which has a beneficial equalizing effect. ReferenceAn overview of gradient descent optimizationalgorithms]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Network_in_Network]]></title>
    <url>%2F2018%2F08%2F18%2FNetwork-in-Network%2F</url>
    <content type="text"><![CDATA[]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[seq2seq]]></title>
    <url>%2F2018%2F08%2F11%2Fseq2seq%2F</url>
    <content type="text"><![CDATA[The input is a sequence and the output is also a sequence. Normal RNN take sequence or vector as input and output Fixed-length vector Sequence with the same length as the input sequence. seq2seq could output a sequence of variable length. It simply utilize an encoder to encode the information given an input, which output a representation C that is a semantic summary of the input(usually fixed-length). After that, the decoder take the representation as input and output a sequence. The decoder is a RNN language model that is conditioned on the input which is the current the previous input word and the hidden state. Reverse the input sequenceDoing so would make the optimization much easier because it introduces short term dependencies. i.e.: a,b,c maps to d,e,f. With reversed input sequence, c,b,a maps to d,e,f. At present, a is much closer than d which is the correct mapping. Since the long dependency c to f may be learnt from previous mapping, longer dependency for it may not be a problem. Encoding Context CThe context C that is used to encode the input sequence do capture the meaning of the sequence. LmitatoinThe context C might be to small to encode the semantic information of a long sentence. DecodingThe decoding could employ beam search. The procedure is actually to maximize the log probability of target sentence given the input sentence.]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CNN]]></title>
    <url>%2F2018%2F08%2F10%2FCNN%2F</url>
    <content type="text"><![CDATA[The convolution in CNN could be regarded as a kind of weighted average over neighbors. Prior parameter distributionA prior probability distribution over theparameters of a model is to encode our belief about what models are resonable before seeing the data. a week prior: distribution with high entropy like Gaussian with high variance. It allows the data to move parameters of model freely. a strong prior: distribution with low entropy like Gaussian with low variance. Such a prior plays a more active role in determining where the parameters end up. Infinitely Strong Prior An infinitely strong prior places zero probability on some parameters It says that some parameter values are forbidden regardless of support from data. Convolution as infinitely strong priorConvolution introduces an infinitely strong prior probability distribution over the parameters of a layer which says that the function the layer should learn contains only local interactions and is equivariant to translation. Pooling as an Infinitely strong priorThe use of pooling is an infinitely strong prior that each unit should be invariant to small translations. High Bias/Underfit can be countered by: Add hidden layers Increase hidden units/layer Decrease regularize parameter. Add features ReferenceDeep Learning Slides ConvolutionEach convolution is acutually a filter doing element-wise multiplication with a local region on all channels of the input. After that sum them up and add the bias. One filter one bias. Parameters SharingParameters in a kernel are used to do convolution in the whole image. In terms of fully connect, for each pixel, there would be a different parameters. Local ConnectivityThe neural network only contains regional connection which means the neurons in the next layer focus on part of neurons in the last layer. Pooling LayerThe employment of pooling layer is significant. It reduce the size of representation so as to reduce the computation and control overfitting. Max pooling could be regarded as extraction of the most useful characteristic. Pooling could be performed on spatial regions, or over the multi-layer outputs. For example, max pooling is performed on feature maps which is a spatial region while 1*1 convolution is performed on the multi-channel feature maps which is usually used to reduce dimension and compress information. When performed on spatial regions, it yields the same output dimension with smaller filter size. This would enable the neural network to be invariant to local small translations of the input, which is useful in case of whether a feature is present is more important than where the feature is. When performed on the multi-layer outputs, it yields the same filter size with fewer dimensionality. This would enable the neural network to be invariant to transformation such as rotation of images. In a nutshell, pooling summarises the response of activation over a region. It improves the computational and statistical efficiency. It is also used in hadling inputs of varying size. Memory Consumption CalculationActivation needs to be stored for back-propagation. So it would be the major memory consumption. i.e.: an image of 224 2243. The activation would be 224 2243=150000 floating numbers. Each floating number would use 4 bytes to store, so its size is actually 600000 bytes which is 600KB for only the first layer in the network. Convolution is a function derived from two given functions by integration that expresses how the shape of one is modified by the other. It is how an input is transformed by a kernel. Feature visualization have shown that: lower layers extract features related to content higher layers extract features related to style** ArchitectureResNetDenseNetDenseNet is a modified ResNet. Resnet employ add for x and f(x) while DenseNet employ concat for x and f(x) so that upper layers have each output from lower layers.]]></content>
  </entry>
  <entry>
    <title><![CDATA[RNN]]></title>
    <url>%2F2018%2F08%2F10%2FRNN%2F</url>
    <content type="text"><![CDATA[RNN allows information flow. Gradient vanishing or exploding:Since the time step 1 would have effect on time step n, when doing back propagation, according to the chain rule, the same weight matrix W would be multiplied by n times which is W to the power of n. So the norm of W is greater than 1, there is gradient exploding. And vice versa, gradient vanishing if the norm of W is smaller than 1. Solution to gradient vanishing:Initialize weight matrix to identity matrix rather than a matrix of random values. And use Relu as activation function. The intuition of this is to take average of input and hidden state. Then start to optimize the weight matrix. Attention ModelThe input includes all or some of the hidden states in the encoder. Utilize a score function to weight importance of those hidden states and feed into the decoder with the input word. Parameters SharingThe same set of parameters are used to compute the hidden states at the current time step given input and the hidden state of last time step. A output layer may be added to each time step to extract information and make predictions. Back-propagationCannot be parallelized because gradient has to computed one by one in terms of time steps. Recurrent Connectiong from output to hiddenTraining can be parallelized because the label is known and could be fed to hidden states directly. Trained with Teacher Forcing which is that given the input sequence and ground truth label in the last time step, produce the output in the current time step. Drawback: require the output to capture the past information, which is difficult because the output dosen’t contains those information. RNN as graphical modelRNN could be viewed as a graphical model. Compared to the graphical model, the number of parameters is far less than that of the graphical model due to parameter sharing. Basic assumption of RNN is that the sequence is stationary. stationary The relationship between the previous time step and the current time step is not dependent on t. RNN could model long dependency while normal graphical model would make Markov assumption to simplify the parameters which makes them less powerful in terms of long dependency. Long dependency makes RNN hard to optimize. RNN introduce hidden states h s.t. the dependency structure is simpler than the normal graphical model because hidden state encodes information including dependency which graphical model utilizes edges to represent dependency directly(too many parameters to optimize). Clip Gradient(Solution to gardient explosion)There are some cliffs which is with large gradient inside a place where surroundings are with small gradient. Due to the leanring rate is adapted, at that time, the learning rate may be large to accelerate learning s.t. at the cliffs the gradient would overshoot. Clip the gradient means if the norm of gradient is larger than a threshold, normalize the gradient. This would make the gradient a biased estimation of the gradient of the mini-batch. Larger gradient would contribute more the overall gradient. But the clipped gradient could solve the problem. OutputThe output of character level or word level RNN is the probability distribution of the next character or word given previous sequences of character or word. We sample from this distribution, and feed it right back in to get the next letter. Repeat this process and you’re sampling text! vector-to-sequence RNNThe vector could be fed into the RNN at the beginning or fed into each hidden state at each time step. Recursive Neural NetworkTree structure neural network. Its advantage is that its depth is far shallow than normal neural network with the same length of input. Challenge of Long-Term DependenciesWith long-term dependency, due to parameter sharing, the weight matrix W would be multiplied for several time which make the gradient exponentially larger or samller. Even the problem of vanishing gradient is solved, due to exponentially samller of gradient, it is very hard to train the network. Tanh is employed in LSTM because the candidate cell state could “reduce” information. If sigmoid is employed, the only thing you could do is to add information. ReferenceThe Unreasonable Effectiveness of Recurrent Neural Networks]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[activation_function]]></title>
    <url>%2F2018%2F08%2F09%2Factivation-function%2F</url>
    <content type="text"><![CDATA[Rectified Linear UnitsEasy to optimize, and train. It is a good practice to initialize the bias b to be small positive values s.t. most units are active at the beginning. Drawback: not able to learn examples that activation is zero. A large gradient could update the unit s.t. it would never be active again if the learning rate is large. Leaky ReLUMake some modifications s.t. the negative part could also be learnt. Maxout unitsGeneralization of ReLU and Leaky Relu. The activation function is now \(\max(w_1^T\cdot x, w_2^T\cdot x)\) where \(w_1^T\cdot x\) is Relu (when w1 are all zeros, it is Relu)and \(w_2^T\cdot x\) is Leaky Relu. SigmoidCould be used as output units for 0-1 classfication because the cost is optimized to be small to avoid saturation. Drawback: hard to train due to saturation. Not zero-centered. TanhA scaled sigmoid, zero-centered. Resemble the identity function]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SVM]]></title>
    <url>%2F2018%2F08%2F05%2FSVM%2F</url>
    <content type="text"><![CDATA[Derivation of SVM: write down the equation. $$\max \limits_{w,b} {1\over ||w||}$$ such that $$ {(w^Tx_i+b)*y_i}\ge1$$ Lagrange multipliers to solve the dual problem and get the answer of the primal problem if the problem satisfy KKT condition. $$\begin{cases}{\partial L \over \partial w}=0 &amp; \{\partial L \over \partial b}=0 &amp;\end{cases} \Longrightarrow\begin{cases}w=\sum \limits_{i=1}^m\alpha_ix_iy_i &amp; \0=\sum \limits_{i=1}^m\alpha_iy_i &amp;\end{cases}$$$$\begin{cases}w^=\sum \limits_{i=1}^m\alpha_i^x_iy_i &amp; \b^=y_j-\sum \limits_{i=1}^m\alpha_i^y_i(x_i\cdot x_j) &amp;\end{cases}$$ $$y_j(w^\cdot x_j+b^)-1=0$$ substitute the w with aforementioned equation and multiply \(y_j\) simultaneously.\(y_j^2=1\) Most \(\alpha_i\)=0, for those \(\alpha_j \neq 0 \), their training samples \(x_i\) are so called support vector. Slove the problem For problems that are not linearly separatable, add a slack variable. Above is so called linear SVM. Non-Linear SVMApply a kernel function K(x,z) to substitute the original inner product \(x_i\cdot x_j\). Affine the original variables in Euclidean space to another feature space to transfer the problem from non-linear to linear. Hinge LossAnything falls between the margin would contribute to the loss. Those classified wrong would contribute more than classfied correct but with low confidence level. It is the upper bound of 0-1 loss. Perceptron LossBetween 0-1 loss and hinge loss. The loss would be zero if classification is right. If the classification is wrong within the margin, the loss between 0 and 1. If the classification is wrong and outside the margin, the loss greater than 1. 0-1 LossThe loss is only 0 or 1 corresponds to wrong or correct classification. Not continuously differentiable. Reference统计学习方法]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CRF]]></title>
    <url>%2F2018%2F08%2F02%2FCRF%2F</url>
    <content type="text"><![CDATA[Conditional Random Field(CRF)It is a kind of generalized logistic regression model. HMMs have difficulty modeling overlapping, non-independent features because the assuption is indepence. In NLP, words are correlated and connection between them contains information. Here is where CRF comes into play. Linear chain CRF, can be thought of as the undirected graphical model version of HMM. It is as efficient as HMMs, where the sum-product algorithm and max-product algorithm still apply. Linear chain CRF is a special case of CRF, where each clique is the nearby two nodes. In terms of graph, the structure would be much more different. Overlapping features are in fact boost up the belief of the result, which should not be eliminated simply. CRF is conditional probability distribution while HMM is joint probability distribution. Formal Definition of CRF$$P(y\mid x) = \frac{1}{Z(x)} \prod_{c \in C} \phi_c(x_c,y_c)$$ where C denotes the set of cliques (i.e. fully connected subgraphs), \(\phi_c(x_c,y_c)\) is a factor. $$Z(x) = \sum_{y \in \mathcal{Y}} \prod_{c \in C} \phi_c(x_c,y_c)$$ Parameterized Form of CRF:$$P(y|x)=\frac{1}{Z(x)}\exp (\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i))$$ $$Z(x)=\sum_y \exp (\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_ls_l(y_i,x,i))$$ where \(\lambda_k, \mu_l\) are weights and \(t_k(y_{i-1},y_i,x,i)\) is the transition probability that given \(y_{i-1}\), the next label is \(y_{i}\) given input x at position i. \(\mu_ls_l(y_i,x,i)\) is the probability that at position i, given x, the output is \(y_{i}\). Inference of label y$$\arg \max_y \phi_1(y_1, x_1) \prod_{i=2}^n \phi(y_{i-1}, y_i) \phi(y_i, x_i)$$ In practice, \(\phi_c(x_c,y_c) = \exp(w_c^T f_c(x_c, y_c)).\) And \(f_c(x_c, y_c) = \lambda_kt_k(y_{i-1},y_i,x,i)+\mu_ls_l(y_i,x,i)\). This could also be written in vector form. The most important realization that need to be made about CRF features is that they can be arbitrarily complex. In fact, we may define CRF that depend on the entire input x. This will not affect computational performance at all, because at inference time, the x will be always observed. Reference统计学习方法 Probabilistic Graphical Models - Principles and Techniques cs228 notes]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word_Representation]]></title>
    <url>%2F2018%2F08%2F01%2FWord-Representation%2F</url>
    <content type="text"><![CDATA[The gradient of the whole word matrix(context matrix) should be outer product rather than normal inner product because their dimensions are not matched. Feedforward Neural Net Language Model (NNLM)Time complexity:Q = N × D + N × D × H + H × V, where the dominating term is H × V. N are the N input examples, D is the dimensionality of the word vectors, H is the hidden layer size, V is the vocab size. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V). Thus, most of the complexity is caused by the term N × D × H. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(Unigram perplexity(V)). Continuous Bag-of-Words Model(CBOW)Given context words, predict the central words. Average the input word vectors, which ignore the order of words, and then predict the output word. The training complexity of this architecture is proportional to Q = N × D + D × log2(V ). C is the context size. The order of words in the history and future does not influence the projection, like bag-of-words model. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM. Here the weight matrix are the word vectors. Continuous Skip-gram ModelSimilar to CBOW, in this model, input the central word and predict the context words, others are the same. This is a The training complexity of this architecture is proportional to Q = C × (D + D × log2(V )) where C is the maximum distance of the words. For each training word we will select randomly a number R in range &lt; 1; C &gt;, and then use R words from history and R words from the future of the context as correct labels. This will require us to do R × 2 word classifications, with the current word as input, and each of the R + R words as output. increasing the context size C improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. 2R words which is less than C are used as correct label. this step contains sampling 2R words from C word. ReferenceSpeech and Language Processing]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep_Learning]]></title>
    <url>%2F2018%2F07%2F31%2FDeep-Learning%2F</url>
    <content type="text"><![CDATA[multi-class classification problemSolutions: Train multiple classifiers and predict the label one by ont Learn the representation and compare. (clustering, or simply compare distance) Training step is actually a mini-batch. An epoch is the entire training set. NotesBefore training, calculate the memory consumption of the neural network and make sure the GPU has enough memory. On-line learningtrain the model on each data sample and those data samples come in a sequential order.]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[feature_selection]]></title>
    <url>%2F2018%2F07%2F30%2Ffeature-selection%2F</url>
    <content type="text"><![CDATA[Mutual Information$$I(x,y)=\sum_{x\in {0,1}} \sum_{y\in {0,1}} p(x,y)\log_2\frac{P(x, y)}{p(x)p(y)}$$ This could be used as the heuristic of feature selection. Higher mutual information means the feature do relates to the class, which is helpful in classfication. chi-squared test$$X^2(D,t,c)=\sum_{e_t\in {0,1}} \sum_{e_c\in {0,1}} \frac{(N_{e_te_c}-E_{e_te_c})^2}{E_{e_te_c}}$$ N is the observed frequency in D and E the expected frequency. chi-squared test is a measure of how much expected counts E and observed counts N deviate from each other. A high value indicates that the hypothesis of independence is incorrect. ProblemsFor a test with one degree of freedom, the so-called Yates correction should be used, which makes it harder to reach statistical significance. degree of freedom = (num of columns -1) * (num of rows -1) Also, whenever a statistical test is used multiple times, then the probability of getting at least one error increases. If 1,000 hypotheses are rejected, each with 0.05 error probability, then 0.05 × 1000 = 50 calls of the test will be wrong on average. Frequency-based feature selectionselect the feature that occurs usually. When many thousands of features are selected, then frequency-based feature selection often does well. Feature selection for multiple classifiersCommonly, feature selection statistics are first computed separately for each class on the two-class classification task c versus not c and then combined. One combination method computes a single figure of merit for each feature, for example, by averaging the values A(t, c) for feature t, and then selects the k features with highest figures of merit. Another frequently used combination method selects the top k/n features for each of n classifiers and then combines these n sets into one global feature set. Comparison of feature selection methodsMutual information prefer common terms while chi-squared test prefer rare terms. Because A single occurrence is not very informative according to the information-theoretic definition of information. All three methods are greedy methods.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Boosting]]></title>
    <url>%2F2018%2F07%2F28%2FBoosting%2F</url>
    <content type="text"><![CDATA[BaggingFor each trial 1,2…T, a training set of size N(identical to the size of the original training set) is sampled(with replacement) from the original training set. Each trial a classifier \(C^t\) is trained. The final classifier \(C^*\) aggregate those T classfiers together. To classify an instance x, a vote for class k is recorded by every classier for which \(C^t(x)=k\) and \(C^*(x)\) is then the class with the most votes. (Ties being resolve arbitrarily) An order􏲵-correct classifier-learning system is one that􏲬 over many training sets􏲬 tends to predict the correct class of a test instance more frequently than any other class􏲮. Aggre􏲵gating classifiers produced by an order-correct learner results in an optimal classi􏲻er Requirement of bagging:each single classifier is unstable – that is, it has high variance, small change to the training set leads to different classifiers. BoostingIntuition of Boosting: Train several weak classifier and combine together to get a strong classifier. AdaBoostUsually employed in classification problem Step: Initially, each training sample is assigned an equal weight. Train a classifier on the training set Compute the error rate Compute the coefficient of the classifier Change the weight of the training samples. Increase the weights of the samples that are classified wrong, pay more attention to them. Decrease the weights of the samples that are classified correct. Here the calculation of the new weight is actually a softmax to make the weight a probability distribution. Train another classifier on the training set with new weight and repeat the above several steps. Finally, combine the classifiers. The output of the final classifiers(absolute value) is the confidence level of the classfication result. More iterations, higher confidence level. AdaBoost could be viewed as forward stagewise algorithm which optimize the set os parameters step by step. Toy Model1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import numpy as npdef smaller_than(x,y): return y-x &gt; 0def classifier(x, v, flag=1): #threshold v '''flag = 1: x&lt;v are classified as 1, x&gt;v are classified as -1 flag = 0: x&lt;v are classified as -1, x&gt;v are classified as 1''' threshold = int(np.floor(v)+1) if flag == 1: if threshold &lt;= 0: return -np.ones(x.shape) elif threshold &gt; 9: return np.ones(x.shape) x[:threshold] = 1 x[threshold:] = -1 else: if threshold &lt;= 0: return np.ones(x.shape) elif threshold &gt; 9: return -np.ones(x.shape) x[:threshold] = -1 x[threshold:] = 1 return xdef find_best_split(x,y,weight): error = 1 best_result = None best_classifier = None for f in range(2): for j in range(x.shape[0]+1): r = classifier(np.array(x,copy=True),j-0.5,f) # print("j", j) # print("classify_result", r) p_error = np.sum((r!=y)*weight)# /float(len(x)) # print("p_error", p_error) # if p_error &lt; error: # if np.log(p_error)-np.log(error) &lt; 0: if smaller_than(p_error,error): #for the sake of numerical stability error = p_error best_result = np.array(r, copy=True) best_classifier = (j-0.5, f) # print("best_result_123", best_result) # print("error_123", error) # print("best_result_123", best_result) # print("error_123", error) return best_result, error, best_classifierif __name__ == '__main__': x = np.arange(10) y = np.array([1,1,1,-1,-1,-1,1,1,1,-1]) iterations = 10 weight = np.ones((x.shape)) / float(len(x)) coefficient_l = [] c_l = [] for i in range(iterations): best_result, error, best_classifier = find_best_split(np.array(x,copy=True),y,weight) c_l.append(best_classifier) # print("best_result", best_result) # print("error", error) alpha = 1/2.0 * np.log((1-error)/error) coefficient_l.append(alpha) numerator = np.exp(-alpha*y*best_result) denominator_weight = np.sum(weight * numerator) # print("weight ", weight) weight = weight * numerator/ denominator_weight # print("weight_updated ", weight) # print(coefficient_l) '''Final classifier''' final_error = None final_result = np.zeros(x.shape) for i in range(len(c_l)): final_result += coefficient_l[i]*classifier(np.array(x,copy=True), v=c_l[i][0],flag=c_l[i][1]) print(final_result) final_result[final_result&gt;0]=1 final_result[final_result&lt;0] = -1 final_error = np.sum(final_result!=y)/float(len(x)) print(final_error) Boosting TreeWhen ths base classifier is the base function, boosting is named boosting tree. Gradient BoostingIt is not easy to optimize normal loss function for boosting tree. Gradient boosting is employed to solve this problem.For regression problem, utilize the the negative gradient of the loss function to approximate the residual error so as to approximate a boosting tree. ReferenceBagging􏰀 Boosting􏰀 and C4.5􏰁􏰂􏰃 统计学习方法]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word_Vectors]]></title>
    <url>%2F2018%2F07%2F28%2FWordVectors%2F</url>
    <content type="text"><![CDATA[The gradient of the whole word matrix(context matrix) should be outer product rather than normal inner product because their dimensions are not matched. Two types of word vectors count-based: LSA, LDA, Glove window-based: word2vec Drawback of count-based method: unstable, may change if there is a new word. high dimension \(V \times V\) although dimension could be decreased by SVD. Word vectors are application-specific. Word2vecInput vectors are also named context vectors.Output vectors are also named word vectors. Feedforward Neural Net Language Model (NNLM)Time complexity:Q = N × D + N × D × H + H × V, where the dominating term is H × V. N are the N input examples, D is the dimensionality of the word vectors, H is the hidden layer size, V is the vocab size. With binary tree representations of the vocabulary, the number of output units that need to be evaluated can go down to around log2(V). Thus, most of the complexity is caused by the term N × D × H. Huffman trees assign short binary codes to frequent words, and this further reduces the number of output units that need to be evaluated: while balanced binary tree would require log2(V) outputs to be evaluated, the Huffman tree based hierarchical softmax requires only about log2(Unigram perplexity(V)). Continuous Bag-of-Words Model(CBOW)Given context words, predict the central words. Average the input word vectors, which ignore the order of words, and then predict the output word. The training complexity of this architecture is proportional to Q = N × D + D × log2(V ). C is the context size. The order of words in the history and future does not influence the projection, like bag-of-words model. Note that the weight matrix between the input and the projection layer is shared for all word positions in the same way as in the NNLM. Here the weight matrix are the word vectors. Continuous Skip-gram ModelSimilar to CBOW, in this model, input the central word and predict the context words, others are the same. This is a The training complexity of this architecture is proportional to Q = C × (D + D × log2(V )) where C is the maximum distance of the words. For each training word we will select randomly a number R in range &lt; 1; C &gt;, and then use R words from history and R words from the future of the context as correct labels. This will require us to do R × 2 word classifications, with the current word as input, and each of the R + R words as output. increasing the context size C improves quality of the resulting word vectors, but it also increases the computational complexity. Since the more distant words are usually less related to the current word than those close to it, we give less weight to the distant words by sampling less from those words in our training examples. 2 R words which is less than C are used as correct label. this step contains sampling 2 R words from C word. There are two kinds of vectors for each word. They are context vectors and word vectors respectively. Normally the final word vectors could be the sum of two vectors or simply the word vector. ModificationIn skip-gram and CBOW, softmax is extremely expensive. Take skip-gram as an example. The objective is to maximize the following ojective function: $$\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\leq{j}\leq{c},j\neq0}\log p(w_{t+j}|w_t)$$ where c is the size of the training context. The p is defined as : $$p(o|c)=\frac{exp({u_o}^Tv_{c})}{\sum_{w=1}^{W}exp({u_{w}}^T*v_{c})}$$ which is an expensive softmax function to squeeze all the values into a probability distribution. 2 choices to improve softmaxHierarchical SoftmaxTurn all words into a tree so that the height of tree is log2(V). Negative SamplingNoise Contrastive Estimation (NCE) states that a good model is able to differentiate data from noise by means of logistic regression. The skip-gram model only focus on leanring good word vectors. Here the noise is words that are outside of the context. Simplified version of NCE: $$\log\sigma{({u_o}^Tv_{c})}+\sum_{i=1}^k E_{j\sim P_n(w)}\log\sigma{(-{u_j}^Tv_{c})}$$ $$P_n(w_i) = \frac{ {f(w_i)}^{3/4} }{\sum_{j=0}^{n} {f(w_j)}^{3/4}}$$ where n is total number of words in the corpus. Subsampling of Frequent WordsThe frequent words may contain less information. During training, it would discard words in the context by chance using the below formula $$P(w_i)=1-\sqrt{\frac{t}{f(w_i)}} $$ In the implementation code, the formula is $$P(w_i) = (\sqrt{\frac{z(w_i)}{0.001}} + 1) \cdot \frac{0.001}{z(w_i)}$$ GloveUtilize statistical information in co-occurrence matrix. Utilize a third word to determine whether two words are related or not. If two words are not correlated, $$\frac{P_{ik}}{P_{jk}} \approx 1$$ where \(P_{ik}\) is conditional probability \(P(k|i)\). The objective is to find a function to represent the relationship between word vectors and conditional probability. $$F(w_i,w_j,\tilde{w}k)=\frac{P{ik}}{P_{jk}}$$ The function F has to satisfy certain requirements: preserve linear structure exchangeable Finally, weight counts because rare co- occurrences may carry less information. $$J(\theta)=\frac{1}{2}\sum_{i,j=1}^{W}f(P_{ij})(u_i^Tv_j-log P_{ij})^2$$ FastTextIt is a model derived from word2vec. In order to better represent morphological words, each word is now represent by a set of n-gram characters. Each word is added two symbols ‘&lt;’ and ‘&gt;’ respectively to indicate start and end of the word. For example, word ‘FastText’ becomes word ‘‘. 3-gram set is (&lt;Fa,Fas,ast,stT,tTe,Tex,ext,xt&gt;). Negetive sampling of word2vec: $$\log\sigma{({u_o}^Tv_{c})}+\sum_{i=1}^k E_{j\sim P_n(w)}\log\sigma{(-{u_j}^Tv_{c})}$$ In word2vec, each word is an unique unit. Each word has the unique word vectors representation which lose the morphological information. In FastText, subword model is employed, in which each word is now represent by a set of n-gram characters to utilize morphological information. The objective function is: $$\log\sigma{(\sum_{g\in G_{o}}{z_g}^Tv_{c})}+\sum_{i=1}^k E_{j\sim P_n(w)}\log\sigma{(-\sum_{g\in G_{j}}{z_{gj}}^Tv_{c})}$$ where \(G_{o}\) is the set of n-grams character of word \(w_o\), \(z_{g}\) is the vector representation of the specific n-gram characters. $$u_o = \sum_{g\in G_{o}}{z_g}$$ The word vector for each word is then the sum of its all n-gram vector representation. WordRankTurn the problem into a ranking problem. Character-level embeddingAdvantages: better in terms of morphological languages In some languages such as Chinese, each sentence is composed of characters directly. Character-level embedding is also better in this case. OOV(out of vocabulary words). Able to handle OOV. OOV Handling Character-level embedding Initialize the unknown word as the sum of all context vectors and refine it with a high learning rate]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python]]></title>
    <url>%2F2018%2F07%2F26%2Fpython%2F</url>
    <content type="text"><![CDATA[DictionaryFind the key that has the largest value in a dictionary. Assume stats is the dictionary. 123max(stats, key=stats.get)max_key = max(stats, key=lambda k: stats[k]) map dictionary:1my_dictionary = dict(map(lambda kv: (kv[0], f(kv[1])), my_dictionary.items())) build a dictionary with the same key from the other dictionary. 1d = dic.fromkeys(dic.keys(), 0) Since in python every thing is reference, for variable instances like list array, normal assign symbol ‘=’ would add a new reference to the instance. If the value of the instance is changed, it would affect other usage. For example:123l1 = [1,2,3]l2 = l1l2[2] = 100 Now l1[2] is also 100. solution:123l1 = [1,2,3]l2 = copy.deepcopy(l1)l2[2] = 100 Now l1[2] is still 3. or1l2 = list(l1) A shallow copy constructs a new compound object and inserts references into it to the objects found in the original.A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original. from pip import mainImportError: cannot import name main solution: Roll back python3 -m pip install –user –upgrade pip==9.0.3 assert condition is True, or else print the string. 1assert condition, '.....' Decimal, Octal, Hexadecimal, Binary 123oct()hex()bin() convert jupyter notebook to python file: 1jupyter nbconvert --to script [YOUR_NOTEBOOK].ipynb 1getattr(x,foo) is the same as x.foo. get the attribute named foo of x. 1__file__ is only available in running in command line mode. Not available in interactive mode like Jupyter. 1os.path.expanduser(path) turn “~” into home directory 1zip(*iterables) Make an iterator that aggregates elements from each of the iterables. *l idiom is to unpack argument lists when calling a function 解包列表中的变量传递给函数 PandasDataframe.groupby() groups the row with the same attributes together. Source: https://www.kaggle.com/crawford/python-groupby-tutorial df.groupby(‘A’).agg({‘B’: [‘min’, ‘max’], ‘C’: ‘sum’})agg is able to specify what kind of operations you want to apply to the group. pandas.DataFrame.reset_index() is used to reset index for multi-level dataframe. Basic funtions in pandas:(could be passed to agg function) | count | Number of non-NA observations || sum | Sum of values || mean | Mean of values || mad | Mean absolute deviation || median | Arithmetic median of values || min | Minimum || max | Maximum || mode | Mode || abs | Absolute Value || prod | Product of values || std | Bessel-corrected sample standard deviation || var | Unbiased variance || sem | Standard error of the mean || skew | Sample skewness (3rd moment) || kurt | Sample kurtosis (4th moment) || quantile | Sample quantile (value at %) || cumsum | Cumulative sum || cumprod | Cumulative product || cummax | Cumulative maximum || cummin | Cumulative minimum | After groupby, there are multi-level indices. use reset_inedx() to reset the indices. pd.merge()Values with the keys would be kept. The difference is how to deal with values that have different keys. Left Merge: drop the right keys and fill left keys with nan Right Merge: drop the left keys and fill right keys with nan drop all keys fill all keys with nan pandas.get_dummies() convert categorical variable into dummy/indicator variables. Multiprocessingpool.starmap(func, iterable[, chunksize]) each element of starmap is supposed to be an iterable item. pool.map(func, iterable[, chunksize]) each element of map is supposed to be a single item stored in an iterable item. pickleread from pkl file:12with open("file_path", 'rb') as f: d = pickle.load(f) write to pkl file:123import picklefavorite_color = &#123; "lion": "yellow", "kitty": "red" &#125;pickle.dump( favorite_color, open( "save.p", "wb" ) ) format:1&apos;We are the &#123;&#125; who say &#123;&#125;!&apos;.format(&apos;knights&apos;, &apos;Ni&apos;) check whether an object has an attribute.12if hasattr(a, &apos;property&apos;): a.property]]></content>
      <tags>
        <tag>Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linear_Algebra]]></title>
    <url>%2F2018%2F07%2F26%2FLinear-Algebra%2F</url>
    <content type="text"><![CDATA[By default, vectors are column vectors. Outer Product\(\left(\begin{array}{cc}{1} \\ {2}\end{array}\right) \) \(\otimes\) \(\left(\begin{array}{cc}10 &amp; 20\end{array}\right)\) = \( \left(\begin{array}{cc}{1 \times 10} &amp; {1 \times 20} \\ {2 \times 10} &amp; {2 \times 20}\end{array}\right) \) Outer product increase the dimension of the matrix. Outer product of a (a1∗a2) matrix and a (b1∗b2) matrix is a (a1∗a2)∗(b1∗b2) matrix. $$\begin{array} \x \otimes y&amp; = \begin{bmatrix}x_1 &amp; \cdots &amp; x_{1n} \\x_2 &amp; \cdots &amp; x_{2n} \\\cdots &amp; \cdots &amp; \cdots \\x_m &amp; \cdots &amp; x_{mn}\end{bmatrix}\begin{bmatrix}y_1 &amp; \cdots &amp; y_{1q} \\y_2 &amp; \cdots &amp; y_{2q} \\\cdots &amp; \cdots &amp; \cdots \\y_p &amp; \cdots &amp; x_{pq}\end{bmatrix} \&amp; =\begin{bmatrix}x_1y_1 &amp; \cdots &amp; x_1y_{1q} &amp; x_1y_{2} &amp; \cdots &amp; x_1y_{pq} \\\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\x_{1n}y_1 &amp; \cdots &amp; x_{1n}y_{1q} &amp; x_{1n}y_{2} &amp; \cdots &amp; x_{1n}y_{pq} \\x_2y_1 &amp; \cdots &amp; x_2y_{1q} &amp; x_2y_{2} &amp; \cdots &amp; x_2y_{pq} \\\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\x_{mn}y_1 &amp; \cdots &amp; x_{mn}y_{1q} &amp; x_{mn}y_{2} &amp; \cdots &amp; x_{mn}y_{pq}\end{bmatrix}\end{array}$$ Element-wise Product (Hadamard product)$$\begin{array} \x \otimes y&amp; = \begin{bmatrix}x_1 &amp; \cdots &amp; x_{1n} \\x_2 &amp; \cdots &amp; x_{2n} \\\cdots &amp; \cdots &amp; \cdots \\x_m &amp; \cdots &amp; x_{mn}\end{bmatrix}\begin{bmatrix}y_1 &amp; \cdots &amp; y_{1n} \\y_2 &amp; \cdots &amp; y_{2n} \\\cdots &amp; \cdots &amp; \cdots \\y_m &amp; \cdots &amp; x_{mn}\end{bmatrix} \&amp; =\begin{bmatrix}x_1y_1 &amp; \cdots &amp; x_{1i}y_{1i} &amp; x_{1j}y_{1j} &amp; \cdots &amp; x_{1n}y_{1n} \\\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\x_{m1}y_{m1} &amp; \cdots &amp; x_{mi}y_{mi} &amp; x_{mj}y_{mj} &amp; \cdots &amp; x_{mn}y_{mn}\end{bmatrix}\end{array}$$ Example\(\left(\begin{array}{cc}1 &amp; 2 \\ 3 &amp; 4\end{array}\right) \) \(\odot\) \(\left(\begin{array}{cc}10 &amp; 20 \\ 30 &amp; 40\end{array}\right) \) = \(\left(\begin{array}{cc}{1 \times 10} &amp; {2 \times 20} \\ {3 \times 30} &amp; {4 \times 40}\end{array}\right) \) Vector NormL1 Norm: The sum of absolute values in the matrix. $$\lVert w \rVert_1 = \textstyle \sum_{i=1}^n |w_i|$$ L2 Norm: The sqrt of the sum of squared values in the matrix. $$\lVert w \rVert_2 = \sqrt {\textstyle \sum_{i=1}^n w_i^2}$$ Matrix NormThe spectral norm of a matrix A is the largest singular value of A.For example, the square root of the largest eigenvalue of the positive-semidefinite matrix \({\displaystyle A^{*}A}\). Identity MatrixAn identity matrix of size n is the n × n square matrix with ones on the main diagonal and zeros elsewhere. $${ I_{n}={\begin{bmatrix}\ 1&amp;0&amp;0&amp;\cdots &amp;0\\ 0&amp;1&amp;0&amp;\cdots &amp;0\\ 0&amp;0&amp;1&amp;\cdots &amp;0\\ \vdots &amp;\vdots &amp;\vdots &amp;\ddots &amp;\vdots \\ 0&amp;0&amp;0&amp;\cdots &amp;1\end{bmatrix}}}$$ 矩阵乘法满足结合律. Reference机器学习中的基本数学知识 Identity matrix]]></content>
  </entry>
  <entry>
    <title><![CDATA[Numpy]]></title>
    <url>%2F2018%2F07%2F26%2FNumpy%2F</url>
    <content type="text"><![CDATA[numpy.ufunc.at. apply function at given index. np.add.at(a, [0, 1, 2, 2], 1): add 1 to array a at index [0, 1, 2, 2] numpy.random.permutation(x):Randomly permute a sequence, or return a permuted range. 即把一序列打成乱序输出 numpy.outer(a, b, out=None);Compute the outer product of two vectors. operator *: element-wise multuply Axis = 0 is column. Axis = 1 is row a[:6:2] means the first element of every two elements from 0 to 5 array dimension: count the bracket flat attribute which is an iterator over all the elements of the array: np.nditer(a, flags=[‘f_index’]) is used to iterate elements access multiple columns elements: a[b,c] while b is from 0 to n, c shows in each line which element you want to access. https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#indexing-slicing-and-iterating access nth column: a[:,n] transpose an one-dimension array: a.reshape(n,len(a)) multi-dimensional array, count the dimension from low to high np.argsort()Returns the indices that would sort an array.numpy.bincountCount number of occurrences of each value in array of non-negative ints. Np.argmax() Returns the indices of the maximum values along an axis.Np.nonzero() return tuple that consists of arrays. The first array indicates dimension, the second one indicates location(index) 1a = np.array(x,copy=True) copy an array to a new variable.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow]]></title>
    <url>%2F2018%2F07%2F23%2FTensorflow%2F</url>
    <content type="text"><![CDATA[Characteristic: Lazy evaluation like scala Two steps to build and run a model: assemble a graph use a session to execute operations in the graph Placeholder: a node to store variable which would not be modified during backpropagation Variable: a node that would be modified during backpropagation Constant: values that cannot be changed Running parts need to be encapsulated into session.run(). session.run() is evaluation of the graph. Variables could have name and scope. Use TF DType to accelerate the computation or tensorflow has to infer the type. create variables with tf.get_variable 1m = tf.get_variable(&quot;matrix&quot;, initializer=tf.constant([[0, 1], [2, 3]])) rather than 123Before running, variables need to be initialized. initializer = tf.global_variables_initializer()with tf.Session() as sess: sess.run(initializer)12Initialize only a subset of variables: with tf.Session() as sess: sess.run([a,b])12Initialize only a single variable: m=tf.get_variable(“matrix”,initializer=tf.constant([[0, 1], [2, 3]]))with tf.Session() as sess: sess.run(m.initializer)1234567891011W.assign(100) creates an assign operation that assign 100 to variable W. The operation needs to be executed in a session to take effect.tensorflow separates definitions of computation and execution. It first assembles a graph. Second use a session to execute.Nodes are operators, variables, and constantsEdges are tensors.A Session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated. Session will also allocate memory to store the current values of variables. tf.Session.run(fetches, feed_dict=None, options=None, run_metadata=None)1234567891011121314151617181920212223242526272829303132fetches is a list of tensors whose values you want. tensorflow would return the values you want and skip computing unnecessary values.Multiple graphs require multiple sessions, each will try to use all available resources by defaultwhy graphs:1. break computation to facilitate auto-differentiation.2. save computation. only compute necessary values3. easier to distribute4. ML models visulised as graphsCan&apos;t pass data between them without passing them through python/numpy, which doesn&apos;t work in distributedCreate the summary writer after graph definition and before running your session.```python# The default path for saving event files is the same folder of this python file.tf.app.flags.DEFINE_string(&apos;log_dir&apos;, os.path.dirname(os.path.abspath(__file__)) + &apos;/logs&apos;,&apos;Directory where event logs are written to.&apos;)# Store all elements in FLAG structure!FLAGS = tf.app.flags.FLAGSwith tf.Session() as sess: sess.run(tf.global_variables_initializer()) writer = tf.summary.FileWriter(os.path.expanduser(FLAGS.log_dir), sess.graph) .....# Closing the writer.writer.close() These codes are used to specify the path of logs directory and store all elements in FLAG structure. 12345# clean flag namefrom absl import flagsfor name in list(flags.FLAGS): delattr(flags.FLAGS, name) 1UnrecognizedFlagError: Unknown command line flag 'f' To solve this problem, just add a flag. In coommand line, there is not such a problem.1tf.app.flags.DEFINE_string('f', '', 'kernel') 1tf.app.flags.DEFINE_string(&apos;str_name&apos;, &apos;def_v_1&apos;,&quot;descrip1&quot;) tf.app.flags is used to assign a name to a variable and parse variables in command line. The first argument is the variable name. The second argument is the default value. The third argument is a description. argparse module in python could also be employed to parse arguments. Any evaluation of variable must be run in a session. Three ways to initialize variables: Initializing Specific Variables. 12345# &quot;variable_list_custom&quot; is the list of variables that we want to initialize.variable_list_custom = [weights, custom_variable]# The initializerinit_custom_op = tf.variables_initializer(var_list=all_variables_list) This enable us to initialize specific variables. We are supposed to initialize all variables after specific initialization. Global variable initialization. This operation has to be done after the construction of a model. Initialization of a variables using other existing variables. New variables can be initialized using other existing variables’ initial values by taking the values using initialized_value().12345# Create another variable with the same value as 'weights'.WeightsNew = tf.Variable(weights.initialized_value(), name="WeightsNew")# Now, the variable must be initialized.init_WeightsNew_op = tf.variables_initializer(var_list=[WeightsNew]) tf.one_hot() The axis argument determines which axis is the one-hot vector. Name scope is for grouping nodes together to facilitate visulisation in Tensorboard. Name scope is for reusing of operations while variable scope is for reusing of variables. Name scope is able to assign names to a set of variables. It is equivalent to add “name_scope/” to the variable name as prefix. 1with tf.name_scope(&quot;outer&quot;): c_2 = tf.constant(2, name=&quot;c&quot;) # =&gt; operation named &quot;outer/c&quot; An Operation is a node in a TensorFlow Graph that takes zero or more Tensor objects as input, and produces zero or more Tensor objects as output. Objects of type Operation are created by calling a Python op constructor (such as [tf.matmul] or [tf.Graph.create_op]. A Tensor is a symbolic handle to one of the outputs of an Operation. It does not hold the values of that operation’s output, but instead provides a means of computing those values in a TensorFlow [tf.Session]. Note that [tf.Tensor] objects are implicitly named after the [tf.Operation]that produces the tensor as output. A tensor name has the form &quot;&lt;OP_NAME&gt;:&lt;i&gt;&quot;. tf.get_variable would search the variable with the given name. If there is not such a variable, it would create a new variable. 1tf.reduce_mean() Sum over a vector in a particular dimension and compute the mean. The axis argument is used to specify the particular dimension. 1tf.nn.softmax_cross_entropy_with_logits_v2() Take unscaled log probabilities as input and perform softmax. After that, compute cross entropy loss. This functino allows gradient to backpropagated to labels. 1tf.cast() Casts a tensor to a new type. 1x = tf.constant([1.8, 2.2], dtype=tf.float32)tf.cast(x, tf.int32) # [1, 2], dtype=tf.int32 12tf.Graph().as_default(): ... There is a default graph in tensorflow. All operations and variables are added to the graph if not specified. This line of code use another graph and add variables to that graph. Outside the code block, variables would be added to the default graph. TensorFlow will create a new [tf.Tensor]each time you use the same tensor-like object. If the tensor-like object is large (e.g. a numpy.ndarray containing a set of training examples) and you use it multiple times, you may run out of memory. By default, tf.Session is bounded to the default graph. If there is multiple graphs, specify graph parameter in tf.Session(). The allow_soft_placement flag allows the switching back-and-forth between different devices. In tensorflow, not all operations are supported in GPU. The log_device_placement flag is to present which operations are set on what devices. The max_to_keep flags determine the maximum number of the saved models that the TensorFlow keeps and its default is set to ‘5’ by TensorFlow. 1tf.zeros_like(input_tensor, dtype=None, name=None, optimize=True) create a tensor filled with zero with the same shape as input_tensor 1tf.fill(dims, value, name=None) create a tensor filled with a scalar value 123tf.lin_space(start, stop, num, name=None)tf.range(start, limit=None, delta=1, dtype=None, name=&apos;range&apos;) create a tensor with a sequence of values scalars are treated like tensors with zero dimension 1a = sess.run(a) #use a_out = sess.run(a) instead for the sake of memory Use TF DType whenever possible for computation convenience. Each session maintains its own copy of variables tf.constant is an op. tf.Variable is a class with many ops. Only use tf.constant for primitive type. Otherwise the loading of graphs would be expensive. Constant values are stored in the graph definition. 123s = tf.get_variable(&quot;scalar&quot;, initializer=tf.constant(2)) #is preferreds = tf.Variable(2) Initializer is an op. You need to execute it within the context of a session. Each variable has its own initializer. Sessions allocate memory to store variable values. Eval() a variable is equivalent to sess.run(a variable) tf.Variable.assign() is an op. 1tf.placeholder(dtype, shape=None, name=None) shape=None is easy to construct graphs but it is difficult for debugging. tensors could be fed with some dummy values for testing purpose. Lazy loading would make the graph bloated and hard to load. Remeber to seperate definition of a graph from computing/running ops. 12# feed 1.0 to xx = tf.placeholder(tf.float32, shape=None, name=&apos;x&apos;) InvalidArgumentError (see above for traceback): You must feed a value for placeholder tensor ‘x’ with dtype float. 12# feed 1.0 to xx = tf.placeholder(tf.float32, name=&apos;x&apos;) works……. 1tf.cond( pred, true_fn=None, false_fn=None, strict=False, name=None) tf.cond: pred is an expression. true_fn and flase_fn are callable function. If the expression is true, return the result of true_fn and vise versa. tf.placeholder is pythonic, but it is also easy to become bottleneck if running in single thread. tf.data is preferred. For prototyping, feed dict can be faster and easier to write (pythonic). But tf.data is tricky to use when you have complicated preprocessing or multiple data sources. Session looks at all trainable variables that optimizer depends on and update them. NCE guarantees approximation to softmax but Negative Sampling doesn’t. tf.train.Saver saves graph’s variables in binary files. It saves session rather than graph. When re-using parameters, it is necessart to construct the graph first and then restore the parameters. 12345with tf.name_scope(&quot;summaries&quot;): tf.summary.scalar(&quot;loss&quot;, self.loss) tf.summary.scalar(&quot;accuracy&quot;, self.accuracy) tf.summary.histogram(&quot;histogram loss&quot;, self.loss) summary_op = tf.summary.merge_all() merge them all into one summary op to make managing them easier. Like everything else in TF, summaries are ops. For the summaries to be built, you have to run it in a session 123loss_batch, _, summary = sess.run([loss, optimizer, summary_op])writer.add_summary(summary, global_step=step) Randomization: operation level. my_var = tf.Variable(tf.truncated_normal((-1.0,1.0), stddev=0.1, seed=0)) graph level. tf.set_random_seed(2) tf dataset: 1tf.data.Dataset.from_tensor_slices() constructs a dataset from one or more tf.Tensor objects. It slices the first dimension of the tensor. 1tf.data.Dataset.batch() or1tf.data.Dataset.map() apply a transformation to a dataset and create another dataset. A [tf.data.Dataset] represents a sequence of elements, in which each element contains one or more Tensorobjects. For example, an element could be an image corresponds to a label or a sequence of tokens correspond to another sequence of tokens or only a simple label. A tf.data.Iterator provides the main way to extract elements from a dataset. The operation returned by Iterator.get_next() yields the next element of a Dataset when executed, and typically acts as the interface between input pipeline code and your model. For more sophisticated uses, the Iterator.initializeroperation enables you to reinitialize and parameterize an iterator with different datasets, so that you can, for example, iterate over training and validation data multiple times in the same program. tf.data.TFRecordDataset constructs dataset from files if the files are on disk in the recommended TFRecord format. If the iterator reaches the end of the dataset, executing the Iterator.get_next() operation will raise a tf.errors.OutOfRangeError. After this point the iterator will be in an unusable state, and you must initialize it again if you want to use it further. A common pattern is to wrap the “training loop” in a try-except block: 1sess.run(iterator.initializer)while True: try: sess.run(result) except tf.errors.OutOfRangeError: break A reinitializable iterator can be initialized from multiple different Dataset objects. For example, you might have a training input pipeline that uses random perturbations to the input images to improve generalization, and a validation input pipeline that evaluates predictions on unmodified data. These pipelines will typically use different Dataset objects that have the same structure (i.e. the same types and compatible shapes for each component). 12iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next() A one-shot iterator is the simplest form of iterator, which only supports iterating once through a dataset, with no need for explicit initialization. One-shot iterators handle almost all of the cases that the existing queue-based input pipelines support, but they do not support parameterization. 123dataset = tf.data.Dataset.range(100)iterator = dataset.make_one_shot_iterator()next_element = iterator.get_next() An initializable iterator requires you to run an explicit iterator.initializer operation before using it. In exchange for this inconvenience, it enables you to parameterize the definition of the dataset, using one or more tf.placeholder()tensors that can be fed when you initialize the iterator. 12345678max_value = tf.placeholder(tf.int64, shape=[])dataset = tf.data.Dataset.range(max_value)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# Initialize an iterator over a dataset with 10 elements.sess.run(iterator.initializer, feed_dict=&#123;max_value: 10&#125;)for i in range(10): value = sess.run(next_element) assert i == value The Iterator.get_next() method returns one or more tf.Tensor objects that correspond to the symbolic next element of an iterator. Each time these tensors are evaluated, they take the value of the next element in the underlying dataset. Note that the iterator does not get to the next element immediately. The returned element is suuposed to be passed to tf.Session.run() to get the next elements and advance the iterator. Applying arbitrary Python logic with tf.py_func() 1tf.stack( values, axis=0, name=&apos;stack&apos;) concatenate tensors along specified dimension. 1tf.data.Dataset.padded_batch() padded_shapes is None, the component will be padded out to the maximum length of all elements in that dimension. Estimators are high-level abstraction of low-level tensorflow operation. It enables easier training, testing on different kinds of devices and modes like training on a single PC or a cluster. 4 steps to use pre-made Estimators write function to import dataset Define the feature columns. Instantiate the relevant pre-made Estimator like specifying number of hidden layers, output features There are training and evaluation methods for Estimators. Call a training, evaluation, or inference method. tf.feature_column could be regarded as a kind of interface between the dataset and model in Estimators. It could abstract continuous values or categorical values.]]></content>
      <tags>
        <tag>Notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dependency_Parsing]]></title>
    <url>%2F2018%2F07%2F23%2FDependency-Parsing%2F</url>
    <content type="text"><![CDATA[Transition-Based Dependency ParsingLinear time, greedy algorithm, no backtracking. A configuration is a stack, a buffer of token lists and an oracle. The key is to train the oracle. The algorithm is intuitive. If there is a match in the candidate relation set, pop the words and add the relation to the result set. Push the words in the buffer into the stack. repeat until no word left. Creating an OracleSupervised learning is employed to do this. Generating Training Datagiven a reference parse and a configuration, the training oracle proceeds as follows: Choose LEFTARC if it produces a correct head-dependent relation Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent relationand (2) all of its dependents in the buffer have already been assigned. Otherwise, choose SHIFT. Featurescreate feature sets. LearningThe dominant approaches to training transition-based dependency parsers have been multinomial logistic regression and support vector machines, both of which can make effective use of large numbers of sparse features. Deep learning approaches have been applied successfully to transition-based parsing. These approaches eliminate the need for complex, hand-crafted features and have been particularly effective at overcoming the data sparsity issues normally associated training transition-basedparsers. Alternative Transition Systemsarc eager transition system is just a modified version of the oracle. The operation of the oracle: LEFTARC: Assert a head-dependent relation between the word at the front ofthe input buffer and the word at the top of the stack; pop the stack. RIGHTARC: Assert a head-dependent relation between the word on the top ofthe stack and the word at front of the input buffer; shift the word at the frontof the input buffer to the stack. SHIFT: Remove the word from the front of the input buffer and push it ontothe stack. REDUCE: Pop the stack. Beam SearchBFS with a heuristic filter that prunes the search frontier to stay within a fixed-size beam width. Graph-Based]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Back_Propagation]]></title>
    <url>%2F2018%2F07%2F21%2FBack-Propagation%2F</url>
    <content type="text"><![CDATA[Derivative of composite function$$\frac{d\frac{f(x)}{g(x)}}{dx} = \frac{f’(x)g(x)-f(x)g’(x)}{g^2(x)}$$ Derivative of Cross Entropy$$CE(\vec{y},\vec{\hat{y}}) = -\sum_iy_i\log(\hat{y_i})$$ Here \(y_i\) is an one-hot vector denotes the correct class \(c_i\). In the actual computation, it only selects out the correct class without any extra function. For a single example \(x_i\): if \(\hat{y_i}=y_i\): $$\frac{dCE(y_i,\hat{y_i})}{dx_i} = \frac{1}{\hat{y_i}}\frac{d(\hat{y_i})}{dx_i} = \frac{\sum_j^Ce^{(x_j)}}{e^{x_i}}\ \frac{e^{x_i}\sum_j^Ce^{x_j}-e^{2x_i}}{(\sum_j^Ce^{x_j})^2} = \hat{y_i}-1$$ if \(\hat{y_i}\ne y_i\): $$\frac{dCE(y_i,\hat{y_i})}{dx_i} = \frac{1}{\hat{y_i}}\frac{d(\hat{y_i})}{dx_i} = \frac{\sum_j^Ce^{(x_j)}}{e^{x_i}}\ \frac{e^{x_i}e^{x_k}}{(\sum_j^Ce^{x_j})^2}=\hat{y_i}$$ The rest is just compute the gradient layer by layer. Note that when compute the gradient with regard to activation function, there would be outer product.]]></content>
      <tags>
        <tag>DL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pagerank]]></title>
    <url>%2F2018%2F07%2F18%2FPagerank%2F</url>
    <content type="text"><![CDATA[A good hub page is one that points to many good authorities; a good authority page is one that is pointed to by many good hub pages. PageRankIntuitionPages that are visited more are given higher weight. Employ graph to represent the internet. Higher in links means visited more. PageRank values would converge to the final value regardless of the start state and it is in the range [0,1]. So the PageRank values could also be viewed as the probability of the surfer is in the node i at any time given the PageRank value of the node i. PageRank could be regarded as a random walk in the internet. Start at any state, and then walk through the out links randomly. In case of circular routes and dead end, employ the scaled version of random walk. As the random walk proceeds, some nodes are visited more often than others; intuitively, these are nodes with many links coming in from other frequently visited nodes. The idea behind PageRank is that pages visited more often in this walk are more important. Teleport:if N is the total number of nodes in the web graph, the teleport operation takes the surfer to each node with probability 1/N. Random Walk InterpretationScaled version of random walk:with alpha probability take teleport operation and with 1-alpha probability take the out links of the current nodes. Interpreted in terms of the (scaled) version of PageRank, Perron’s Theorem tells us that there is a unique vector y that remains fixed under the application of the scaled update rule, and that repeated application of the update rule from any starting point will converge to y.This vector y thus corresponds to the limiting PageRank values we have been seeking. Graph InterpretationEach vertex is a node. PageRank value is 1 in total. Initially, each node has the same pagerank value 1/N. With the different in links and out links, the pagerank value would flow through those links to different notes. So the pagerank value would change accordingly. Scaled version of graph:With alpha probability take teleport operation and with 1-alpha probability do the normal flow operation. In terms of graph, it discount the overall pagerank value to \(1-\alpha \) and assign the rest \(\alpha \) pagerank value to each node. So each node would get the same pagerank value \(\alpha /N\). The trainsition matrix = the original transition matrix \(\alpha\) + (1-\(\alpha\)) (1/N, ….1/N) which means with \(\alpha\) probability takes the normal walk and with 1-\(\alpha\) probability takes the random walk. Markov chainA Markov chain is a discrete-time stochastic process: a process that occurs in a series of time-steps in each of which a random choice is made. A Markov chain consists of N states. Each web page will correspond to a state in the Markov chain. A Markov chain is characterized by an N × N transition probability matrix P each of whose entries is in the interval [0, 1]; the entries in each row of P add up to 1. Transition matrix P:The rows represent that given a node i, the probability of the node i transits to nodes j.(distribute the pagerank score)The columns represent that given a node j, the probability of nodes i transits to node j.(incoming pagerank score) ERGODIC MARKOV CHAIN:Definition: A Markov chain is said to be ergodic if there exists a positive integer T0 such that for all pairs of states i, j in the Markov chain, if it is started at time 0 in state i then for all t &gt; T0, the probability of being in state j at time t is greater than 0. For a Markov chain to be ergodic, two technical conditions are required of its states and the non-zero transition probabilities; these conditions are known as irreducibility and aperiodicity. irreducibility ensures that there is a sequence of transitions of non-zero probability from any state to any other. aperiodicity ensures that the states are not partitioned into sets such that all state transitions occur cyclically from one set to another. Scaled version of random walk which take teleport operation with alpha probability guarantees transition probability are greater than 0 and no loop transitions.(irreducibility and aperiodicity) Compute PageRank Iteratively compute \(\vec{x}P^t\) s.t. \(\vec{x}\) becomes unchanged. Compute the eigenvector coresponding to the largest eigenvalue 1 which is the PageRank. But compute the eigenvector could be computationally expensive. ImplementationMatrix-Multiplication version of PageRank:123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as npclass Pagerank(object): def __init__(self, N=0, alpha=0): self.alpha = alpha self.N = N self.pg_vector = None '''derive transition matrix from adjacency matrix''' def derive_transition(self,adjacency_matrix,alpha=0): if self.alpha != 0: alpha = self.alpha N = adjacency_matrix.shape[0] transition_m = np.zeros(adjacency_matrix.shape) for i in range(adjacency_matrix.shape[0]): if np.sum(adjacency_matrix[i,:]) == 0: transition_m[i,:] = np.array([1/N]*N) #this node is a dead end, transit to one of all nodes in the graph else: # with alpha probability transit to one of all nodes, 1-alpha take normal random walk transition_m[i, :] = alpha * np.array([1 / N] * N) + (1-alpha) * adjacency_matrix[i,:] / np.sum(adjacency_matrix[i,:]) return transition_m def propagate_transition(self, transition_m, pg_vector=None): #pg_vector is column vectors if pg_vector is not None: return np.dot(transition_m.T, pg_vector) else: if self.pg_vector is None: self.initialize_pg_vector() self.pg_vector = np.dot(transition_m.T, self.pg_vector) return self.pg_vector def initialize_pg_vector(self): assert self.N != 0, "Please input number of Nodes N" self.pg_vector = np.ones((self.N)).T self.pg_vector /= self.N print(self.pg_vector) Graph version of PageRank:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127import numpy as npimport copyclass Graph(object): def __init__(self, d, vertex=None): assert isinstance(d,dict), "please input a dict" self.edges = d self.d = d self.vertexes = self.get_vertexes() def get_vertexes(self): return list(self.d.keys()) def get_edges(self): l = [] for k in self.d.keys(): for v in self.d[k]: l.append(k+'-'+v) return l def __matrix_to_dict(self, matrix): #convert adjacency matrix to dict pass def BFS(self): done = [] for k in self.d.keys(): if k not in done: done.append(k) for v in self.d[k]: if v in done: continue else: done.append(v) return done def DFS(self): done = [] l = list(self.d.keys()) while len(l) != 0: current = l.pop() if current not in done: done.append(current) l.extend(list(self.d[current])) return done def DFS_recursive(self): done = [] l = list(self.d.keys()) def recursive(element): waiting_l = self.d[element] for e in waiting_l: if e not in done: done.append(e) recursive(e) while len(l) != 0: current = l.pop() if current not in done: done.append(current) recursive(current) return doneclass PageRank(object): def __init__(self, graph): self.g = graph self.vertexes = graph.vertexes self.pr_value = &#123;&#125; self.num_points = len(self.vertexes) self.__pr_value_initialization__() self.new_pr_value = &#123;&#125; def __pr_value_initialization__(self): for ver in self.vertexes: self.pr_value[ver] = 1.0 / self.num_points def __new_pr_value_initialization__(self): for ver in self.vertexes: self.new_pr_value[ver] = 0 def update_a_round(self, alpha=0): self.__new_pr_value_initialization__() for ver in self.vertexes: if len(self.g.edges[ver]) == 0: continue # self.new_pr_value[ver] = self.pr_value[ver] value_flow = self.pr_value[ver] / len(self.g.edges[ver]) for child in self.g.edges[ver]: self.new_pr_value[child] += value_flow if alpha != 0: for ver in self.vertexes: self.new_pr_value[ver] = alpha * 1.0 / self.num_points + (1-alpha) * self.new_pr_value[ver] self.pr_value = copy.deepcopy(self.new_pr_value) def propagate_transition(self, alpha=0): last = np.zeros((self.num_points)) # for i in range(100): while True: self.update_a_round(alpha) current = np.array(list(self.pr_value.values())) error = abs(current - last) # print('last', last) # print('current', current) # print('i', i) # print('error: ', error) if np.sum(error) &lt; 1e-6: break last = current'''Test'''d_map = &#123;&#125;d = &#123;'A':['B','C'],'B':['D','E'],'C':['F','G'],'D':['H','A'],'E':['H','A'],'F':['A'],'G':['A'],'H':['A'],'P':[]&#125;# d = &#123;'A':['B'],'B':['A','C'],'C':['B']&#125;g = Graph(d)pg = PageRank(g)pg.propagate_transition()print(pg.pr_value) Topic Specific PageRankStart at a random page in the topic and also end at a random page in the topic. \(\vec{x}_{sports}\) is a topic specific pagerank vector where for pages belongs to sports, there is a topic specific pagerank and pagerank of other pages are 0. The only difference is that topic specific pagerank do not take random walk to all pages with the same probability. It only take random walk to s set of topic pages \(S_{sports}\) and \(S_{politics}\). For example, the probability is \(\alpha\) and peference on sports is 0.6 while peference on politics is 0.4. Then the probability of teleport to sports is \(\alpha0.6\) while probability of teleport to politics is \(\alpha0.4\). Calculate the PageRank of a page with regard to topics. The only difference is that the teleportation is different. The naive teleport go to a node in the graph with probability 1/N. The teleportation of topic specific pagerank is to go to a page in a specific topic with probability alpha. Given the number of pages S in the topic T, the probability is \(\frac{1}{S}\) rather than \(\frac{1}{N}\). i.e.:Let s be [A,B,C,D]. A belongs to topic 1. B,C and D belongs to topic 2.When calculating the topic specific pagerank, for topic 2, the formula is still \(\vec{x}P^t\). But here \(P\) for topic 2 is \(\alpha M + (1-\alpha)\frac{1}{S}\) rather than \(\alpha M + (1-\alpha)\frac{1}{N}\). Actually the formula for topic 2 is \(\alpha M + (1-\alpha)[0,\frac{1}{3},\frac{1}{3},\frac{1}{3}]\) and in normal PageRank it should be \(\alpha M + (1-\alpha)[\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]\). Personalized PageRankNow the difference is still the teleportation. The teleportation is tailored according to the users’ interest. Let’s assume the interest of a person is sports and finance. Their weights are 0.8,0.2 specifically. The teleportation is to go to a topic with the corresponding weight and then a page in the specific topic with probability alpha. This could also be viewed as a linear combination of topic specific vectors. In this example, the formula would be \(\alpha M + (1-\alpha) [0.8\vec{x_{sports}} + 0.2\vec{x_{finance}}]\). ReferenceNetworks, Crowds, and Markets: Reasoning About a Highly Connected World Introduction to Information Retrieval]]></content>
  </entry>
  <entry>
    <title><![CDATA[Parsing]]></title>
    <url>%2F2018%2F07%2F13%2FParsing%2F</url>
    <content type="text"><![CDATA[Context-Free Grammar(CFG) The symbols that correspond to words in the language (“the”, “nightclub”) are called terminal symbols. They are left nodes in a parse tree. The lexicon is the set of rules that introduce these terminal symbols. The symbols that express abstractions over these terminals are called non-terminals. Ther are the non-left Each grammar must have one designated start symbol, which is often called S &lt;\s&gt;. Formal Definition of CFGG=(T,N,S,R) T is a set of terminal symbols. N is a set of nonterminal symbols S is the start symbol (S \(\in\) N) R is a set of rules/productions of the form X -&gt; \(\gamma\). A grammar G generates a language L. Conversion to CNFChomsky Normal Form(CNF): for each non-terminal, only two non-ternimal or single terminal could be derived from it. Unit Production: Rules with a single non-terminal on the right. three situations needed to address in any generic grammar: rules that mix terminals with non-terminals on the right-hand side rules that have a single non-terminal on the right-hand side rules in which the length of the right-hand side is greater than 2 rules that mix terminals and non-terminals is to introduce a new dummy non-terminal to substitute the original terminal. For example, a rule for an infinitive verb phrase such as INF-VP → to VP would be replaced by thetwo rules INF-VP → TO VP and TO → to. To eliminate unit production:if A-&gt;&gt; B by a chain of one or more unit productions and B → \(\gamma\) is a non-unit production in our grammar, then we add A → γ for each such rule in the grammar and discard all the intervening unit productions. Rules with right-hand sides longer than 2 are normalized through the introductionof new non-terminals that substitute the longer sequences iteratively. i.e.: A → B C D. After transformation, A → X D and X -&gt;B C The entire conversion process can be summarized as follows: Copy all conforming rules to the new grammar unchanged. Convert terminals within rules to dummy non-terminals. Convert unit-productions. Make all rules binary and add them to new grammar. Learn PCFG RulesCount-based For unambiguous parsing, just count each rule showed in the resulting parsing tree and get the probability like language model. For ambiguous parsing, initialize the parsing s.t. each rule is assigned the equal probability. And then parse the sentece an calculate probability of each parsing. Update the weight. Iteratively doing this, an extension of EM algorithm. Problems of PCFG Lack of Sensitivity to Lexical Dependencies Independence Assumptions Miss Structural Dependencies Between Rules. For different location, each rule may be expanded differently. Improving PCFGs by Splitting Non-Terminals split the NP non-terminal into two versions: one for subjects, one for objects. parent annotation This method also induce new problem. More rules lead to less training which may cause overfitting. Modern models automatically search for the optimal splits. The split and merge algorithm for example, starts with a simple X-bar grammar, alternately splits the non-terminals, and merges non-terminals, finding the set of annotated nodes that maximizes the likelihood of the training set treebank.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CKY]]></title>
    <url>%2F2018%2F07%2F13%2FCKY%2F</url>
    <content type="text"><![CDATA[CKY is to initialize a matrix and fill in the upper right triangular matrix. Each word is filled into the matrix from left to right and bottom to up. There is a probability distribution about contituent for each word. And then the matrix is filled up using viterbi algorithm. Take the max probability. Transfer the rules to binary reduce the complexity.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Language_Model]]></title>
    <url>%2F2018%2F07%2F09%2FLanguage-Model%2F</url>
    <content type="text"><![CDATA[Language model is able to capture information and predict next word. It is aware of world information. Unigram$$p(w_i) = \frac{c(w_i)}{N}$$ \(c(w_i)\) is the count of the word. N is the total number of word. Bigram$$p(w_i|w_{i-1}) = \frac{c(w_iw_{i-1})}{c(w_{i-1})}$$ N-GramGiven the previous N words, estimate the probability of the current word. $$p(w_i|w_{i-N-1}^{N-1}) = \frac{c(w_iw_{i-N-1}^{N-1})}{c(w_{i-N-1}^{N-1})}$$ Deal with Unknown WordsThere are two common ways to train the probabilities of the unknown word model . The first one is to turn the problem back into a closed vocabulary one by choosing a fixed vocabulary in advance: Choose a vocabulary (word list) that is fixed in advance. Convert in the training set any word that is not in this set to the unknown word token in a text normalization step. Estimate the probabilities for from its counts just like any other regular word in the training set. The second alternative, replacing words in the training data by based on their frequency.i.e.: replace all words by that occur fewer than n times in the training set or choose the top V words by frequency and replace the rest by UNK. The third way:$$p(w_i)=\lambda_1p_{ML}(w_i)+(1-\lambda_1)\frac{1}{N}$$ Save some probability for unknown words \((\lambda_{unk} = 1-\lambda_1)\). This is in fact another kind of smoothing. Discount the probability of known words and assign to the unknown words. \(\lambda\) is a hyperparameter. It is 0.95 by default. N is the total number of words. CoverageThe percentage of known words in the corpus. Usually omit the symbol of the end of a sentence . Entropy of a sentence$$H(S)= -\frac{1}{N}\sum_{w\in S}\log_2 P(w)$$ State-of-the-art word embeddings or word tokensword embeddings or word tokens could be a CNN over a sequence of character embeddings or simply a token embedding. BERT: Pre-training of Deep Bidirectional Transformers for Language UnderstandingUniversal Language Model Fine-tuning (ULMFiT)First pre-train a language model, the key concept is how to fine-tune. fine-tune target task language model Discriminative fine-tuning. Each layer is updated separately with different learning rate. Best practice is to choose learning rate by only fine-tuning the last layer. And decrease the selected learning rate layer by layer. Slanted triangular learning rates. Quickly converge to a suitable region of the parameter space in the beginning of training and then refine its parameters. First increase learning rate to make the model converge to a suitable region. And then decrease the learning rate to fine-tune the model. Target task classifier fine-tuning Concat pooling: maxpool and mean pool all hidden states, and do concatenation. Gradual unfreezing: unfreeze layer one-by-one and do fine-tuning. ELMOELMO apply a bidirectional RNN to compute the hidden states that are used to predict next words. Here bidirectional means the final output to the softmax is concat of two language model outputs. So it is indeed two single directional language models rather than true bidirectional language models like BERT. For example, “i like machine learning and natural language processing”. When predicting the word “learning”, the left LSTM computes information form “i” to “machine” while the right LSTM computes information from “processing” to “and”. The input to softmax is concat of those two feature vectors. The final word vector of each word is concat of the original word vectors(the first hidden layer) and all hidden states in the middle bidirecitonal RNN. The word vectors of ELMO could be task-specific. It is a linear combination of all hidden states in the middle bidirecitonal RNN. The weight for each hidden state is different for different tasks. TagLM将预训练过程中隐藏层的输出特征和任务过程中隐藏层的特征concat到一起，作为最后模型决策的输入特征，显著提升任务效果。concat hidden state of last layer from two direction of RNN as a kind of features. After that, concat hidden states which is the output of RNN during sequence labeling and feed to another RNN as input. This actually enrich the features. Downstream tasks‘’‘Word embeddings are nowadays pervasive on a wide spectrum of Natural Language Processing (NLP) and Natural Language Understanding (NLU) applications. These word representations improved downstream tasks in many domains such as machine translation, syntactic parsing, text classification, and machine comprehension, among other’‘’Downstream tasks refer to any task in NLP like text classification, QA and so on. Reference TagLM: Semi-supervised sequence tagging with bidirectional language models ELMO: Deep contextualized word representations ULMFiT: Universal Language Model Fine-tuning for Text Classification BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word_Senses]]></title>
    <url>%2F2018%2F07%2F06%2FWord_Senses%2F</url>
    <content type="text"><![CDATA[Word sense disambiguation(WSD)Word sense disambiguation(WSD) is significant because it is common that there are multiple senses within a word. Better word sense disambiguation would improve the performance of tasks such as question answering and machine translation. Two kinds of WSD tasks: lexical sample all-words Lexical sample: given a set of target words and with an inventory of senses from some lexicon. Supervised learning is employed to solve this problem at usual. Hand-label a small set of words and corresponding sense, and feed into a classifier. All-words: given entire texts and a lexicon that contains an inventory of senses for each entry. The objective is to disambiguate every content word in the text. It is impractical to train a classifier for each term because the number of words is large. Supervised Learning Approach Feature Vector Extraction. Process the sentence in a context window to extract feature vector. There are two kinds of feature vector: collocation features. Multiple words in a position-specific relationship to a target word. It is effective at encoding local lexical and grammatical information that usually isolate a given sense. High performing systems generally use POS tags and word collocations of length 1,2 and 3 from a window of words 3. bag-of-words: neglecting the position information, just record number of context words in a context window. The context words could be pre-selected(frequent used words) given a target word. Evaluation: most frequent sense as baseline. Most freauent sense usually choose the most frequently used sense given a context. Dictionary and ThesaurusThe Lesk AlgorithmIt is a family of dictionary-based algorithms.Simplified algorithm compute the number of overlapped words between a given context and all of the word senses. Output the sense with the most overlapped words. Original Lesk Algorithm compute the number of overlapped words between sense of each context word and the sense of the target word.i.e.: Bank is a financial institute for deposit.Simplified algorithm compute the number of overlapping words between each sense and the context [financial,institute, deposit] and output the sense with the most overlapped words.The original algorithm compute all senses with all senses of each word in the context [financial,institute, deposit] and output the sense with the most overlapped words. Notes:stop-words are not taken into consideration. Problem: Entries of the dictionary for the target word are not enough which reduce the chance of overlapping. Solution: To expand the sense, include senses of similar words which may increase the number of overlapping words. The best solution is to utilize sense-tagged corpus, which is so called Corpus Lesk Algorithm. To expand the signature for a word sense, add all word senses with the same label. i.e.: add word sense of large, huge to big. Instead of employ a stop-list, the algorithm apply IDF to reduce the weight of function words like the,of. This algorithm is usually a baseline. The bag-of-words approach could be improved by combining the Lesk algorithm. The glosses and example sentences for the target sense in WordNet would be used as words features along with sense-tagged corpus like SemCor. Graph-based MethodsA graph is employed to represent words. Each sense is a node and relations between senses are edges. It is usually an undirected graph. The correct sense is the one that is central in the graph. Use degree to decide centrality. Another approach is to assign probability to nodes according to personalized page rank. Problem of both supervised and dictionary-based approach: need hand-built resouces Semi-Supervised: Bootstrapping Use a small set of labeled data to train the classifier. Do classification. Select the result with high confidence and add to the training set. Train the classifier and repeat step 2 and 3. Stop until there is no untagged data left or reach a specific error rate threshold. To construct the training set, hand-label it or use heuristic to help. Heuristic: one sense per collocation: certain words or phrases strongly indicates a specific sense. one sense per discourse:a particular word appearing multiple times in a text or discourse often appeared with the same sense. This heuristic seems to hold better for coarse-grained senses and particularly for cases of homonymy rather than polysemy. Unsupervised Approach:Word Sense Induction(WSI)It is actually a clustering of word senses. For each token(sense) of a word in the corpus, use context word vector \(\vec{c}\) to represent it. Do clustering s.t. there is N clusters and each token \(\vec{c}\) is assigned to a cluster. Each cluster defines a sense. Recompute the center s.t. the center vector \(\vec{s_j}\) represent a sense. To do word sense disambiguation: compute context word vector \(\vec{c}\) to represent the sense of the word. Do clustering and assign the context vector to a cluster. All we need is a clustering algorithm and a distance metric between vectors. A frequently used technique in language applications is known as agglomerativeclustering. Recent algorithms have also used topic modeling algorithms like Latent Dirichlet Allocation (LDA), another way to learn clusters of words based on their distributions. It is fair to say that no evaluation metric for this task has yet become standard. Word Similarity(Thesaurus Methods)Word similarity refers to meaning(synonyms).Word relatedness characterizes relationships(antonyms,synonyms). Path-length based ApproachThesaurus could be viewed as a graph. The simplest thesaurus-based algorithms use the edge between words to measure similarity. pathlen(c1, c2) = 1 + edges in the shortest path between the sense nodes c1 and c2. path-length based similarity:$$sim_{path}(c_1,c_2)=\frac{1}{pathlen(c_1, c_2)}$$ Path lengths are not the same in different level of the hierarchy.It is possible to refine path-based algorithms with normalizations based on depth in the hierarchy. In general an approach that independently represent the distance associated with each edge it is preferred. For data without sense tag(words are not presented according to their senses), measure similarity according to senses of words. If there is a pair of senses is similar, then two words are similar. $$word_{sim}(w_1,w_2)=\max_{c_1\in senses(w_1)\ c_2\in senses(w_2)} sim(c_1, c_2)$$ information-content word-similarity algorithmsstill rely on the structure of the thesaurus but also add probabilistic information derived from a corpus. The corpus is represented as a set of concepts. And it is represented as a tree structure. The leaf nodes are basic concepts and parent nodes contain the children which is a superset of concepts. Each node is assigned a probability. So p(root) = 1. $$P(c) = \frac{\sum_{w\in words(c)}count(w)}{N}$$ c is a specific concept. The probability of a concept is number of words in the concept divided by total number of words. information content: information content (IC) of a concept c $$IC(c) = −\log P(c) $$ the lowest common subsumer(LCS) of two concepts:LCS(c1, c2) = the lowest common subsumer,the lowest node in the hierarchy that subsumes both c1 and c2.(The lowest parent node of c1 and c2) Resnik similarity: similarity between two words is related to their common information; the more two words have in common, the more similar they are. Resnik proposes to estimate the common amount of information by the information content of the lowest common subsumer of the two nodes. $$sim_{Resnik}(c_1,c_2)=-\log P(LCS(c_1,c_2))$$ Lin similarity Similarity Theorem: The similarity between A and B is measured by the ratiobetween the amount of information needed to state the commonality of A andB and the information needed to fully describe what A and B are.$$sim_{Lin}(A,B)=\frac{common(A,B)}{description(A,B)}$$ the information in common between two concepts is twice the information in the lowest common subsumer LCS(c1, c2). Adding in the above definitions of the information content of thesaurus concepts$$sim_{Lin}(C_1,C_2)=\frac{2\times \log P(LCS(c_1,c_2))}{\log P(c_1)+\log P(c_2)}$$ Jiang-Conrath distance express distance rather than similarity, work as well as or better than all the other thesaurus-based methods: $$dist_{JC}(c_1, c_2) = 2\times \log P(LCS(c_1, c_2))−(\log P(c_1) +\log P(c_2)) $$ It could be transferred to similarity by take reciprocal.$$sim_{JC}=\frac{1}{dist_{JC}}$$ dictionary-based method: This method makes use of glosses, which are, in general, a property of dictionaries rather than thesauruses. Two concepts/senses are similar if their glosses contain overlapping words. For each n-word phrase that occurs in both glosses, Extended Lesk adds in ascore of \(n^2\).(the relation is non-linear because of the Zipfian relationship between lengths of phrases and their corpus frequencies; longer overlaps are rare, so they should be weighted more heavily) Given such an overlap function, when comparing two concepts (synsets), ExtendedLesk not only looks for overlap between their glosses but also between theglosses of the senses that are hypernyms, hyponyms, meronyms, and other relationsof the two concepts. If we just considered hyponyms and definedgloss(hypo(A)) as the concatenation of all the glosses of all the hyponym senses ofA, the total relatedness between two concepts A and B might be $$similarity(A,B) = overlap(gloss(A), gloss(B))+overlap(gloss(hypo(A)), gloss(hypo(B)))+overlap(gloss(A), gloss(hypo(B)))+overlap(gloss(hypo(A)),gloss(B))$$ Extended Lesk overlap measure: $$sim_{eLESK}(c_1,c_2) =\sum_{r,q\in RELS}overlap(gloss(r(c_1)),gloss(q(c_2)))$$ RELS is the set of possible relations whose glosses we compare. Possible relations are like hypernyms, hyponyms, meronyms, and other relations of the two concepts. Evaluating Thesaurus-Based SimilarityThe most common intrinsic evaluation metric computes the correlation coefficient between an algorithm’s word similarity scores and word similarity ratings assigned by humans. Zipf’s law states that given some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table.The most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word. ReferenceZipf’s law]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vector_semantics]]></title>
    <url>%2F2018%2F07%2F05%2Fvector-semantics%2F</url>
    <content type="text"><![CDATA[Pointwise Mutual Information (PMI)Normal count-based word-context matrix would provide a lot of useless information. i.e.: the entry (apple, the) is large but provide useless information. Instead we’d like context words that are particularly informative about the targetword. The best weighting or measure of association between words should tell ushow much more often than chance the two words co-occur. mutual information The pointwise mutual information is a measure of how often two events x and y occur, compared with what we would expect if they were independent:$$I(x,y)=\log_2\frac{P(x, y)}{p(x)p(y)}$$ The pointwise mutual information of a word w and a cotext word c is:$$PMI(w,c)=\log_2\frac{P(w, c)}{p(w)p(c)}$$ The ratio gives us an estimate ofhow much more the target and feature co-occur actually than just by chance. Positive PMI: use 0 to substitute negetive PMI. It is used commonly because negative PMI is unreliable. Negative PMI means that the two words co-occur less than expectation which could be due to the small corpora. A normal count-based co-occurrence matrix could be turned into PPMI matrix. Problem:biased toward infrequent words because the demoninator which is the frequency of the word is low. To solve this problem,change the computation for P(c). $$PPMI_\alpha(w,c)=\max(\log_2\frac{P(w, c)}{p(w)p_\alpha(c)},0)$$$$p_\alpha(c) = \frac{count(c)^\alpha}{\sum_ccount(c)^\alpha}$$ Usually \(\alpha\)=0.75 drawing on a similar weighting used for skipgrams.This increases the probability assigned to rarecontexts and solve this problem. Another solutio is Laplace smoothing. TF-IDFIDF is used to give a higher weight to rare words because they are likely to present more information. $$idf_i = \log (\frac{N}{df_i})$$ Combining term frequency with IDF results in a scheme known as tf-idf weighting of the value for word i in document j,\(w_{ij}\):\ $$w_{ij}=tf_{ij}idf_i$$ The tf-idf weighting is by far the dominant way of weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing including summarization. t-testSimilarity Measure for vectors(binary)Jaccard similarity:$$sim_{Jaccard}=\frac{\sum_{i=1}^N\min(v_i,w_i)}{\sum_{i=1}^N\max(v_i,w_i)}$$ $$J(A,B)=\frac{|A\cap B|}{|A\cup B|}$$ The min of \(v_i\) and \(w_i\)means if a feature exists in one vector but not the other vector, the result would be zero. Thedenominator can be viewed as a normalizing factor. Dice measure:$$sim_{Dice}=\frac{2 \times \sum_{i=1}^N\min(v_i,w_i)}{\sum_{i=1}^N(v_i+w_i)}$$ KL divergenceif two vectors, \(\vec{v}\) and \(\vec{w}\), each express a probabilitydistribution (their values sum to one), then they are are similar to the extent that these probability distributions are similar. Given two probability distribution or vectors P and Q: $$D(P||Q) = \sum_xP(x)\log \frac{P(x)}{Q(x)}$$ Unfortunately, the KL-divergence is undefined when Q(x) = 0 and P(x) \(\neq\) 0, which is a problem since these word-distribution vectors are generally quite sparse. Jensen-Shannon divergence solves this problem.$$JS(P||Q)=D(P|\frac{Q+P}{2})+D(Q|\frac{Q+P}{2})$$ Using syntax to define a word’s contextInstead of defining a word’s context by nearby words, we could instead define it bythe syntactic relations of these neighboring words. i.e.: the word duty could be represented by a bunch of combinations like additional,administrative(adj) or assert, assign(verb). Evaluating Vector Modelstest their performance on similarity, and in particular on computing the correlation between an algorithm’s word similarity scores and word similarity ratingsassigned by humans.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hierarchical_Softmax]]></title>
    <url>%2F2018%2F07%2F04%2FHierarchical-Softmax%2F</url>
    <content type="text"><![CDATA[Hierarchical softmaxIt truns the normal softmax to a tree structure to avoid computing every context word with the center word. Normal softmax requires normalization which is extremely expensive especially in the case of Natural Language Processing that there are usually millions or billions of tokens in the corpus. Normal softmax would squeeze the values of these millions or billions of tokens into a probability distribution each time predicting the context word. Normal softmax computes P(Y |X) directly which needs normalization, the author chooses to assign Y to different clusters such that there is a function mapping X to different clusters. Given the cluster, compute the conditional probability to avoid normalization over the entire X.$$P(Y = y|X = x) =P(Y |C = c(Y ), X)P(C =c(Y )|X)$$ Prove:$$P(Y |X) = \sum_i P(Y, C = i|X) = \sum_i P(Y |C =i, X)P(C = i|X) = P(Y |C = c(Y ), X)P(C =c(Y )|X)$$ C(Y) is a function to classify Y into C classes. \(P(Y |C = c(Y ), X)P(C =c(Y )|X)\) is equivalent to \(\sum_i P(Y |C =i, X)P(C = i|X)\) if every Y is classified into a class and the probablities of assign a Y to a cluster are multiplied together. To utilize the architecture of binary trees, do hierarchical clustering such that each time two clusters are mergerd. Initially each word is a cluster. As a result, each level consists of a number of cluster. From the root to the leaf, multiply the probabilities together and we get the probability. Each word v must be represented by a bit vector \((b_1(v), . . . b_m(v))\) (where m depends on v) $$P(v|w_{t−1}, . . . , w_{t−n+1}) =\prod_{j=1}^mP(b_j(v)|b_1(v), . . . , b_{j−1}(v), w_{t−1}, . . . , w_{t−n+1})$$ \(b_j(v)\) is the intermediate vector in the tree. The intermediate node is in fact a cluster. Each leaf node is a word. Each intermediate note could be viewed as a group of similar-meaning words. Each node is associated with a feature vector. $$P(b = 1|node, w_{t−1}, . . . , w_{t−n+1}) =sigmoid(\alpha_{node} + \beta’\cdot tanh(c + Wx + UN_{node}))$$]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sql]]></title>
    <url>%2F2018%2F07%2F03%2Fsql%2F</url>
    <content type="text"><![CDATA[RDBMS stands for Relational Database Management System. Every table is broken up into smaller entities called fields which is a column in a table designed to maintain specific information about every record in the table. A record, also called a row, is each individual entry that exists in a table. A record is a horizontal entity in a table. A column is a vertical entity in a table that contains all information associated with a specific field in a table. A database most often contains one or more tables. Each table is identified by a name. Tables contain records (rows) with data. Semicolon is the standard way to separate each SQL statement in database systems that allow more than one SQL statement to be executed in the same call to the server. Keywords COUNT SELECT COUNT(Country) FROM Customers; Return the number of country from table Customers Distinct SELECT Country FROM Customers; Selects all values from the “Country” column in the “Customers” table. WHERE The WHERE clause is used to extract only those records that fulfill a specified condition. SELECT * FROM Customers WHERE Country=’Mexico’; Selects all the customers from the country “Mexico”, in the “Customers” table. AND operator displays a record if all the conditions separated by AND is TRUE.The OR operator displays a record if any of the conditions separated by OR is TRUE.The NOT operator displays a record if the condition(s) is NOT TRUE. ORDER BY INSERT INTO IS NULL and IS NOT NULL to test null values SELECT TOP = Mysql SELECT column_name(s) FROM table_name WHERE condition LIMIT number; LIKE The LIKE operator is used in a WHERE clause to search for a specified pattern in a column. %:The percent sign represents zero, one, or multiple characters _:The underscore represents a single character IN: = multiple OR conditions. BETWEEN: selects values within a given range. The values can be numbers, text, or dates.It is inclusive: begin and end values are included. SQL aliases are used to give a table, or a column a temporary name. An alias only exists for the duration of the query. JOIN: SELECT column_name(s) FROM table1 INNER JOIN table2 ON table1.column_name = table2.column_name; UNION: combine the result-set of two or more SELECT statements. GROUP BY: used with aggregate functions (COUNT, MAX, MIN, SUM, AVG) to group the result-set by one or more columns. The HAVING clause was added to SQL because the WHERE keyword could not be used with aggregate functions. The EXISTS operator is used to test for the existence of any record in a subquery. The ANY operator returns true if any of the subquery values meet the condition. The ALL operator returns true if all of the subquery values meet the condition. The SELECT INTO statement copies data from one table into a new table. The INSERT INTO SELECT statement copies data from one table and inserts it into another table. Notes SQL keywords are NOT case sensitive: select is the same as SELECT SQL requires single quotes around text values. Numeric fields should not be enclosed in quotes: (INNER) JOIN: Returns records that have matching values in both tables LEFT (OUTER) JOIN: Return all records from the left table, and the matched records from the right table RIGHT (OUTER) JOIN: Return all records from the right table, and the matched records from the left table FULL (OUTER) JOIN: Return all records when there is a match in either left or right table Each SELECT statement within UNION must have the same number of columns.The columns must also have similar data types.The columns in each SELECT statement must also be in the same order]]></content>
      <tags>
        <tag>Database sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Semantics_with_Dense_Vectors]]></title>
    <url>%2F2018%2F07%2F03%2FSemantics-with-Dense-Vectors%2F</url>
    <content type="text"><![CDATA[Skip-gram ModelGiven a center word, predict its neighbor word(context word). There are two matrices, one is context matrix and the other is word matrix. The training objective of the Skip-gram model is to find word representations that are useful for predicting the surrounding words in a sentence or a document.The objective of the Skip-gram model is to maximize the average log probability \(\frac{1}{T}\sum_{t=1}^T\sum_{−c≤j≤c,j\neq 0}\log p(w_{t+j}|w_t) \) The basic Skip-gram formulation defines \(p(w_{t+j} |w_t)\) using the softmax function:$$p(w_{O}|w_I) = \frac{exp({v’_{w_O}}^Tv_{w_I})}{\sum_{w=1}^Wexp({v’w}^Tv{w_I})}$$ Negative SamplingNoise Contrastive Estimation (NCE). NCE posits that a good model should be able to differentiate data from noise by means of logistic regression. While NCE can be shown to approximately maximize the log probability of the softmax, the Skipgram model is only concerned with learning high-quality vector representations, so we are free tosimplify NCE.Here is Negative Sampling, its objective funtion: $$\log\sigma({v’_{w_O}}^Tv_{w_I})+\sum_{i=1}^kE_{w_i\sim P_n(w)}[\log\sigma({-v’_{w_i}}^Tv_{w_I})]$$ The above objective function is used to replace every \(\log P(w_O|w_I )\) term in the Skip-gram objective. Thus the task is todistinguish the target word \(w_O\) from draws from the noise distribution \(P_n(w)\) using logistic regression, where there are k negative samples for each data sample. k in the range 5–20 are useful for small training datasets, while for large datasets the k 2–5. The main difference between the Negative sampling and NCE is that NCE needs both samples and the numerical probabilities of the noise distribution, while Negative sampling uses only samples. And NCE approximately maximizes the log probability of the softmax, this property is not important for our application. Essentially, the probability for selecting a word as a negative sample is related to its frequency, with more frequent words being more likely to be selected as negative samples.Raise to the power of 3/4 is an emprical result. $$P(w_i) = \frac{ {f(w_i)}^{3/4} }{\sum_{j=0}^{n}\left( {f(w_j)}^{3/4} \right) }$$ To counter the imbalance between the rare and frequent words, we used a simple subsampling approach: each word \(w_i\)in the training set is discarded with probability computed by the formula $$p(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$$ where \(f(w_i)\) is the frequency of word \(w_i\) and t is a chosen threshold, typically around \(10^{−5}\). Another implementation:$$p(w_i)=(\sqrt{\frac{z(w_i)}{0.001}}+1)\cdot \frac{0.001}{z(w_i)}$$ 0.001 is the default sample value. Smaller values of ‘sample’ mean words are less likely to be kept. \(z(w_i)\) is the frequency of the word i. P(wi)=1.0 (100% chance of being kept) when z(wi)&lt;=0.0026. This means that only words which represent more than 0.26% of the total words will be subsampled. P(wi)=0.5 (50% chance of being kept) when z(wi)=0.00746. projection layer input is one-hot vector, multiplied by embedding layer, the output is the projection layer. The units in the projection layer are word vectors. Hyperparametersa context window size L CBOWcontinuous bag of words Reversed version of skip-grams, predicting the current word \(w_j\) from the context window of 2L words around it $$p(w_{c,j}|w_j) = \frac{exp(c_k\cdot v_j)}{\sum_{i\in|V|}exp(c_i\cdot v_j)}$$ \(c_k\)means the \(k_{th}\) context word. \(v_j\)means the word vector of given word \(w_j\). The computing of demoninator \(\sum_{i\in|V|}exp(c_i\cdot v_j)\) is extremely expensive because it requires computing for each given word \(w_j\). Relationship between Word VectorsWord vector matrix W and context word vector matrix C multiply together would produce a |V|*|V| matrix X.Skip-gram’s optimal value occurs whenthis learned matrix X is actually a version of the PMI matrix, with the values shiftedby logk (where k is the number of negative samples):$$WC = X^{PMI} −\log k$$ In other words, skip-gram is implicitly factorizing a (shifted version of the) PMImatrix into the two embedding matrices W and C, just as SVD did, albeit with a different kind of factorization. Once the embeddings are learned, for each word \(w_i\), there is a word vector \(v_i\) and a context word vector \(c_i\). We can choose to throw away the C matrix and just keep W, as we did with SVD.Alternatively we can add the two embeddings together, using the summed embedding\(v_i + c_i\) as the new d-dimensional embedding, or we can concatenate theminto an embedding of dimensionality 2d. As with the simple count-based methods like PPMI, the context window sizeL effects the performance of skip-gram embeddings, and experiments often tunethe parameter L on a dev set. As with PPMI, window sizing leads to qualitativedifferences: smaller windows capture more syntactic information, larger ones moresemantic and relational information. One difference from the count-based methodsis that for skip-grams, the larger the window size the more computation the algorithm requires for training. Properties of embeddingsOffsets between vector embeddingscan capture some relations between words. Brown ClusteringBrown clustering is an agglomerative clustering algorithm for deriving vector representations of words by clustering words based on their associations with the preceding or following words. Each word is initially assigned to its own cluster. merge each pair of clusters. The pair whose merger results in the smallest decrease in the likelihood of the corpus (according to the class-based language model) is merged. Clustering proceeds until all words are in one big cluster Each cluster contains words that are contextually similar(similar meaning/sense).After clustering, a word can be represented by the binary string that corresponds to its path from the root node.Each prefix represents a cluster that the word belongs to.These prefixes can then be used as a vector representation for the word; the shorter the prefix, the more abstract the cluster. (i.e.:the string 0001 represents the names of common nouns for corporate executives {chairman, president}, 1 is verbs {run, sprint, walk}, and 0 is nouns.) The length of the vector representation can thus be adjusted to fit the needs of the particular task. a 4-6 bit prefix to capture part of speech information and a full bit string to represent words. The first 8 or9-bits of a Brown clustering perform well at grammar induction. Because they arebased on immediately neighboring words, Brown clusters are most commonly usedfor representing the syntactic properties of words, and hence are commonly used asa feature in parsers. Nonetheless, the clusters do represent some semantic properties as well. Note that the naive version of the Brown clustering algorithm described above isextremely inefficient — \(O(n^5)\): at each of n iterations, the algorithm considers each of \(O(n^2)\) merges, and for each merge, compute the value of the clustering by summing over \(O(n^2)\)terms. because it has to consider every possible pair of merges. In practicewe use more efficient \(O(n^3)\) algorithms that use tables to pre-compute the values for each merge. hard clustering algorithm:each word has only one cluster class-based language modela model in which each word \(w\in V\) belongs to a class \(c\in C\) with a probability P(w|c).Class based LMs assigns a probability to a pair of words \(w_{i−1}\) and \(w_i\) by modeling the transition between classes rather than between words:$$P(w_i|w_{i−1}) = P(w_i|c_i)P(c_i|c_{i−1})$$ Given the class label of word \(c_{i-1})\), compute the transition probability and then given the new transition, the probability of current word \(w_i\). The class-based LM can be used to assign a probability to an entire corpus givena particularly clustering C as follows: $$P(corpus|C) = \prod_{i−1}^nP(c_i|c_{i−1})P(w_i|c_i)$$ Class-based language models are generally not used as a language model for applications like machine translation or speech recognition because they don’t workas well as standard n-grams or neural language models. LSA(latent semantic analysis):applying SVD to the term-document matrix LSA is a particular application of SVD to a |V| × c term-document matrix X representing |V| words and their co-occurrence with c documents or contexts.SVD factorizes any such rectangular |V| × c matrix X into the product of three matricesW, \(\sum\), and \(C^T\). In the matrix W, each of the w rows still represents a word, but each column represents one of m dimensions in a latent space, such that the m column vectors are orthogonal to each other and the columns are ordered by the amount of variance in the original dataset. The number of such dimensions m is the rank of X (the rank of a matrix is the numberof linearly independent rows). \(\sum\) is a diagonal matrix, with singular valuesalong the diagonal, expressing the importance of each dimension. The matrix\(C^T\) still represents documents or contexts, but each row now represents one of the new latent dimensions and the m row vectors are orthogonal to each other.By using only the first k dimensions instead of all m dimensions, the product of these 3 matrices becomes a least-squares approximation to the original X. Since the first dimensions encode the most variance, one way to view the reconstruction is as modeling the most important information in the original dataset. Using only the top k dimensions leads to a reduced |V| ×k matrix \(W_k\), with one k-dimensioned row per word. This row now acts as a dense k-dimensional vector (embedding) representing that word, substituting for the very high-dimensional rows of the original X. LSA implementations generally use a particular weighting of each co-occurrence cell that multiplies two weights called the local and global weights for each cell (i, j)—term i in document j. The local weight of each term i is its log frequency: log f(i, j) +1. The global weight of term i is a version of its entropy: $$1+\frac{\sum_jp(i,j)\log p(i,j)}{\log D}$$D is the number of documents. Advantages: Solve the problem of one word multiple senses or one sense multiple words by reduce the demensionality and extract the key information. Information extraction is to project the original co-occurence data to a new axes which could be viewed as a semantic space. Disadvantages: SVD requires lots of time to compute. ReferenceDistributed Representations of Words and Phrasesand their CompositionalityWord2Vec Tutorial Part 2 - Negative Sampling]]></content>
      <tags>
        <tag>NLP SVD Skip-gram CBOW Brown Clustering</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neural_Networks_and_Neural_Language_Model]]></title>
    <url>%2F2018%2F07%2F01%2FNeural-Networks-and-Neural-Language-Model%2F</url>
    <content type="text"><![CDATA[A neural network with one hidden layer can be shown to learn any function. Stochastic gradient descentis to choose a training example or a batch of examples to compute the gradients rather than use the entire training set. XOR Problemnot linearly separablea hidden layer rotates the axis or converts the original data to another plane. Cross Entropy Loss$$L(\hat{y}, y) = \log p(\hat{y}_i) $$If \(\hat{y}\) != y, it doesn’t contribute to the loss. Only the true class would contribute the loss. If \(p(\hat{y})\) =1, \(\log p(\hat{y}_i)=0\). If \(p(\hat{y})\) =0, \(\log p(\hat{y}_i)=\infty\).\(\hat{y}\) means output of the network. Embedding LayerCould be understood as a weight matrix. Because the one-hot vector is multiplied by the embedding layer to get the word vector, so the embedding layer is actually also a weight matrix. Via back propagation, it could also be trained simultaneously with other weights in the neural network.]]></content>
  </entry>
  <entry>
    <title><![CDATA[Logistic_Regression]]></title>
    <url>%2F2018%2F06%2F29%2FLogistic-Regression%2F</url>
    <content type="text"><![CDATA[Binomial Logistic Regression$$p(1|x) = \frac{\exp(\vec{w} \cdot \vec{x})}{1+\exp(\vec{w} \cdot \vec{x})}$$ $$p(0|x) = \frac{1}{1+\exp(\vec{w} \cdot \vec{x})}$$ odds: the probability that an event occur divided by the probability that an event doesn’t occur. \(\frac{p}{1-p}\) log odds: take log of odds. \(\log \frac{p}{1-p}\). In terms of logistic regression, \(\log \frac{p}{1-p}=\vec{w} \cdot \vec{x}\). Actually logistic regression turns the linear function \(\vec{w} \cdot \vec{x}\) into probability distribution. So logistic regression is acutually a linear classification model with some operations to turn the output into probability distribution. The final output label is the one with the highest value. Parameters EstimationUse gradient descent. Multinomial Logistic Regressionmultinomial logistic regression, sometimes referred to within language processing as MaxEnt entropy modeling, MaxEnt for short. Logistic regression belongs to the family of classifiers known as the exponential or log-linear classifiers.Technically, logistic regression refers to a classifier that classifiesan observation into one of two classes, and multinomial logistic regression is used when classifying into more than two classes First, wrap the exp function around the weight-feature dot-product \(w\cdot f\) , which will make the values positive. A discriminative model discriminativemodel takes this direct approach, computing P(y|x) by discriminating among the different possible values of the class y rather than first computing a likelihood. Logistic Regression $$p(c|x) = \frac{\exp(\sum_iw_if_i(c,x))}{\sum_{c’ \in C}\exp(\sum_{i=1}^N w_if_i(c’,x)}$$ In vector from: $$p(c|x) = \frac{\exp(\vec{w} \cdot \vec{f(c,x)})}{\sum_{c’ \in C}\exp(\vec{w}\cdot \vec{f(c,x)})}$$ Here c actually range from 1 to N+1. And by default \(\exp(\sum_{i=1}^N w_if_i(N+1,x)=0\) exp is to make to result greater than 0.\(f_i(c, x)\), meaning feature i for a particular class c for a given observation x. In NLP, x is usually a document. Feature i usually means whether a specific word \(x_i\) has shown in the document for a particular class. \(w_i\) is the weight corresponding to the feature i. For example, in sentiment analysis, classes are 0 and 1 corresponding to positive and negative. The word best is a feature, namely \(\ f_1\), its value for class 1 coube be +10 and -10 for class 0. Computing the actual probability rather than just choosing the best class is useful when the classifier is embedded in a larger system. Otherwise compute the nominater is sufficient to do classification. \(\hat{c}={argmax}{c\in C}\sum{i=1}^N w_if_i(c,x)\) Learning weightsThe intuition is to choose weights that make the classes of the training examples more likely. Indeed, logistic regression is trained with conditional maximum likelihood estimation. This means we choose the parameters w that maximize the (log) probability of the y labels in the training data given the observations x. $$\hat{w} = {argmax}_w\sum_jlogP(y^{(j)}|x^{(j)})$$ It means given a training set, we choose weights that maximize the probability of \(j^{th}\) training example \(x^{(j)}\) is classified to the lable \(y^{(j)}\). The objective function L that we are maximizing is thus$$L(w) = \frac{\exp(\sum_i^Nw_if_i(y^{(j)},x^{(j)}))}{\sum_{y’ \in Y}\exp(\sum_{i=1}^N w_if_i(y’^{(j)},x^{(j)})}$$ Max EntropyWhen there is no training data available, the model assign the same probability to each class. If there is training data, the model try to reach the max entropy with the constraint that satisfy the training data. ReferenceSpeech and Language Processing 统计学习方法]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine_Learning]]></title>
    <url>%2F2018%2F06%2F29%2FMachine-Learning%2F</url>
    <content type="text"><![CDATA[contingency table consists of 4 entries: True Positive, True Negative, False Positive, False Negative. Accuracy doesn’t work well whenthe classes are unbalanced. Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$ Recall measures the percentage of items actually present in the input that werecorrectly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$ precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.In practice, we generally combine precision and recall into a single metric calledF-measure the F-measure that is defined as:$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$ The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall. Harmonic mean is used because it is a conservative metric; the harmonic mean oftwo values is closer to the minimum of the two values than the arithmetic mean is.Thus it weighs the lower of the two numbers more heavily. In any-of or multi-label classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d. one-of or multinomial classification, multinomial classification in which the classes are mutually exclusive and each document or item appears inexactly one class. Build a separate binary classifier trained on positiveexamples from c and negative examples from all other classes. Now given a testdocument or item d, we run all the classifiers and choose the label from the classifier with the highest score. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. The only problem with cross-validation is that because all the data is used fortesting, we need the whole corpus to be blind; we can’t examine any of the datato suggest possible features and in general see what’s going on. But looking at thecorpus is often important for designing the system. For this reason, it is commonto create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set. regularization: used to penalize large weightsL1 regularization is called ‘the lasso’ or lasso regression and L2 regression is called ridge regression, and both are commonly used in language processing. L2 regularization is easier to optimize because of its simple derivative, while L1 regularization is more complex (the derivative of |w| is noncontinuousat zero). But where L2 prefers weight vectors with many small weights, L1 prefers sparse solutions with some larger weights but many more weights set to zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features. L2 regularization:$$\sum_{j=1}^Nw_j^2$$ L1 regularization:$$\sum_{j=1}^N|w_j|$$ Both L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplaceprior on the weights. L2 regularization corresponds to assuming that weights aredistributed according to a gaussian distribution with mean µ = 0. The regularization technique is useful for avoiding overfitting by removing or downweighting features that are unlikely to generalize well. Many kinds of classifiers including naive Bayes do not have regularization, and so instead feature selection is used to choose the important features and remove the rest. The basis of feature selection is to assign some metricof goodness to each feature, rank the features, and keep the best ones. The number of features to keep is a meta-parameter that can be optimized on a dev set. Feature SelectionFeatures are generally ranked by how informative they are about the classification decision. A very common metric is information gain. Information gain tells us how many bits of information the presence of the word gives us for guessing the class, and can be computed as follows (where ci is the ith class and ¯w means that a document does not contain the word \(w^-\)): While feature selection is important for unregularized classifiers, it is sometimesalso used in regularized classifiers in applications where speed is critical, since it is often possible to get equivalent performance with orders of magnitude fewer features. The overly strong conditional independence assumptions of Naive Bayes mean that if two features are correlated naive Bayes will multiply them, overestimating the evidence. Logistic regression is much more robust to correlated features; if two features f1 and f2 are perfectly correlated, regression will simply assign half the weight to w1 and half to w2. naive Bayes works extremely well (even better than logistic regression or SVMs) on small datasets or short documents.Furthermore, naive Bayes is easy to implement and very fast to train. Nonetheless, algorithms like logistic regression and SVMs generally work better on larger documentsor datasets. bias-variance tradeoffThe bias of a classifier indicates how accurate it is at modeling different training sets.The variance of a classifier indicates how much its decisions are affected by small changesin training sets. Models with low bias (like SVMs with polynomial or RBF kernels) are very accurate at modeling the training data. Models with low variance (like naiveBayes) are likely to come to the same classification decision even from slightly different training data. low-bias models tend to overfit, and do not generalize well to very different test sets.low-variance models tend to generalize so well that they may not have sufficient accuracy.Thus model trades off bias and variance. Adding more features decreases bias by making it possible to more accurately model the training data, butincreases variance because of overfitting. Regularization and feature selection areways to improve (lower) the variance of classifier by downweighting or removingfeatures that are likely to overfit. In addition to the choice of a classifier, the key to successful classification is thedesign of appropriate features. Features are generally designed by examining thetraining set with an eye to linguistic intuitions and the linguistic literature on the domain. For some tasks it is especially helpful to build complex features that are combinationsof more primitive features. For logistic regression and naive Bayes thesecombination features or feature interactions have to be designed by hand. feature interactionsSome other machine learning models can automatically model the interactionsbetween features. For tasks where these combinations of features are important(especially when combination of categorical features and real-valued features mightbe helpful), the most useful classifiers may be such classifiers,including SupportSVMs Vector Machines (SVMs) with polynomial or RBF kernels, and random forests dimensionality reduction(PCA,SVD)First rotate the axes of the original dataset into a new space. The new space is chosen so that the highest order dimensioncaptures the most variance in the original dataset, the next dimension captures the next most variance, and so on. In this new space, the data if represented with a smaller number of dimensions and stillcapture much of the variation in the original data. For binary classification, sigmoid would suffice the requirement. Softmax(Logistic Regression)Linear decision boundary. Bias TermBias is significant because it allows the activation function to shift left and right. The weight is to change the steepness of the activation function. Role of Bias in Neural Networks Max-margin Objective function]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentiment_Classification_Naive_Bayes]]></title>
    <url>%2F2018%2F06%2F29%2FSentiment-Classification%2F</url>
    <content type="text"><![CDATA[In most text classificationapplications, however, using a stop word list doesn’t improve performance,and so it is more common to make use of the entire vocabulary and not use a stopword list Naive Bayes Classifier for Sentiment AnalysisCorrect estimation implies accurate prediction, but accurate prediction does not imply correct estimation. NB classifiers estimate badly, but often classify well. The assumption that terms are independent does not hold. But in terms of classification, the class that a document belongs is usually irrelevant to position. This somehow reduce the influence of the dependence. i.e.: Finance industry is promising in HK.HK finance is booming. $$C_{NB}=\underset{c\in C}{argmax}\ logP(c)+\sum_{i \in positions}logP(w_i|c)$$ Each word \(w_i\) could be viewed as a feature for a specific document. Maximum Likelihood to train Naive Bayes$$\hat{p}(c)=\frac{N_c}{N_{doc}}$$ The standard solution for an unknown word that doesn’t show up in the training set is to ignore such words—remove them from the test document and not include any probability for them at all. Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization to deal with zero count. $$\hat{P}(w_i|c) = \frac{count(w_i, c) +1}{(\sum_{w\in V}(count(w, c)) +|V|}$$ Note that the vocabulary V consists of the union of all the word types in all classes, not just the words in one class. If the denominator is the word in the class \(c_i\), then given a class, the probability of the sentence shows up is 1. That is meaningless. Binary Multinominal Naive Bayes(The Bernoulli model)Whether a word occurs or not seems to matter more than its frequency. The feature in this model is not term frequency anymore. It is whether a word has shown up in the sentence. Smoothing could also be employed in this model. First,it remove all duplicate words such that each sentence contains only one distinct word before concatenating them into the single big document. Second deal with negation during text normalization. Typically adding negation prefix such as NOT_ to every word. i.e., like -&gt; Not_like. In some situations we might have insufficient labeled training data. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are pre-annotated with positive or negative sentiment. Naive Bayes as a Language ModelIf we use only individual word features, and we use all of the words in the text(not a subset), then naive Bayes has an important similarity to language modeling.Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model. contingency table consists of 4 entries: True Positive, True Negative, False Positive, False Negative. Accuracy doesn’t work well whenthe classes are unbalanced. Precision measures the percentage of the items that the system detected that are in fact positive. In other words, precision measures the percentage that the system makes correct classfication over total samples that are classified as correct. Precision is defined as$$ Precision =\frac{true\ positives}{true\ positives + false\ positives}$$ Recall measures the percentage of items actually present in the input that werecorrectly identified by the system. In other words, recall measures the percentage that the system makes correct classfication over total correct samples. Recall is defined as$$Recall =\frac{true\ positives}{true\ positives + false\ negatives}$$ precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.In practice, we generally combine precision and recall into a single metric calledF-measure the F-measure that is defined as:$$F_\beta =\frac{(\beta^2 +1)PR}{\beta^2P+R}$$ The β parameter differentially weights the importance of recall and precision. Values of β &gt; 1 favor recall, while values of β &lt; 1 favor precision. When β = 1, precision and recall are equally balF1anced; this is the most frequently used metric, and is called \(F_{\beta}=1\) or just \(F_1\).F-measure comes from a weighted harmonic mean of precision and recall. Harmonic mean is used because it is a conservative metric; the harmonic mean oftwo values is closer to the minimum of the two values than the arithmetic mean is.Thus it weighs the lower of the two numbers more heavily. In any-of or multi-label classification, each document or item can be assigned more than one label. Solve any-of classification by building separate binary classifiers for each class c. Given a test document or item d, then each classifier makes their decision independently, and we may assign multiple labels to d. one-of or multinomial classification, multinomial classification in which the classes are mutually exclusive and each document or item appears inexactly one class. Build a separate binary classifier trained on positiveexamples from c and negative examples from all other classes. Now given a testdocument or item d, we run all the classifiers and choose the label from the classifier with the highest score. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single contingency table, and compute precision and recall from that table(sum the number of decision like true for all classfiers and divide by total number of decision). A microaverage is dominated by the more frequent class, since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important. The only problem with cross-validation is that because all the data is used fortesting, we need the whole corpus to be blind; we can’t examine any of the datato suggest possible features and in general see what’s going on. But looking at thecorpus is often important for designing the system. For this reason, it is commonto create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set. Toy Model123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200import nltkimport refrom nltk.corpus import stopwordsfrom nltk.tokenize import RegexpTokenizerfrom utils import *import numpy as npimport copyimport randomdef processing_raw_text(sentence): intermediate = sentence.split('\t') try: sen, label = intermediate[0].strip(), intermediate[1] except IndexError: return (None,None) # print("sen", sen) # print(label) return sen, labeldef processing_sentence(sentence, tokenizer=None, stop_words=True, punctuation=True, stem=False): '''Given tokenizer, please deal with punctuation yourself''' if tokenizer is None: if punctuation == True: tokenizer = RegexpTokenizer(r'\w+') token = tokenizer.tokenize(sentence) else: token = nltk.word_tokenize(sentence) else: token = tokenizer.tokenize(sentence) if stop_words == True: sw_l = set(stopwords.words('english')) filtered_token = [w for w in token if not w in sw_l] else: filtered_token = token if stem == True: porter = nltk.PorterStemmer() filtered_token = [porter.stem(t) for t in filtered_token] return filtered_tokendef calculate_class_probability(label_l): label_distinct = set(label_l) d = &#123;key: label_l.count(key) for key in label_distinct&#125; d = &#123;key: d[key] / float(sum(d.values())) for key in label_distinct&#125; return ddef predict(test_example, class_p, word_p): '''predict''' predict_l = [] label_distinct = set(label_l) for i in range(len(test_example)): p_example = &#123;key: class_p[key] for key in class_p.keys()&#125; for w in test_example[i]: for lab in label_distinct: try: p_example[lab] += np.log(word_p[lab][w]) except KeyError: p_example[lab] += np.log(word_p[lab]['unknown']) # p_example[lab] = float("inf") predict_l.append(max(p_example, key=p_example.get)) return predict_ldef process_dic_unknown(dic, percent=20): #The lowest 20 percent words are regarded as unknown words. v_l = list(dic.values()) v_l.sort() i = int(len(v_l)*percent/100) unknown_count = 0 d = copy.deepcopy(dic) for k in dic.keys(): if dic[k] &lt;= v_l[i]: unknown_count += dic[k] del d[k] d['unknown'] = unknown_count return d # return ddef add_k_smoothing(dic, k=1): d = dict(map(lambda i: (i[0],(i[1]+k)/float(len(dic.keys())*k+sum(dic.values())) ) ,dic.items())) # print(d) return ddef build_labeled_vocab(example_l, label_l, dic, sort=True, smooth=False, deal_with_unknown = True): label_distinct = set(label_l) label_index = &#123;key: [-1] for key in label_distinct&#125; for i in range(len(label_l)): label_index[label_l[i]].append(i) label_index = &#123;k: v[1:] for k, v in label_index.items()&#125; dic = copy.deepcopy(dic) if deal_with_unknown == True: dic = copy.deepcopy(process_dic_unknown(dic)) d_labeled = &#123;&#125; for lab in label_distinct: d = dic.fromkeys(dic.keys(), 0) for ind in label_index[lab]: for w in example_l[ind]: if w in d.keys(): d[w] += 1 else: d[w] = 1 if sort==True: d_sorted = &#123;&#125; # sort the key for key in sorted(d.keys()): d_sorted[key] = d[key] d_labeled[lab] = d_sorted else: d_labeled[lab] = d if isinstance(smooth,int): d_labeled = dict(map(lambda i: (i[0], add_k_smoothing(i[1], smooth)),d_labeled.items())) return d_labeleddef calculate_conditional_probability(dic): '''calculate p(w|c)''' label_distinct = dic.keys() p_dic = &#123;key: &#123;k: dic[key][k] / float(sum(dic[key].values())) for k in dic[key].keys()&#125; for key in label_distinct&#125; return p_dicdef read_data(file_l): sen_l = [] label_l = [] for file in file_l: # print(file) with open(file,'r') as f: sen = 1 #dummy number while sen != '': sen = f.readline().strip() # print(sen) # print(sen) s,lab = processing_raw_text(sen) if s is None: continue sen_l.append(s) label_l.append(lab) # print(l) # print(len(sen_l)) return sen_l, label_ldef build_train_test_set(example, label, percent=0.2): # % of data as test set train_example = copy.deepcopy(example) train_label = copy.deepcopy(label) k = int(percent * len(example)) # print(len(example)) index = random.sample(range(len(example)), k) test_example, test_label = [], [] for i in index: test_example.append(example[i]) test_label.append(label[i]) index.sort(reverse=True) for i in index: train_example.pop(i) train_label.pop(i) return train_example, train_label, test_example, test_labelif __name__ == '__main__': file = '/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/imdb_labelled.txt' file1 = '/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/amazon_cells_labelled.txt' file2 = '/Users/lmk/Documents/NLP/Datasets/sentiment labelled sentences/yelp_labelled.txt' f_l = [file,file1,file2] sen_l, label_l = read_data(f_l) token_l = [processing_sentence(s) for s in sen_l] # filtered_token = list(map(lambda t: [w for w in t if not w in stop_words], token_l)) # print(token_l) '''build training set''' # train_example = token_l[:800] # train_label = label_l[:800] # test_example = token_l[800:] # test_label = label_l[800:] accu_l = [] num_iterations = 15 for j in range(num_iterations): train_example, train_label, test_example, test_label = build_train_test_set(token_l, label_l) all_words_dic = build_vocab(train_example) # print(all_words_dic) # print(process_dic_unknown(all_words_dic)) label_dic = build_labeled_vocab(train_example, train_label, all_words_dic, smooth=1) class_p = calculate_class_probability(train_label) #Propability of each class word_p = calculate_conditional_probability(label_dic) predict_l = predict(test_example, class_p, word_p) # print(predict_l) # print(test_label) accuracy = np.sum(np.array(test_label) == np.array(predict_l))/len(test_label) accu_l.append(accuracy) # print(accuracy) print("average accuracy: ", sum(accu_l)/num_iterations) without smoothing and dealing with unknown word: 0.555 without smoothing but dealing with unknown word: 0.715 add-1 smoothing and dealing with unknown word: 0.82 add-3 smoothing and dealing with unknown word: 0.785 add-5 smoothing and dealing with unknown word: 0.765 stem average accuracy: 0.784888888888889 without stem average accuracy: 0.7562222222222222 Bayesian networksBayesian networks represent the conditional dependence in a compat way. In real life, absolute dependence is not true most time. Bayesian networks is a Directed Acyclic Graph(DAG) that model local conditional dependence. It joint probability distribution, which turns conditional probability distribution to joint probability distribution. Bayesian networks is assembled by causality although causality is not necessary numerically in bayesian networks. Bayesian networks that reflect causality would be simpler and easier to think about. In Bayesian networks, each variable is a node. Edges between nodes are directed, which represent dependence. To calculate probability, arrage the netword properly s.t. \(p(x_1,…x_n) = p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)….p(x_n|x_1,x_2….x_{n-1})\). InferenceBy EmunerationEmunerate the variables and their states, ioin the variables to build a huge table. i.e.: N variables each with d states, the table entries would be \(d^N\). By Variable EliminationJoin the variables and sum out the probability to eliminate the variable immediately. Do it iteratively to avoid building a big table. i.e.: \(p(x_1,…x_n)p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)….p(x_n|x_1,x_2….x_{n-1})\). First join x1 to x2 to get \(p(x_2,x_1)\), and then sum out x1 to get \(p(x_2)\). Do it iteratively to get \(p(x_n)\). Finding the optimal ordering is a NP problem. Reference统计学习方法]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Noisy_Channel]]></title>
    <url>%2F2018%2F06%2F27%2FNoisy-Channel%2F</url>
    <content type="text"><![CDATA[Noisy Channel ModelThe intuition of the noisy channel model is to treat the misspelled word as if a correctly spelled word had been “distorted” by being passed through a noisy communication channel. $$\hat{w} = \operatorname{arg\,max}_{w\in C} p(x|w)p(w)$$ C is the list of candidate words. Above is a kind of Bayesian inference. We see an observation x (a misspelled word) and find the word w that generated this misspelled word. Out of all possible words in the vocabulary V or just a list of candidate word C, we want to find theword w such that P(w|x) is highest. Channel Model p(x|w)It’s expensive to get a perfect channel model. But a pretty reasonable estimateof P(x|w) could be generated just by looking at local context: the identity of the correct letter itself, themisspelling, and the surrounding letters. confusion matrixContains counts of errors. In general, each entry in a confusion matrix represents the number of times one thing was confused with another. Thus for example a substitution confusion matrix will be a square matrix of size 26×26 (or more generally |A| × |A|,for an alphabet A) that represents the number of times one letter was incorrectlyused instead of another. del[x, y]: count(xy typed as x) ins[x, y]: count(x typed as xy) sub[x, y]: count(x typed as y) trans[x, y]: count(xy typed as yx) Methods to get confusion matrix: 1, extract them from lists of misspellings $$p(x|w) = \frac{ins[x_{i-1},w_i]}{count[w_{i-1}]},\quad \text{if insertion}$$ $$p(x|w) = \frac{sub[x_{i},w_i]}{count[w_{i}]},\quad \text{if substitution}$$ The probability of substitution is calculated by:given the correct word w and error word x, the number of times that ith character wi and the ith character xi is substituted divided by total number of ith character wi.wi is the ith character of the correct word w and xi is the ith character of the typo x 2, EM algorithm.compute the matrices by iteratively using the spelling error correction algorithm itself. The iterative algorithm first initializes the matrices with equal values; thus, any character is equally likely to be deleted, equally likely to be substituted for any other character,etc. Next, the spelling error correction algorithm is run on a set of spellingerrors. Given the set of typos paired with their predicted corrections, the confusionmatrices can now be recomputed, the spelling algorithm run again, and so on. Evaluating spell correction algorithms is generally done by holding out a training,development and test set from lists of errors like those on the Norvig and Mittonsites mentioned above. Candidate words:Utilize the extended minimum edit distance algorithm introduced so that in addition to insertions, deletions, and substitutions, there is transpositions, in which two letters are swapped. All words with edit distance of 1 constitutes the candidate words set. For words with specific context, it is important to use larger language models than unigrams such that the pobability p(w) is extended to \(p(w_{i-1})*p(w_i)\) for bigram model to take the context into consideration. Real-word spelling errorsNoisy channel algorithm could also be applied. The algorithm takes the input sentence \(X = {x_1, x_2,…, x_k,…, x_n}\), generates a large set of candidate correction sentences C(X),then picks the sentence with the highest language model probability. To generate candidate set C(X), each word is assigned the probability that would make an error. And score each sentence like the aforementioned way. The difference is that the probability that the word is indeed correct needs to be considered. Given a word w, let’s assume the correct probability is \(\alpha\). And the error probability is \(\frac{1-\alpha}{|C(x)|} \) if \(x \in C(x)\). The others are totally the same as normal noisy channel model for wrong words.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NLP_Notation]]></title>
    <url>%2F2018%2F06%2F26%2FNLP-Notation%2F</url>
    <content type="text"><![CDATA[Language models assign a probability to each sentence \( w_1^n\) is a sequence of N words from word 1 to word N. A held-out corpus is an additional training corpus that we use to set hyperparameters. Non-word spelling correction is the detection and correction of spelling errors that result in non-words (like graffe for giraffe). real word spelling correction is the task of detecting and correcting spelling errors even if they accidentally result in an actual word of English (real-word errors). edit probability The probability of typo like deletion, insertion respectively. text categorization: classifying an entire text by assigning it a textcategorization label drawn from some set of labels. classification: take a single observation, extract some usefulfeatures, and thereby classify the observation into one of a set of discrete classes. Generative classifiers like naive Bayes build a model of each class. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes.While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role. bag-of-words: an unordered set of words with their position ignored, keeping only their frequency in the document. linear classifiers: use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression. period disambiguation: deciding if a period is the end of a sentence or partof a word co-occurrence matrix: a way of representing how often words co-occur. term-document matrix: each row represents a word in the vocabulary andeach column represents a document. vector space model:a document is represented as a count vector(column vector), each entry is the count of a specific word. Information retrieval(IR): is the task of finding the document d from the Ddocuments in some collection that best matches a query q which is also a vector of length |V|. term-context matrix: a |V| × |V| matrix. Each entry(i,j) represent that the number of times a word i shows in the context word j. The context could be a document or a context window of length |L|. WordNet(Sense-Tagged data):WordNet is a large lexical database of English. Nouns, verbs, adjectives and adverbs are grouped into sets of synonyms , each expressing a distinct concept. part-of-speech tags: CC Coordinating conjunction CD Cardinal number DT Determiner EX Existential there FW Foreign word IN Preposition or subordinating conjunction JJ Adjective JJR Adjective, comparative JJS Adjective, superlative LS List item marker MD Modal NN Noun, singular or mass NNS Noun, plural NNP Proper noun, singular NNPS Proper noun, plural PDT Predeterminer POS Possessive ending PRP Personal pronoun PRP$ Possessive pronoun RB Adverb RBR Adverb, comparative RBS Adverb, superlative RP Particle SYM Symbol TO to UH Interjection VB Verb, base form VBD Verb, past tense VBG Verb, gerund or present participle VBN Verb, past participle VBP Verb, non-3rd person singular present VBZ Verb, 3rd person singular present WDT Wh-determiner WP Wh-pronoun WP$ Possessive wh-pronoun WRB Wh-adverb Lemmatization is the task of determining that two words have the same root, despite their surface differences. Usually including plural to singular, but the word form such as ing, ed would not change. This naive version of morphological analysis is called stemming. Stemming usually includes turning a word to its morpheme. Two broad classes of morphemes: stems: the central morpheme of the word, supplying the main meaning. i.e.: girls, girl and s are the morphemes. The central morpheme is girl. affixes: adding “additional”meanings of various kinds. i.e.: homegrown, ‘home’ is added before the grown. unkind: ‘un’ is added before the word kind. concept drift – the gradual change over time of the con- cept underlying a class like US president from Bill Clinton to George W. Bush]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Smoothing]]></title>
    <url>%2F2018%2F06%2F26%2FSmoothing%2F</url>
    <content type="text"><![CDATA[Smoothing:basically discount the observed words to assign to words with zero count so as to deal with the zero probability words. Laplace SmoothingAlso called add-one smoothing. the unsmoothed maximum likelihood estimate of the unigram probability of the word wi is its count ci normalized by the total number of word tokens N $$ p(w_i)= \frac{c_i}{N} $$ Laplace smoothing merely adds one to each count. Denominator is also adjusted by adding V(number of observations). If the demoninator is still unchanged, the sum of all probability would exceed 1. $$ p_{Laplace}(w_i)= \frac{c_i+1}{N+V} $$ adjusted count: describe how a smoothing algorithm affects the numerator $$ c_i^* = \frac{c_i+1}{N+V} \times{N} = p_{Laplace}(w_i) \times{N} $$ the ratio of the discounted counts to the original counts describe a smoothing algorithm in terms of a relative discount dc: $$ d_c= \frac{c^*}{c} $$ Normal bigram probabilities are computed by normalizing each row of counts by the unigram count: $$ p(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)}{c(w_{n-1})} $$ After smoothing: $$ p_{Laplace}(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)+1}{c(w_{n-1})+V} $$ These adjusted counts can be computed by equation below to show the reconstructed counts. $$ c^*(w_{n-1}w_n) = \frac{c(w_{n-1}w_n)+1}{c(w_{n-1)}+V} \times{c(w_{n-1})} = p_{Laplace}(w_n|w_{n-1}) \times{c(w_{n-1})} $$ Add-k smoothing:add k rather than one; $$ p_{Add-k}^*(w_n|w_{n-1})= \frac{c(w_{n-1}w_n)+k}{c(w_{n-1})+kV} $$ chosse k by optimizing on a devset. Although add-k is useful for some tasks (including text classification), it turns out that it still doesn’t work well for language modeling, generating counts with poor variances and often inappropriate discounts Backoff and Interpolationbackoff: If there is zero evidence for a higher-order N-gram, use lower-order N-gram instead. i.e.: If we are trying to compute $$ P(w_n|w_{n−2}w_{n−1}) $$ but we have no examples of a particular trigram wn−2wn−1wn, we can instead estimate its probability by using the bigram probability $$ P(w_n|w_{n−1}) $$. interpolation: we always mix the probability estimates from all the N-gram estimators, weighing and combining the trigram, bigram, and unigram counts. In a slightly more sophisticated version of linear interpolation, each λ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the λs for those trigrams higher and thus give that trigram more weight in the interpolation. $$ \hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 (w_{n-2}^{n-1})p(w_n|w_{n-2}w_{n-1}) + \lambda_2 (w_{n-2}^{n-1})p(w_n|w_{n-1}) +\lambda_3 (w_{n-2}^{n-1})p(w_n) $$ simple interpolation: $$ \hat{p}(w_n|w_{n-2}w_{n-1}) = \lambda_1 p(w_n|w_{n-2}w_{n-1}) + \lambda_2 p(w_n|w_{n-1}) +\lambda_3 p(w_n) $$ In both kinds of interpolation: $$ \sum_i \lambda_i = 1 $$ Both the simple interpolation and conditional interpolation λ are learned from a held-out corpus. choose the λ values that maximize the likelihood of the held-out corpus.There are various ways to find this optimal set of λs. One way is to use the EM algorithm, which is an iterative learning algorithm that converges on locally optimal λs In terms of quality of N-gram model: quadrigram &gt; trigram &gt; bigram &gt; unigram In order for a backoff model to give a correct probability distribution, we have to discount the higher-order N-grams to save some probability mass for the lower order N-grams. In addition to this explicit discount factor, we’ll need a function α to distribute this probability mass to the lower order N-grams. This kind of backoff with discounting is also called Katz backoff. In Katz backoff we rely on a discounted probability \(P^∗\) if we’ve seen this N-gram before (i.e., if we have non-zero counts). Otherwise, we recursively back off to the Katz probability for the shorter-history (N-1)-gram. The probability for a backoff N-gram PBO is thus computed as follows: $$\begin{cases} P^*(w_n|w_{n-N+1}^{n-1})\quad \text{if}\;C(w_{n-N+1}^{n-1}) &gt; 0 \ \alpha(w_{n-N+1}^{n-1})P_{BO}(w_n|w_{n-N+2}^{n-1})\quad \text{otherwise} \end{cases} $$ Katz backoff is often combined with a smoothing method called Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computationfor estimating the Good-Turing smoothing and the \(P^*\) and \(\alpha\) values. Kneser-Ney SmoothingIntuition: Discount the probability from N gram model and assign it to phrase so as to take different word combinations into consideration. Simple example:Finance in _ is promissing.Here you can just simply fill in a word with high probability like USA, Germany. With Kneser-Ney Smoothing, taking word combinations into consideration, it might fill in world finance centre like London and HongKong because they often appear together. Novel continuation means the first time that the word \(w_{i-1}\) preceeds the word \(w_i\). The number of times a word w appears as a novel continuation can be expressed as: $$P_{continuation}(w_i)\propto \lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert$$ \(w_i\) is the current word. \(w_{i-1}\) is the words occur before the current word(also called preceeding word). To turn this count into a probability, we normalize by the total number of bigram word. In summary: $$P_{continuation}(w_i)=\frac{\lvert {w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert}{\lvert{(w_{j-1},w_j):c(w_{j-1}w_j)&gt;0}\rvert}$$ Or normalized by the number of words preceding all words as follows: $$P_{continuation}(w_i)=\frac{\lvert { w_{i-1}:C(w_{i-1}w_i)&gt;0}\rvert}{\sum_{w’i} \lvert {w’{i-1}:C(w’_{i-1}w’_i)&gt;0}\rvert}$$ The number of words preceeding all words is:given word \({w’}i\),the number of word \(w’{i-1}\) that preceed \({w’}_i\).\({w’}_i\) are all words in the corpus. So The number of words preceeding all words = the total number of bigram word. The final equation for Interpolated Kneser-Ney smoothing for bigrams is then: $$P_{KN}(w_i|w_{i-1})=\frac{max(C(w_{i-1}w_i)-d,0)}{C(w_{i-1})}+\lambda(w_{i-1})P_{continuation}(w_i)$$ The λ is a normalizing constant that is used to distribute the probability mass that is discounted: $$\lambda(w_{i-1})=\frac{d}{C(w_{i-1})}\cdot \lvert {w:C(w_{i-1},w)&gt;0}\rvert$$ The first term \(\frac{d}{C(w_{i-1})}\) is the normalized discount. The second term \(\lvert {w:C(w_{i-1},w)&gt;0}\rvert\)is the number of word types that can follow \(w_{i−1}\) or, equivalently, the number of word types that we discounted; in other words, the number of times we applied the normalized discount. The general recursive formulation is as follows: $$P_{KN}(w_i|w_{i-n+1}\cdots w_{i-1})=\frac{max(0,C_{KN}(w_{i-n+1} \cdots w_i) - d)}{C_{KN}(w_{i-n+1}\cdots w_{i-1})}+\lambda(w_{i-n+1}\cdots w_{i-1})\cdot P_{KN}(w_i|w_{i-n+2}\cdots w_{i-1})$$ $$\lambda(w_{i-n+1}\cdots w_{i-1})=\frac{d}{C_{KN}(w_{i-n+1}\cdots w_{i-1})}\cdot \lvert {w:C_{KN}(w_{i-n+1} \cdots w_{i-1}w)&gt;0}\rvert$$ where the definition of the count \(c_{KN}\) depends on whether we are counting the highest-order N-gram being interpolated (for example trigram if we are interpolat- ing trigram, bigram, and unigram) or one of the lower-order N-grams (bigram or unigram if we are interpolating trigram, bigram, and unigram): $$C_{KN}(\cdot)=\begin{cases}count(\cdot) &amp;,\text{for the highest order}\continuationcount(\cdot) &amp;,\text{for all other lower orders}\end{cases}$$ The continuation count is the number of unique single word contexts for ·.At the termination of the recursion, unigrams are interpolated with the uniform distribution, where the parameter ε is the empty string: $$P_{KN}(w)=\frac{max(c_{KN}(w)-d,0)}{\sum_{w’}c_{KN}(w’)} +\lambda(\epsilon)\frac{1}{V}$$ If we want to include an unknown word , it’s just included as a regular vo-cabulary entry with count zero, and hence its probability will be a lambda-weighteduniform distribution \(\frac{\lambda(\epsilon)}{V}\). The best-performing version of Kneser-Ney smoothing is called modified Kneser- Ney smoothing. Rather than use a single fixed discount d, modified Kneser-Ney uses three different discounts d1, d2, and \(d3_+\) for N-grams with counts of 1, 2 and three or more, respectively. EntropyEntropy is the minimum number of bits(if log base 2 is used) that is needed to encode(represent) information in the optimal coding scheme. Entropy rate the entropy of a sequence divided by the number of words. To measure entropy rate of a language, entropy rate of a infinite sequence needs to be calculated. $$H(L) = −\lim_{n\to\infty} \frac{1}{n}H(w_1,w_2,…,w_n)= −\lim_{n\to\infty}\sum_{W\in L} p(w_1,…,w_n)\frac{1}{n} \log p(w_1,…,w_n)$$ The Shannon-McMillan-Breiman theorem states that if the language is rboth stationary and ergodic $$H(L) = \lim_{n\to\infty}-\frac{1}{n}\log p(w_1,…,w_n)$$ The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.That is, we can take a single sequence that is long enough instead of summing over all possible sequences. A stochastic process is said to be stationary if the probability distribution for words at time t is the same as the probability distribution at time t + 1. Markov models, and hence N-grams, are stationary. Natural language is not stationary, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability. The cross-entropy is useful when we don’t know the actual probability distribution p that generated some data. It allows us to use some m, which is a model of p (i.e., an approximation to p) to approximate the real entropy. Thecross-entropy of m on p is defined by $$H(p,m) = \lim_{n\to\infty}\sum_{W\in L}- p(w_1,…,w_n)\frac{1}{n} \log m(w_1,…,w_n)$$ That is, we draw sequences according to the probability distribution p, but sum the log of their probabilities according to m.Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process: $$H(p,m) = \lim_{n\to\infty}-\frac{1}{n}\log m(w_1,…,w_n)$$ This means that, as for entropy, we can estimate the cross-entropy of a model m on some distribution p by taking a single sequence that is long enough instead of summing over all possible sequences.What makes the cross-entropy useful is that the cross-entropy H(p,m) is an upper bound on the entropy H(p).For any model m: $$H(p) ≤ H(p,m)$$ This means that we can use some simplified model m to estimate the true entropy of a sequence of symbols drawn according to probability p. The more accurate m is, the closer the cross-entropy H(p,m) will be to the true entropy H(p). Thus, the difference between H(p,m) and H(p) is a measure of how accurate a model is. The lower the cross-entropy, the better is the model(better approximation of the true entropy). The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy. Cross-entropy is defined in the limit, as the length of the observed word sequence goes to infinity. A (sufficiently long) sequence of fixed length N is employed to approximate the cross-entropy of a model M = $ P(w_i|w_{i−N+1}…w_{i−1}) $(N is the N of a N-gram model) on a sequence of words W is $$H(W) = -\frac{1}{N}\log p(w_1,…,w_n)$$ The perplexity of a model P on a sequence of words W is now formally defined as the exp of this cross-entropy: $$Perplexity(W) = 2^{H(W)} = P(w_1,…,w_n)^{-\frac{1}{N}}$$]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Max_Matching]]></title>
    <url>%2F2018%2F06%2F25%2FMax-Matching%2F</url>
    <content type="text"><![CDATA[Max Matching AlgorithmIt is a kind of greedy algorithm used to tokenize sentences. It is a baseline of tokenize algorithms. It works especially well on Chinese. I think it is because that Chinese words are single word that could not be took apart while English words are sequences of characters. 12345678910111213141516171819202122232425262728293031323334353637def read_words(file): with open(file) as f: l = f.readlines() return ldef max_matching(text, dictionary): # l = [] w = '' for i in range(len(text),0,-1): word = text[:i] if word in dictionary: w += word+' ' w += max_matching(text[i:],dictionary) + ' ' return w if len(text) != 0: w = text[0]+' ' w += max_matching(text[1:],dictionary) + ' ' return w else: return ''if __name__ == "__main__": file = '/Users/lmk/Documents/NLP/word.csv' l = read_words(file) # l = read_words(file) l = list(map(lambda i: i[3:-1], l)) l[0] = l[0][1:] dic = l # s = "ilovechina" s = "is also possible to use a list as a queue, where the first" sentence = max_matching(s,dic) token = sentence.split() # flatten = lambda l: [item for sublist in l for item in sentence] # flat_list = [item for sublist in l for item in sublist] print(sentence) print(token)]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Minimum_Edit_Distance]]></title>
    <url>%2F2018%2F06%2F25%2FMinimum-Edit-Distance%2F</url>
    <content type="text"><![CDATA[Minimum Edit Distanceminimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another. It’s useful in terms of portential splleing checking and string alignment which is significant in terms of machine translation and speech recognition. 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npclass Solution: def insertion_cost(self,c1): '''customize your insertion cost''' return 1 def deletion_cost(self,c1): '''customize your deletion cost''' return 1 def substitution_cost(self,c1,c2): '''customize your substitution cost''' if c1 == c2: return 0 return 1 def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ n = len(word1)+1 #row of matrix m = len(word2)+1 #column of matrix l = np.zeros((m,n)) # l = [n*[0] for i in range(m)] for i in range(m): l[i,0] = i for i in range(n): l[0,i] = i # print(l) for i in range(1,m): for j in range(1,n): # print(i,j) l[i][j] = min(l[i-1][j]+self.insertion_cost(word2[i-1]), l[i][j-1]+self.deletion_cost(word1[j-1]), l[i-1][j-1]+self.substitution_cost(word1[j-1],word2[i-1])) # l = [list(range(n)) for i in range(m)] # print(l) return l[-1][-1] minimum edit distance with backtrace123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990import numpy as npclass Edit_Distance: def __init__(self): self.back_trace = None self.n = 0 self.m = 0 self.l = None def insertion_cost(self,c1): '''customize your insertion cost''' return 1 def deletion_cost(self,c1): '''customize your deletion cost''' return 1 def substitution_cost(self,c1,c2): '''customize your substitution cost''' if c1 == c2: return 0 return 2 def minDistance(self, word1, word2): """ :type word1: str :type word2: str :rtype: int """ n = len(word1)+1 #row of matrix m = len(word2)+1 #column of matrix self.n = n self.m = m l = np.zeros((m,n)) # back_trace = np.zeros((m,n)) back_trace = [n*[0] for i in range(m)] for i in range(m): l[i,0] = i for i in range(n): l[0,i] = i # print(l) for i in range(1,m): for j in range(1,n): # print(i,j) cost = [l[i-1][j]+self.insertion_cost(word2[i-1]), l[i][j-1]+self.deletion_cost(word1[j-1]), l[i-1][j-1]+self.substitution_cost(word1[j-1],word2[i-1])] min_cost = min(cost) print((i,j),min_cost) back_trace[i][j] = [i for i,j in enumerate(cost) if j == min_cost] l[i][j] = min_cost # l = [list(range(n)) for i in range(m)] # print(l) # print(back_trace) self.l = l self.back_trace = back_trace return l[-1][-1] # def print_direction(self,l): # if def output_back_trace(self): i = self.n-1 j = self.m-1 while i &gt;= 0 and j &gt;= 0: # print((i,j)) direction_list = self.back_trace[i][j] #choose the last direction, here every direction has the same cost print(direction_list) if isinstance(direction_list,list): direction = direction_list[0] else: direction = direction_list if direction == 2: i -= 1 j -= 1 elif direction == 1: j -= 1 else: i -= 1 # print(self.l[i][j])s = Edit_Distance()# word1 = "intention"# word2 = "execution"word1 = "execution"word2 = "intention"leg = s.minDistance(word1,word2)print(leg)s.output_back_trace()print(s.l)]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Port_Stemmer]]></title>
    <url>%2F2018%2F06%2F25%2FPort-Stemmer%2F</url>
    <content type="text"><![CDATA[LemmatizationLemmatization is the task to determine whether two words have the same root. If proper lemmatization has been applied, number of tokens could be reduced significantly and understanding of the text could also be facilitated. Problem of suffix strippingprobe and probatewand and wanderSometimes removal of suffix could affect the original meaning and degrade the performance.]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sort]]></title>
    <url>%2F2018%2F06%2F07%2FSort%2F</url>
    <content type="text"><![CDATA[Quick SortPartition the array into two parts such that elements in part a &lt; that of part b. Recursively invoke quick sort to do in-place sorting. Average performance is the best in practice. O(nlogn). The worst case happens when the array is inversely sorted, which leads the subarray after partition is extremely imbalanced. Time complexity is O(n^2). Merge SortDivide and conquer. Divide the array into two parts and then merge recursively. The process of sorting is just like building a binary tree in a top-down manner and then merge the array bottom-up. Heap Sortbuild a max heap so that the first element in the heap is always the largest. Sorting is just switching the largest value with the last element in the array and maintain a heap. after poping the element, reduce the heap size by one. ##PerformanceQucik sort is the fastest. After it is merge sort. And then heap sort. build a heap takes considerable time. Buble sort and insertion sort are far slower.Random an array of length 1000000, quick sort took 5.30535101890564 seconds and merge sort took 5.918827056884766 seconds. Their perfomance are quite good and close to each other. Heap sort took 13.471954822540283 seconds. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223class Sort(): def __init__(self, l = None): if l is not None: assert type(l) == list,("please input a list") else: l = self._generate() self.l = l self.result = None def insert_sort(self): l = self.l #add new reference to self.l, change l would change self.l for i in range(1,len(l)): #use built-in methods to insert for j in range(i-1,-1,-1): if l[i] &lt; l[j]: if j == 0: l.insert(j,l[i]) del(l[i+1]) elif l[i] &gt;= l[j-1]: l.insert(j, l[i]) del (l[i + 1]) def insert_sort_v1(self): #much slower than using built-in method l = self.l #add new reference to self.l, change l would change self.l for i in range(1,len(l)): #switch position for j in range(i-1,-1,-1): if l[j+1] &lt; l[j]: l[j], l[j+1] = l[j+1], l[j] def bubble_sort(self): l = self.l for i in range(0,len(l)-1): for j in range(0,len(l)-i-1): if l[j] &gt; l[j+1]: l[j], l[j+1] = l[j+1], l[j] def quick_sort(self, p=None, r=None): '''extra space usage''' '''divide the original into two parts such that elements in part a &lt; that of part b''' if p is None or r is None: p = 0 r = len(self.l) # print("p ",p) # print("r ", r) if p&lt;r-1: q = self._quick_sort_partition(p, r) # print("q ", q) self.quick_sort(p,q) self.quick_sort(q,r) # self._quick_sort_partition(p, q) # self._quick_sort_partition(q, r) # q = self._quick_sort_partition(p, r) # print(q) # while True: # if self._quick_sort_partition(p, q) == 0: # break # while True: # if self._quick_sort_partition(q, r) == r-1: # break # q = self._quick_sort_partition(0,len(self.l)) # self.quick_sort(p,q) # self.quick_sort(q,r) # else: # q = self._quick_sort_partition(p, r) # self.quick_sort(p, q - 1) # self.quick_sort(q, r) def _quick_sort_partition(self, p, r): # l = list(self.l) l = self.l i = p-1 pivot = l[r-1] for j in range(p,r-1): #in-place exchange without memory overhead if l[j] &lt;= pivot: i += 1 l[i], l[j] = l[j], l[i] # print(i) # print(l) l[i+1], l[r-1] = l[r-1], l[i+1] # print(l) # print(self.l) # print("i ",i) return i+1 def merge_sort(self, p=None, r=None): l = self.l if p is None or r is None: p = 0 r = len(l) #not include r if p &lt; r-1: q = int((p+r)/2) self.merge_sort(p,q) self.merge_sort(q, r) self.merge(p,q,r) def merge(self, p,q,r): l = self.l L = l[p:q] R = l[q:r] L.append(float('inf')) R.append(float('inf')) i,j = 0,0 for k in range(p,r): #r = len(list), not include r if L[i] &lt; R[j]: l[k] = L[i] i+=1 else: l[k] = R[j] j += 1 def merge_two_lists(self,l1,l2): assert l1 is None or l2 is None,("empty list") l = [] i = 0 j = 0 while i&lt;len(l1) and j&lt;len(l2): if l1[i] &lt; l2[j]: l.append(l1[i]) i+=1 else: l.append(l2[j]) j += 1 if i == len(l1): l.extend(l2[j:]) else: l.extend(l1[i:]) return l def heap_sort(self): l = self.l self.heap_size = len(l) self.build_heap() for i in range(len(l)-1,0,-1): #no need to include 0. change with itself is meaningless l[i], l[0] = l[0], l[i] self.heap_size -= 1 self.max_heaplify(0) def build_heap(self): #bottom up. the first level of the tree has no child, no need to maintain for i in range(int(self.heap_size/2),-1,-1): self.max_heaplify(i) def max_heaplify(self,i): l = self.l left = 2*i +1 right = 2*i +2 largest = i if left &lt; self.heap_size and l[left] &gt; l[i]: largest = left if right &lt; self.heap_size and l[right] &gt; l[largest]: largest = right if largest != i: '''if position changes, need to recursively run max_heaplify to guarantee the descendant are also max heap''' l[i], l[largest] = l[largest], l[i] self.max_heaplify(largest) def _evaluate(self): if self.result is None: self.result = self.l for i in range(1,len(self.result)): if self.result[i] &lt; self.result[i-1]: return False return True def _generate(self,length = None): try: import random if length is None: length = random.randint(0, 1000) # length = random.randint(0,100000) l = [] for i in range(length): l.append(random.randint(-length,length)) return l except ImportError: print("no random module, please input a sequence")l = [4,6,1,0,5,-9,0,5,]# l = [4,6,10,23,-1,-6]# s = Sort(l)# print(s._quick_sort_partition(0,len(l)))# print(s.l)s = Sort()# s.quick_sort()print("length ",len(s.l))s.heap_sort()# s.build_heap()# s.merge_sort()# s.insert_sort_v1()# s.insert_sort()print(s._evaluate())# s._quick_sort_partition(0,len(l))print(s.l)def compare_time(): import time print("**************compare time**************") s = Sort() l = s._generate(1000000) start_time = time.time() Sort(l).quick_sort() print("--- %s seconds ---" % (time.time() - start_time)) start_time = time.time() Sort(l).merge_sort() print('--- &#123;&#125; seconds ---'.format(time.time() - start_time)) start_time = time.time() Sort(l).heap_sort() print('--- &#123;&#125; seconds ---'.format(time.time() - start_time))compare_time() ###Reference: Introduction to Algorithms]]></content>
  </entry>
  <entry>
    <title><![CDATA[Remove_Duplicates_from_Sorted_List_II]]></title>
    <url>%2F2018%2F06%2F05%2FRemove-Duplicates-from-Sorted-List-II%2F</url>
    <content type="text"><![CDATA[##Remove Duplicates from Sorted List II: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293# Definition for singly-linked list.class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ node = head if head == None: return None if head.next == None: return head if head.next.val != head.val: #the first two elements are not the same current_node = node node = node.next while node != None: if node.next == None: current_node.next = node return head if node.next.val != node.val: current_node.next = node current_node = node else: # when next node is the same as the current node while node != None: if node.next == None: current_node.next = None return head else: node = node.next if node.next == None: current_node.next = None return head if node.val != node.next.val: break node = node.next else: current_node = None #the first two elements are same, pointers point to None while node != None: if node.next == None: if current_node == None: # all elements includes the first few elements are same return node #return None current_node.next = node #add the last node to the linked list return head if node.next.val != node.val: #current node is to record the last node in the linked list if current_node == None: #which is identical in the list current_node = node head = current_node else: current_node.next = node current_node = node else: # when next node is the same as the current node while node != None: # if the node is the same with next node if node.next == None: # this loop is to skip the node current_node.next = None return head else: node = node.next if node.next == None: if current_node == None: return None current_node.next = None return head if node.val != node.next.val: break node = node.nexts = Solution()l = [1,1,2]# l = [1,2,3,3,4,4,5,5]# l = [2,2,3,4,5,5]LN = ListNode(l[0])# for i in range(1,len(l)):# LN.nextnode_list = list(map(ListNode,l))# print(node_list[0])# print(ListNode(l[0]))for i in range(len(node_list)-1): node_list[i].next = node_list[i+1]h = s.deleteDuplicates(node_list[0])while h!= None: print(h.val) h = h.next This version uses three pointer to do the same thing. From leetcode. 123456789101112131415161718192021222324252627282930class ListNode: def __init__(self, x): self.val = x self.next = Noneclass Solution: def deleteDuplicates(self, head): """ :type head: ListNode :rtype: ListNode """ if not head or not head.next: return head fakehead = ListNode(0) fakehead.next = head prev = fakehead slow = head fast = head.next while fast: if fast.val == slow.val: while fast and fast.val == slow.val: fast = fast.next slow = prev else: prev = slow slow = slow.next slow.val = fast.val fast = fast.next slow.next = None return fakehead.next]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Sentence_Similarity]]></title>
    <url>%2F2018%2F05%2F27%2FSentence-Similarity%2F</url>
    <content type="text"><![CDATA[NLP typical Step:Tokenize: separate the words in the sentence Convert the sentence to vector: the sentence has been divided into several words, and load word2vec model and convert each word to a vector Similarity MeasurementTF-IDF model: convert the sentence to a vector of words, use cosine similarity to measure. In this applicatoin, not very good. Bag of words: Modern way: NLP step as above. Use pre-train word vectors to represent a sentence. Compute distance. Huge amount of computation. In this application, extract several keywords first as heuristic to rule out a lot of sentences and then start using this method to reduce computation amount. The best accuracy in this application. Change pre-train vectors and brute-force computation would increase the accuracy. Some NotesTokenize: separate the words in the sentence Bag of words: just count the frequency of words that appear in the sentence and neglect the sequence of words Word representation: one-hot representationNot easy to extract connection because dot product between any two words are 0 Featurized representation: word embedding. Using a matrix to represent values of features, and stand for a word. It could be understood as a word embedded in a high dimensional space Analogy of word embedding. Use cosine similarity to measure whether two words are similar. Take the dot product of two words vectors and divide multiplication of their lengths. Embedding matrix: num of features * number of words. Each column is a feature vector. Use one-hot vector to multiply embedding matrix get the corresponding feature vector Word2vec:Skip-gram: randomly choose a context word and corresponding target word. Target word is in the window(like 5 words before or after the context word) of context wordTarget is to learn good word embedding rather than perform good supervised learningHow to sample context word? Employ some techniques to avoid commonly appeared word like the ,a and so on Negative sampling: one positive sample and k randomly chosen negative sample to simplify computation Content of word d: means the range of plus or minus 5 words Sequence to sequence:Machine translation: input sentence to encoder, and output is a decoder. Conditional language model. Green part is encoder, purple is decoder Beam search: a kind of approximate optimization algorithmbeam width n: track The most possible n sequences with the highest probability. Larger n, the algorithm runs better but need more memory. Smaller n: more efficient, less accurateunlike BFS and DFS, beam search do not guarantee to find the best solution. It’s not complete because it only track a few states.Start by choosing the first word for n sentences, and then second……. Maximize this probability to find the most possible sequenceMultiplication of several values that are less than one leads to numerical problem. Take Log on it solve this problem. Error analysis: working to optimize some sort of cost function/objective function that is output by a learning algorithm like RNNRNN output the p(y|x) for sentences and beam search choose argmaxP(y|x). if RNN output the correct probability but beam search choose the wrong one, beam is at fault. And vise versa, if RNN output the wrong probability for sentences, RNN is at fault. Correct probability means P(correct sentence | x) &gt; P(wrong sentence | x)Compare beam search with RNN, find which one is at fault. And then modify and improve it. Attention model:Just focus on a few words around rather than the whole sentence to translate. Divide and conquer, get better result. Speech recognition:Turn time domain signal to frequency domain signalUsually input is much larger than output CTC cost:Basic idea: collapse repeated characters not separated by “blank” Normalizationputs pieces of information in a standard format, so that they can be compared to other pieces of information. An example of nor- malization is formatting dates and times in a stan- dard way, so that “12pm”, “noon” and “12.00h” all map to the same representation. ReferenceSpeech and Language Processing]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[String_Matching]]></title>
    <url>%2F2018%2F05%2F25%2FString-Matching%2F</url>
    <content type="text"><![CDATA[##String Matching: ###The Rabin-Karp algorithmIntuition:Use the values of strings as heuristic, if they are equal, they matches, else they don’t. Because the string may be very long to do the computation efficiently, we induce mod so as to reduce the computation complexity dramatically. In general, with a d-ary alphabet {0,1,2… d-1}, we choose q so that d*q fits within a computer word.d is the length of the alphabet set. 123456789101112131415161718192021'''please input pattern and string'''p = input()s = input()d = int(input())q = int(input())l_p = len(p)h = d**(l_p-1) % q'''Preprocessing'''p_sum, s_sum = 0,0for i in range(0,l_p): p_sum = (d*p_sum + int(p[i])) %q s_sum = (d * s_sum + int(s[i])) %q # p_sum = (p_sum+(d*p_sum + int(p[i]))) % q # s_sum = (s_sum+(d * s_sum + int(s[i]))) % qfor j in range(0,len(s)-l_p+1): if p_sum == s_sum: if s[j:j+l_p] == p: print(s[j:j+l_p]) if j &lt; len(s)-l_p: s_sum = (d*(s_sum-int(s[j])*h) + int(s[j+l_p])) % q Worst case: the string and pattern have all same character. O((n-m+1)*m)Best case: O(n) + O(m(v+n/q)) ###KMP algorithm Each time the given string match with the pattern, by utilizing the information contains in the pattern string, the algorithm may skip several matching without affecting the correctness. Let’s assume current position of the string is s. Length of matched string is q. There may exists a k&lt;q(if k &gt; q, we don’t know what’s them outside the pattern window) such that P[1…k] = T[s’+1…s’+k]. s’ is the new position of pointer. K is the length that the substring T[s’] starting from s’ matches with P[1..k].So s’ = s+q-k Preprocessing:Find the longest prefix of the matched string such that it is also the suffix of the matched string. Time complexity: O(n) 12345678910111213141516171819202122232425def compute_prefix(p): l = [0] * len(p) k = 0 for i in range(1,len(p)): '''#This line decrease k. because 0 &lt; k &lt; i, the total time that i has been increased &lt; len(p), so the total time that while loop execute would &lt; len(p)''' while k&gt;0 and p[i] != p[k]: k = l[k] if p[i] == p[k]: k += 1 # this increase k l[i] = k return ldef KMP_Match(s,p): l = compute_prefix(p) # print(l) q = 0 for i in range(0,len(s)): while q &gt; 0 and p[q] != s[i]: q = l[q] if p[q] == s[i]: q += 1 if q == len(p): print(s[i-q+1:i+1]) q = l[q-1]]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Recommendation_System]]></title>
    <url>%2F2018%2F05%2F24%2FRecommendation-System%2F</url>
    <content type="text"><![CDATA[Advertisement recommendation is important! It is a significant source of income for many companies. Abuse of advertisement would annoy user. How to choose potential interested user is a challenging problem. Our application is an advertisement recommending system, which simulates the process of recommend advertisements to users who may need them. Different people have different appetites, this works same on advertisements. Packages Descriptions Xen 4.4.1 Hypervisor for virtual machines Hadoop 2.6.0 open-source software framework for storage and large scale processing of data_sets on clusters of commodity hardware. Spark 2.1.0 A fast and general engine for large-scala data processing Spark MLlib An apache Spark’s scalable machine learning library. Hive A data warehouse software facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. Structure can be projected onto data already in storage SparkSql An Apache Spark’s module for working with structured data Tomcat 8.5.29 Web server Key Algorithms Collaborative Filtering Metric Learning K-means Key Features K-means clustering with Mahalanobis Distance SQL to form a utility matrix according to user behavior Employ clustering to estimate empty entries in the matrix Recommend to user who have bought similar items Xen Configuration Pin CPUs to virtual machines Migrate some memory to virtual machines to optimize performance Optimization of PerformanceSmall dataset is employed to optimize parameters. Parameters Running Time Num-execution 5—-7 3 mins 45 sec -&gt; 3 mins 13 sec Parallelism 8—-16 3 mins 13 sec -&gt; 1 min 38 sec Executor memory 1000m-2000m 3 mins 22 sec -&gt; 2 mins 54 sec Novelty In clustering, the distance functionis Mahalanobis Distance rather than euclidean distance. Traditional collaborative filtering utilizes rows and columns of utility matrix to estimate similarity of items and users as well as empty entries in the utility matrix.In our algorithm, we use clustering to estimate the empty entries. Algorithm IntroductionClusteringTwo kinds of clustering: hierarchica and point assignment. Hierarchical:Merge clusters. Each unassigned point is a cluster.The measurements are diameter, radius, average distance, minimum distance.Radius: the maximum distance from any points to centroid.Diameter: maximum distance between any two points. Point assignment: Assign each point to the nearest cluster. K-means++:initialize several points which are as far away as possible from each other as centroid of clusters.Assign the points to these different clusters and recompute the centroids. Complexity:O(n K I * d )n = number of records, K = number of clusters,I = number of iterations, d = number of attributes Recommendation SystemThe ultimate goal of a recommendation system is to estimate those empty entries in the utility matrix. To estimate empty entry, one possible way is to find n similar users and count their normalized ratings. Another possible way is to find the rating of similar items given by the same user. Methods: Collaborative filtering: use columns and rows of utility matrix to identify similar items or users and then do recommendation Content based:Utility matrix: ranked item is 1(Boolean) or actual rating(numerical) * normalizing factor. Others are represented by 0 or blank. Association Analysis. Characteristics of different methordsUser-based: tailer-made, accurate. Not easy to measure similarity.Content-based: Reliable, wider-range of recommendation. Construction of item profiles are tedious. NotesBelow are just notes of this project, feel free to skip it.Item features could also be represented by tags. But it is tedious to get tags data. Content-based recommendation is to create both an item profile consisting of feature-value pairs and a user profile summarizing the preferences of the user, based of their row of the utility matrix. Discretize numerical data to nominal data would lose the structure implicit in numbers. That is, two ratings that are close but not identical should be considered more similar than widely differing ratings. There is no harm if some components of the vectors are Boolean and others are real-valued or integer-valued. But it is necessary to somehow normalize the data so that the scale would not affect the result too much. Machine learning technique: ML algorithm to predict the empty entries in the utility matrix. Training set is the data. Optimize the loss. i.e. classification algorithm: build a decision tree to classify the data as like or don’t like for a particular user group. Recommendation System:Our recommendation system employs a modified version of collaborative filtering. In traditional recommendation system, in order to avoid constructing user and item profile which is a tedious and tricky step, rows and columns of utility matrix are utilized to estimate similarity of item and user as well as empty entries in the utility matrix. a utility matrix, giving for each user-item pair, a value that represents what is known about the degree of preference of that user for that item. Values come from an ordered set. We assume that the matrix is sparse, meaning that most entries are “unknown.” An unknown rating implies that we have no explicit information about the user’s preference for the item. Target of recommendation system:It is not necessary to predict every blank entry in a utility matrix. Rather, it is only necessary to discover some entries in each row that are likely to be high. In most applications, the recommendation system does not offer users a ranking of all items, but rather suggests a few that the user should value highly. It may not even be necessary to find all items with the highest expected ratings, but only to find a large subset of those with the highest ratings. User Profile: aggregation of utility matrix which represent connection between users and items. After getting user profile, make recommendation according to proximity between unseen object and content in user profile. (cosine similarity) The process of identifying similar users and recommending what similar users like is called collaborative filtering.Cosine distance treat empty entries as 0.Preprocessing: Round the data: round ratings to 0 or 1. Normalize the data and apply proximity measure. Measuring similarity:Jaccard distance: may emit important information like the scale of rating.Cosine distance The Duality of Similarity To estimate an entry, for item-based, find the most similar m items, and average them to get an estimate of rating for the entry(item). After cluster, get the entry for that item clusters by averaging the item Since there is user and ad profile, try to do clustering first. Data is large, initialize small cluster first to summarize the data points. Curse of dimensionality: in high dimension, cosine similarity approaches to 0, which means that random vectors are orthogonal. Distance and density become meaningless because distance is very far away compared to low dimensionality. Clustering:Euclidean space in which the cluster could be represented by a center.Clustroid: the closest point to other points in the clusterCentroid: the arithmetic center point calculated. Mahalanobis distance is distance between x and y divided by square root of covariance, which is undistorting the ellipse to make a circle ExtentionWord2Vec to do recomendation: sequence of actions of a user in a specific time range(within 30 minutes) is regarded as a context like the context words in word2vec. The search /query/clicked item is the center. As what has been done in word2vec, train the model s.t. the center item is close to context item and far away than other items. The learnt embeddings have been shown to correctly encode information like style,price and so on. The Airbnb makes some other modification: The final booking item is added s.t. the objective is to make the center item close to final booking item too. Sample items from the same location but different style or other condition as negative samples. ClusteringClustering is employed to promote diversity of recommendation. After the vectors of products are learnt, they are clustered together. If a customer buy something in a cluster, the system would choose items in another cluster in which other or similar users usually buy items after buying items in the same cluster. This avoid the problem of recommending the similar item with the same function to users. After choosing the cluster, recommend the top k items that are similar to the purchased item. Cold Start ProblemAverage 3 geographically closest listings accoding to location or age or something like that. ReferenceApplying word2vec to Recommenders and Advertising Listing Embeddings in Search Ranking]]></content>
      <tags>
        <tag>Project</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maximum_Subarray]]></title>
    <url>%2F2018%2F05%2F24%2FMaximum-Subarray%2F</url>
    <content type="text"><![CDATA[Dynamic ProgrammingTime Complexity: O(n) Store the first value. Traverse the array. If the last value of maximum subarray l is greater than 0, add the current value to it and store it. Else store it directly. Because if the last value of maximum subarray l is less than 0, no need to add the current value to it. If the last value of maximum subarray l is greater than 0, add the current value to it. Next time we check it, if it greater than 0, we add next value to it, else we discard it because it could not be the maximum subarray. 1234567891011121314151617181920212223242526class Solution: def maxSubArray(self, nums): """ :type nums: List[int] :type k: int :rtype: float """ if len(nums) == 1: return nums[0] l = [nums[0]] for i in range(1,len(nums)): # if nums[i] &lt;0: if l[i-1] &gt;0: l.append(nums[i] + l[i-1]) else: l.append(nums[i]) return max(l) # else:s = Solution()# leg = s.maxSubArray([-1,2,3,-6,8])leg = s.maxSubArray([-2,1,-3,4,-1,2,1,-5,4])# leg = s.max_cross([-2,1,-3,4,-1,2,1,-5,4])print(leg) divide and conquerrecursively divide a problem into subproblems.Time complexity: O(nlogn)logn: Each time divide the problem into two parts, so an array of size N could construct a log2N levels tree. Each level needs at most N time to solve. Reference: Introduction to Algorithm Cha 4.1123456789101112131415161718192021222324252627282930313233343536373839404142class Solution: def maxSubArray(self, nums): """ :type nums: List[int] :type k: int :rtype: float """ left = 0 right = len(nums)-1 #index of the last item # if right == 0: # return None if right == left: #only one element return nums[0] mid = int(len(nums) / 2) # print("mid",mid, left, right) # print(self.max_cross(nums), # self.maxSubArray(nums[left:mid]), self.maxSubArray(nums[mid:right]),) m = self.max_cross(nums) l = self.maxSubArray(nums[left:mid]) r = self.maxSubArray(nums[mid:right+1]) return max(m,l,r) # return r def max_cross(self, nums): mid = int(len(nums)/2) left = 0 right = len(nums)-1 left_sum_max, right_sum_max = float("-inf"), float("-inf") left_sum, right_sum = 0,0 left_index, right_index = -1,-1 for i in range(mid-1,left-1,-1): left_sum += nums[i] if left_sum&gt;left_sum_max: left_index = i left_sum_max = left_sum for i in range(mid,right+1): right_sum += nums[i] if right_sum&gt;right_sum_max: right_index = i right_sum_max = right_sum return left_sum_max+right_sum_max]]></content>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
</search>
